1960 -- txt
1970 -- DBMS
1980 -- RDBMS
1990 -- DW
1995 -- DFS (Failed)

2000+ -- Hey Google can you tell me how you deal data analytics

2003 --- Open paper -- Storage -- GFS --- Doug Cutting

GFS=== DFS


2004 --- Process -- Map Reduce for processing --- Doug Cutting Took this Papers

He started reading it. and completely literally Shocked


2005 ---Similar Mapreduce framework for DFS

2006 --- -- name (DFS and Mapreduce)-- Where is my Hadoop ?

Son's Elephant Doll Name

Hadoop  -- Technology Author -- Roger --- 


Big data analytics is possible using Hadoop --- Book

==================================
7z (Window Users)  --- Download
==================================

https://www.7-zip.org/a/7z2201-x64.exe

==================================
Oracle Virtual Box  --- Download
==================================

Window Users
https://download.virtualbox.org/virtualbox/6.1.36/VirtualBox-6.1.36-152435-Win.exe

Ubuntu Users
https://www.virtualbox.org/wiki/Linux_Downloads

Mac Users
https://download.virtualbox.org/virtualbox/6.1.36/VirtualBox-6.1.36-152435-OSX.dmg

==================================
Putty  (Window users)
==================================

https://the.earth.li/~sgtatham/putty/latest/w64/putty.exe

==================================
Mobaxterm Download(Window users)
=================================

Windows
https://download.mobatek.net/2212022060563542/MobaXterm_Installer_v22.1.zip

==================================
Cloudera --- Any one link to download (MAC ,  Window, Ubuntu)
==================================

Use Any One
https://drive.google.com/file/d/11VyzQsPt5xIwGVHqbDwOcjMj63WiGfKa/view?usp=sharing

https://drive.google.com/file/d/1VYNYI6Cmj0_Ko4BPGcztMj7PRAWL7XH6/view?usp=sharing

https://drive.google.com/file/d/1d538E823H4GuTVWcCLUJC6xpkYWsW_WO/view?usp=sharing

https://drive.google.com/file/d/1ebMjbwzC4Thrb-LnmpihHIpa4TBFL_qH/view?usp=sharing

https://drive.google.com/file/d/1ZVZgJnmCyyHmM2AoEHjlAUx8H5ResG8V/view?usp=sharing

https://drive.google.com/file/d/1Hx5fAeiAXd2aRe046yK1755zh2csdg9G/view?usp=sharing

https://zeyodevbb.s3.amazonaws.com/Sai_Zeyobron_Powered_Cloudera.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA5TH4P6EXLQC3OUV2%2F20220828%2Fap-south-1%2Fs3%2Faws4_request&X-Amz-Date=20220828T043054Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6f49882a6c0516aff6e0aaeef27f12b4ba245381a96df145f6a712333d108998

https://drive.google.com/file/d/11sEiI7f5jM4v7-O8QZBw6sSC6yJDn1Nh/view?usp=sharing

https://drive.google.com/file/d/1u7F9hLdnWAk0DiflDnymjEDFB21vA2kR/view?usp=sharing

https://drive.google.com/file/d/1T3y3qXXEcnSDcg5WFxZElTc8n5vhJcEQ/view?usp=sharing

https://drive.google.com/file/d/1ycm76Y3Pji5XUBIsdQquR2ToBZPghrzO/view?usp=sharing

https://drive.google.com/file/d/1yjNZD4m_yLlHmfkk0XjaQVSazWqdQP5f/view?usp=sharing

https://drive.google.com/file/d/1DPSfwelNTK4vFgccipeSmNGWCuCSEAIe/view?usp=sharing

https://drive.google.com/file/d/1-4xh-vo2kQyy_koPXesVB8E22NT7kASF/view?usp=sharing

https://drive.google.com/file/d/1TJLcu1o1ymKz-2Dz8ZtiBQxUXaDfayEW/view?usp=sharing

https://drive.google.com/file/d/1mTlcm71Vtm9WoIcTYRPSrwP0OSxVCtyy/view?usp=sharing

https://drive.google.com/file/d/1RxCwKrV1cLeiIAr95T1d2VH-qoJULLBB/view?usp=sharing

https://drive.google.com/file/d/19jUP51V_95mQ1aAZVCa8dGFTP6p1t8nc/view?usp=sharing

https://drive.google.com/file/d/1Cfow1iNyy_Nveo72SWyR9RVQLFkXRgHF/view?usp=sharing

https://drive.google.com/file/d/15RNIYynf_gSSC2v1nvCMR6OxE-AA3xR5/view?usp=sharing

S3 Link for Cloudera Download

https://zeyodevbb.s3.amazonaws.com/Sai_Zeyobron_Powered_Cloudera.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA5TH4P6EXLQC3OUV2%2F20220828%2Fap-south-1%2Fs3%2Faws4_request&X-Amz-Date=20220828T055457Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c02504e8f62b6f32502607bc240db014fe5cf5b0960b3326d508d6edd52495d3

Cloudera

Easy to download with below link

https://zeyodevbb.s3.amazonaws.com/Sai_Zeyobron_Powered_Cloudera.zip


==================================
7z (Window Users)  --- Download
==================================

https://www.7-zip.org/a/7z2201-x64.exe

==================================
Oracle Virtual Box  --- Download
==================================

Window Users
https://download.virtualbox.org/virtualbox/6.1.36/VirtualBox-6.1.36-152435-Win.exe

Ubuntu Users
https://www.virtualbox.org/wiki/Linux_Downloads

Mac Users
https://download.virtualbox.org/virtualbox/6.1.36/VirtualBox-6.1.36-152435-OSX.dmg

==================================
Putty  (Window users)
==================================

https://the.earth.li/~sgtatham/putty/latest/w64/putty.exe

==================================
Mobaxterm Download(Window users)
=================================

Windows
https://download.mobatek.net/2212022060563542/MobaXterm_Installer_v22.1.zip

==================================
Cloudera --- Any one link to download (MAC ,  Window, Ubuntu)
==================================

https://zeyodevbb.s3.amazonaws.com/Sai_Zeyobron_Powered_Cloudera.zip


==================================
7z (Window Users)  --- Download
==================================

https://www.7-zip.org/a/7z2201-x64.exe

==================================
Oracle Virtual Box  --- Download
==================================

Window Users
https://download.virtualbox.org/virtualbox/6.1.36/VirtualBox-6.1.36-152435-Win.exe

Ubuntu Users
https://www.virtualbox.org/wiki/Linux_Downloads

Mac Users
https://download.virtualbox.org/virtualbox/6.1.36/VirtualBox-6.1.36-152435-OSX.dmg

==================================
Putty  (Window users)
==================================

https://the.earth.li/~sgtatham/putty/latest/w64/putty.exe

==================================
Mobaxterm Download(Window users)
=================================

Windows
https://download.mobatek.net/2212022060563542/MobaXterm_Installer_v22.1.zip

==================================
Cloudera --- Any one link to download (MAC ,  Window, Ubuntu)
==================================

https://zeyodevbb.s3.amazonaws.com/Sai_Zeyobron_Powered_Cloudera.zip

==================================
7z (Window Users)  --- Download
==================================

https://www.7-zip.org/a/7z2201-x64.exe

==================================
Oracle Virtual Box  --- Download
==================================

Window Users
https://download.virtualbox.org/virtualbox/6.1.36/VirtualBox-6.1.36-152435-Win.exe

Ubuntu Users
https://www.virtualbox.org/wiki/Linux_Downloads

Mac Users
https://download.virtualbox.org/virtualbox/6.1.36/VirtualBox-6.1.36-152435-OSX.dmg

==================================
Putty  (Window users)
==================================

https://the.earth.li/~sgtatham/putty/latest/w64/putty.exe

==================================
Mobaxterm Download(Window users)
=================================

Windows
https://download.mobatek.net/2212022060563542/MobaXterm_Installer_v22.1.zip

==================================
Cloudera --- Any one link to download (MAC ,  Window, Ubuntu)
==================================

https://zeyodevbb.s3.amazonaws.com/Sai_Zeyobron_Powered_Cloudera.zip


Set up -- Try Max Or Follow the Lab Access Proceedure

Installation Steps

Step 1- Windows -- Install 7z -

Step 2 -- Windows/MAC/Ubuntu -- Install Virtual Box

Step 3 --- 
	Windows - Extract Cloudera using 7z
	Mac 	- Extract Cloudera using unarchiver
	Linux   - Extract Cloudera of Unzip 

Step 4 --- Open Virtual Box and Pull extracted Cloudera

	If you get 32-bit - Windows -- enable VtxTechnology

	Use Mobile- Open Below Video

	Windows 10 --

	https://www.youtube.com/watch?v=MOuTxfzCvMY

	Windows 11 --

	https://www.youtube.com/watch?v=UMo-is3fjPI
	https://www.youtube.com/watch?v=0WuFGCn036E
	https://www.youtube.com/watch?v=t8f-zw_wcWM


Step 5 --- Proceed with 64 Bit -- 

	New -> 64 Bit --->Set memory 
			16GB- 5000
			8GB -4000
			4GB -2700  
	Next ---> Select Third option --> Click Folder ---> +ADD -- Go to the extracted Folder - Pull Orange Icon File ----> Choose --> ok ---> Setting -> General -- advanced -- Both  bidirectional


Step 6 ---- Start -- If it abruptly closes -- Enable VTX


Cloudera Installation Video

https://youtu.be/xsTbkZ8r1qo

Cloudera download links (anyone is fine)

https://drive.google.com/file/d/1u7F9hLdnWAk0DiflDnymjEDFB21vA2kR/view?usp=sharing

https://drive.google.com/file/d/1T3y3qXXEcnSDcg5WFxZElTc8n5vhJcEQ/view?usp=sharing

https://drive.google.com/file/d/1ycm76Y3Pji5XUBIsdQquR2ToBZPghrzO/view?usp=sharing

https://drive.google.com/file/d/1yjNZD4m_yLlHmfkk0XjaQVSazWqdQP5f/view?usp=sharing

https://drive.google.com/file/d/1DPSfwelNTK4vFgccipeSmNGWCuCSEAIe/view?usp=sharing

https://drive.google.com/file/d/1-4xh-vo2kQyy_koPXesVB8E22NT7kASF/view?usp=sharing

https://drive.google.com/file/d/1TJLcu1o1ymKz-2Dz8ZtiBQxUXaDfayEW/view?usp=sharing

https://drive.google.com/file/d/1mTlcm71Vtm9WoIcTYRPSrwP0OSxVCtyy/view?usp=sharing

https://drive.google.com/file/d/1RxCwKrV1cLeiIAr95T1d2VH-qoJULLBB/view?usp=sharing

https://drive.google.com/file/d/19jUP51V_95mQ1aAZVCa8dGFTP6p1t8nc/view?usp=sharing

https://drive.google.com/file/d/1Cfow1iNyy_Nveo72SWyR9RVQLFkXRgHF/view?usp=sharing

https://drive.google.com/file/d/15RNIYynf_gSSC2v1nvCMR6OxE-AA3xR5/view?usp=sharing


Major Updates 

1) Lab Final Names would be taken tomorrow

2) ACTIVATE LAB IN JUST 2 MIN

https://youtu.be/FgySJ1mykvU

1-- join above group

2-- post rs.1000 screenshot with ur email id..



3-- later activate it with below video

https://youtu.be/FgySJ1mykvU

Switch on your Cloudera or Lab Open Please

To Activate Lab - watch this video

https://youtu.be/FgySJ1mykvU

cd
mkdir zdir 
ll      		(validate it)
cd zdir 		(go inside zdir)
touch zeyofile		(create empty file)
ll    			(validate it)
rm zeyofile  		(remove file)
cd ..
pwd     		(/home/cloudera/)
rmdir zdir  		(remove directory)
ll      		(zdir should not exist)

Task 1 ---

cd
mkdir z1
cd z1
mkdir z11
cd z11
pwd       (/home/cloudera/z1/z11)
mkdir /home/cloudera/z2/   ( lab  mkdir /home/<LABUSER>/z2/ )
ls /home/cloudera/ ( lab  ls /home/<LABUSER>/ )


Task 2 ---

cd
echo zeyobron>zeyofile
cat zeyofile

Putty connect Video

https://youtu.be/QACCTS9ioTQ


Pls watch the whole video instructions then start connecting to it


Cloudera Folks

cd
ls /home/cloudera/
mkdir /home/cloudera/zzdir
touch /home/cloudera/zzdir/zeyofile
rm /home/cloudera/zzdir/zeyofile
rmdir /home/cloudera/zzdir
ls /home/cloudera/

cd

hadoop fs -ls /user/cloudera/
hadoop fs -mkdir /user/cloudera/zzdir
hadoop fs -ls /user/cloudera/
hadoop fs -touchz /user/cloudera/zzdir/zeyofile
hadoop fs -ls /user/cloudera/zzdir/
hadoop fs -rm /user/cloudera/zzdir/zeyofile
hadoop fs -rmdir /user/cloudera/zzdir
hadoop fs -ls /user/cloudera/       (zzdir will not exist)

Lab Folks

cd
ls /home/<LABUSER>/
mkdir /home/<LABUSER>/zzdir
touch /home/<LABUSER>/zzdir/zeyofile
rm /home/<LABUSER>/zzdir/zeyofile
rmdir /home/<LABUSER>/zzdir
ls /home/<LABUSER>/

cd

hadoop fs -ls /user/<LABUSER>/
hadoop fs -mkdir /user/<LABUSER>/zzdir
hadoop fs -ls /user/<LABUSER>/
hadoop fs -touchz /user/<LABUSER>/zzdir/zeyofile
hadoop fs -ls /user/<LABUSER>/zzdir/
hadoop fs -rm /user/<LABUSER>/zzdir/zeyofile
hadoop fs -rmdir /user/<LABUSER>/zzdir
hadoop fs -ls /user/<LABUSER>/       (zzdir will not exist)


Cloudera Safe mode command

hadoop dfsadmin -safemode leave


Cloudera Folks

hadoop dfsadmin -safemode leave
cd
echo zeyobron>zfile
cat zfile     ( you will see zeyobron )
hadoop fs -put /home/cloudera/zfile /user/cloudera/
hadoop fs -ls /user/cloudera/
hadoop fs -cat /user/cloudera/zfile      ( you will see zeyobron )


Lab Folks

cd
echo zeyobron>zfile
cat zfile     ( you will see zeyobron )
hadoop fs -put /home/<LABUSER>/zfile /user/<LABUSER>/
hadoop fs -ls /user/<LABUSER>/
hadoop fs -cat /user/<LABUSER>/zfile      ( you will see zeyobron )


Cloudera Folks

hadoop dfsadmin -safemode leave
cd
echo zeyobron>zfile
cat zfile     ( you will see zeyobron )
hadoop fs -put /home/cloudera/zfile /user/cloudera/
hadoop fs -ls /user/cloudera/
hadoop fs -cat /user/cloudera/zfile      ( you will see zeyobron )


Lab Folks

cd
echo zeyobron>zfile
cat zfile     ( you will see zeyobron )
hadoop fs -put /home/<LABUSER>/zfile /user/<LABUSER>/
hadoop fs -ls /user/<LABUSER>/
hadoop fs -cat /user/<LABUSER>/zfile      ( you will see zeyobron )

Cloudera Folks

rm zfile      (give y)
cat zfile    ( no file)
hadoop fs -get /user/cloudera/zfile /home/cloudera/
ls
cat zfile

Lab Folks

rm zfile      (give y)
cat zfile    ( no file)
hadoop fs -get /user/<LABUSER>/zfile /home/<LABUSER>/
ls
cat zfile

Tasks Updated (Lab Folks might face issues. Corrected here)

Cloudera Folks

hadoop dfsadmin -safemode leave
cat data.avro | head -10
click ctrl+c (comes out to terminal)
hadoop fs -put /home/cloudera/data.avro /user/cloudera
hadoop fs -ls /user/cloudera
hadoop fs -cat /user/cloudera/data.avro

Lab Folks

sqoop import -Dmapreduce.job.user.classpath.first=true --connect jdbc:mysql://ms.itversity.com/nyse_export --username nyse_user --password itversity --table zeyotab1 --m 1 --delete-target-dir --target-dir /user/<LABUSER>/avrodata --as-avrodatafile

hadoop fs -cat /user/<LABUSER>/avrodata/*



Lab Folks Please execute below commands before the session starts

mysql --host=zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com --user=root --password=Aditya908

create database <LABUSER>; 

Mention your lab username in the command

Cloudera Folks

hadoop fs -put -f /home/cloudera/data.avro /user/cloudera
hadoop fs -cat /user/cloudera/data.avro | head -4
ctrl+c  in keyboard
hadoop fs -text /user/cloudera/data.avro | head -4 (readable)

Lab Folks

sqoop import --connect jdbc:mysql://zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com/zeyodb --username root --password Aditya908 --table data  --delete-target-dir --m 1 --target-dir /user/<LABUSER>/datadir --as-avrodatafile

hadoop fs -cat /user/<LABUSER>/datadir/*
ctrl+c in keyboard
hadoop fs -text /user/<LABUSER>/datadir/*

Cloudera Folks how do not have data.avro


sqoop import --connect jdbc:mysql://zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com/zeyodb --username root --password Aditya908 --table data  --delete-target-dir --m 1 --target-dir /user/cloudera/datadir --as-avrodatafile

hadoop fs -cat /user/cloudera/datadir/*
ctrl+c in keyboard
hadoop fs -text /user/cloudera/datadir/*

Lab Folks

sqoop import -Dmapreduce.job.user.classpath.first=true --connect jdbc:mysql://zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com/zeyodb --username root --password Aditya908 --table data  --delete-target-dir --m 1 --target-dir /user/<LABUSER>/datadir --as-avrodatafile

hadoop fs -cat /user/<LABUSER>/datadir/*
ctrl+c in keyboard
hadoop fs -text /user/<LABUSER>/datadir/*

Lab Folks

sqoop import --connect jdbc:mysql://zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com/zeyodb --username root --password Aditya908 --table zt --m 1 --delete-target-dir --target-dir /user/<LABUSER>/datadir

hadoop fs -ls /user/<LABUSER>/datadir
hadoop fs -cat /user/<LABUSER>/datadir/part-m-00000


Cloudera Folks

sqoop import --connect jdbc:mysql://localhost/mysql --username root --password cloudera --query "select user,Select_priv from user where \$CONDITIONS  limit 4" --m 1 --delete-target-dir --target-dir /user/cloudera/datadir

hadoop fs -ls /user/cloudera/datadir
hadoop fs -cat /user/cloudera/datadir/part-m-00000

Tasks

Cloudera Folks

mysql -uroot -pcloudera
create database zdb;
use zdb;
create table customer(id int,name varchar(100),city varchar(100),amount int);
insert into customer values(1,'zeyo','chennai',40);
insert into customer values(2,'vish','hyderabad',50);
insert into customer values(3,'hasy','chennai',10);
insert into customer values(4,'vidu','bangalore',70);
insert into customer values(5,'vasu','hyderabad',90);
insert into customer values(6,'ravi','bangalore',20);
select * from customer;
quit

sqoop import --connect jdbc:mysql://localhost/zdb --username root --password cloudera --table customer --where "city='chennai'" --m 1 --delete-target-dir --target-dir /user/cloudera/chdata
hadoop fs -ls /user/cloudera/chdata
hadoop fs -cat /user/cloudera/chdata/part-m-00000


Lab Folks

sqoop import --connect jdbc:mysql://zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com:3306/zeyodb --username root --password Aditya908 --table customer --where "city='chennai'" --m 1 --delete-target-dir --target-dir /user/<LABUSER>/chdata

hadoop fs -ls /user/<LABUSER>/chdata
hadoop fs -cat /user/<LABUSER>/chdata/part-m-00000

notes

hostname
portnumber
username
password
database
table

targetdir

sqoop import --connect jdbc:mysql://<HOSTNAME>:<PORT>/<DATABASE> --username <USERNAME> --password <PASSWORD> --table <TABLE> --m 1 --target-dir <TARGETDIR>

Mail --- Using below details can you import data to /user/cloudera/newimp

hostname - zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com
port  - 3306
username - root
password - Aditya908
database - zeyodb
table - zt

sqoop import --connect jdbc:mysql://zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com:3306/zeyodb --username root --password Aditya908 --table zt --m 1 --target-dir /user/cloudera/newimp


Boss - 

Sai ..  In one of the RDBMS' we have a table customer consist of Chennai,Bangalore and Hyderabad. Can you check the table let me know 



Sai --- boss I could see it.


Boss --- Sai can you find a way to import only chennai data


hostname -- zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com
port -- 3306
username -- root
password -- Aditya908
database - zeyodb
table - customer

target-dir -- /user/cloudera/chdata


sqoop import --connect jdbc:mysql://zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com:3306/zeyodb --username root --password Aditya908 --table customer --where "city='chennai'" --m 1 --target-dir /user/cloudera/chdata


City


https://youtu.be/dNv8nPmhKFk


Task Video and Solution Document

Both Lab and Cloudera users

===============
ClouderaW
===============

mysql -uroot -pcloudera
create database zadb;
use zadb;

create table cust(id int,name varchar(100),location varchar(100),mode  varchar(100));


insert into cust values(1,'zeyo','chennai','cash');
insert into cust values(2,'hema','hyderabad','credit');
insert into cust values(3,'haas','chennai','credit');
insert into cust values(4,'vasu','bangalore','cash');
insert into cust values(5,'ravi','hyderabad','cash');
select * from cust;

quit

sqoop import --connect jdbc:mysql://localhost/zadb --username root --password cloudera --table cust --where "location='chennai' and mode='cash'" --columns name,location --m 1 --delete-target-dir --target-dir /user/cloudera/chennaidir


hadoop fs -ls /user/cloudera/chennaidir
hadoop fs -cat /user/cloudera/chennaidir/part-m-00000


==========
Lab
==========


sqoop import --connect jdbc:mysql://zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com/itv002921 --username root --password Aditya908 --table cust --m 1 --where "location='chennai' and mode='cash'" --columns name,location --delete-target-dir --target-dir /user/<LABUSER>/chennaidir


hadoop fs -ls /user/<LABUSER>/chennaidir
hadoop fs -cat /user/<LABUSER>/chennaidir/part-m-00000

Cloudera Safe Mode Issue

hadoop dfsadmin -safemode leave

===============
Cloudera query
===============

mysql -uroot -pcloudera
create database zadb1;
use zadb1;

create table c1(id int,name varchar(100));

insert into c1 values(1,'zeyo');
insert into c1 values(2,'hema');
insert into c1 values(3,'haas');
insert into c1 values(4,'vasu');
insert into c1 values(5,'ravi');


create table c2(id int,location varchar(100),mode  varchar(100));

insert into c2 values(1,'chennai','cash');
insert into c2 values(2,'hyderabad','credit');
insert into c2 values(3,'chennai','credit');
insert into c2 values(4,'bangalore','cash');
insert into c2 values(5,'hyderabad','cash');

select * from c1;
select * from c2;

select a.*,b.location,b.mode from c1 a join c2 b on a.id=b.id;
quit

sqoop import --connect jdbc:mysql://localhost/zadb1 --username root --password cloudera --query "select a.*,b.location,b.mode from c1 a join c2 b on a.id=b.id where \$CONDITIONS" --m 1 --delete-target-dir --target-dir /user/cloudera/joindir

==========
Lab query
==========


mysql --host=zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com --user=root --password=Aditya908
use zdb;

select * from c1;
select * from c2;

select a.*,b.location,b.mode from c1 a join c2 b on a.id=b.id;
quit




sqoop import --connect jdbc:mysql://zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com/zdb--username root --password Aditya908 --query "select a.*,b.location,b.mode from c1 a join c2 b on a.id=b.id where \$CONDITIONS" --m 1  --delete-target-dir --target-dir /user/<LABUSER>/joindir


hadoop fs -ls /user/<LABUSER>/joindir
hadoop fs -cat /user/<LABUSER>/joindir/part-m-00000

Lab Folks

sqoop import --connect jdbc:mysql://zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com/zdb --username root --password Aditya908 --query "select a.*,b.location,b.mode from c1 a join c2 b on a.id=b.id where \$CONDITIONS" --m 1  --delete-target-dir --target-dir /user/<LABUSER>/joindir

hadoop fs -ls /user/<LABUSER>/joindir
hadoop fs -cat /user/<LABUSER>/joindir/part-m-00000


=========
Cloudera Incremental
=========

mysql -uroot -pcloudera
create database inc;
use inc;
create table cust(id int,name varchar(100),location varchar(100),mode  varchar(100));

insert into cust values(1,'zeyo','chennai','cash');
insert into cust values(2,'hema','hyderabad','credit');
insert into cust values(3,'haas','chennai','credit');
insert into cust values(4,'vasu','bangalore','cash');
insert into cust values(5,'ravi','hyderabad','cash');
select * from cust;
quit 

sqoop import --connect jdbc:mysql://localhost/inc --username root --password cloudera --table cust --m 1 --delete-target-dir --target-dir /user/cloudera/indir

hadoop fs -ls /user/cloudera/indir
hadoop fs -cat /user/cloudera/indir/part-m-00000



=====
add data
=====


mysql -uroot -pcloudera
use inc;

insert into cust values(6,'rani','bangalore','cash');
insert into cust values(7,'vinu','hyderabad','cash');
select * from cust;
quit


sqoop import --connect jdbc:mysql://localhost/inc --username root --password cloudera --table cust --m 1 --target-dir /user/cloudera/indir --incremental append --check-column id --last-value 5

hadoop fs -ls /user/cloudera/indir
hadoop fs -cat /user/cloudera/indir/part-m-00001


=========
Lab Incremental Folks
=========

mysql --host=zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com --user=root --password=Aditya908
create database <LABUSER>;
use <LABUSER>;
create table custi(id int,name varchar(100),location varchar(100),mode  varchar(100));

insert into custi values(1,'zeyo','chennai','cash');
insert into custi values(2,'hema','hyderabad','credit');
insert into custi values(3,'haas','chennai','credit');
insert into custi values(4,'vasu','bangalore','cash');
insert into custi values(5,'ravi','hyderabad','cash');
select * from custi;
quit 

sqoop import --connect jdbc:mysql://zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com/<LABUSER> --username root --password Aditya908 --table custi --m 1 --delete-target-dir --target-dir /user/<LABUSER>/indir

hadoop fs -ls /user/<LABUSER>/indir
hadoop fs -cat /user/<LABUSER>/indir/part-m-00000



=====
add data
=====

mysql --host=zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com --user=root --password=Aditya908

use <LABUSER>;

insert into custi values(6,'rani','bangalore','cash');
insert into custi values(7,'vinu','hyderabad','cash');
select * from custi;
quit


sqoop import --connect jdbc:mysql://zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com/<LABUSER> --username root --password Aditya908 --table custi --m 1 --target-dir /user/<LABUSER>/indir --incremental append --check-column id --last-value 5

hadoop fs -ls /user/<LABUSER>/indir
hadoop fs -cat /user/<LABUSER>/indir/part-m-00001


======================
Cloudera Job
======================

mysql -uroot -pcloudera
create database incjob;
use incjob;
create table cust(id int,name varchar(100),location varchar(100),mode  varchar(100));

insert into cust values(1,'zeyo','chennai','cash');
insert into cust values(2,'hema','hyderabad','credit');
insert into cust values(3,'haas','chennai','credit');
insert into cust values(4,'vasu','bangalore','cash');
insert into cust values(5,'ravi','hyderabad','cash');
select * from cust;
quit 

sqoop job --create injob --  import --connect jdbc:mysql://localhost/incjob --username root --password cloudera --table cust --m 1 --target-dir /user/cloudera/jobdir --incremental append --check-column id --last-value 0
 
sqoop job --list

sqoop job --exec injob                         -- give password cloudera

hadoop fs -ls /user/cloudera/jobdir
hadoop fs -cat /user/cloudera/jobdir/part-m-00000



mysql -uroot -pcloudera
use incjob;

insert into cust values(6,'rani','bangalore','cash');
insert into cust values(7,'vinu','hyderabad','cash');
select * from cust;
quit 


sqoop job --exec injob					-- give password cloudera

hadoop fs -ls /user/cloudera/jobdir
hadoop fs -cat /user/cloudera/jobdir/part-m-00001


cd
cat /home/cloudera/.sqoop/metastore.db.script | grep 'last.value'

======================
Lab Job
======================

mysql --host=zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com --user=root --password=Aditya908
create database <LABUSER>;
use <LABUSER>;
create table custij(id int,name varchar(100),location varchar(100),mode  varchar(100));

insert into custij values(1,'zeyo','chennai','cash');
insert into custij values(2,'hema','hyderabad','credit');
insert into custij values(3,'haas','chennai','credit');
insert into custij values(4,'vasu','bangalore','cash');
insert into custij values(5,'ravi','hyderabad','cash');
select * from custij;
quit


sqoop job --create injob -- import --connect jdbc:mysql://zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com/<LABUSER> --username root --password Aditya908 --table custij --m 1 --target-dir /user/<LABUSER>/jobdir --incremental append --check-column id --last-value 0
 
sqoop job --list     

sqoop job --exec injob    =====give password as Aditya908

hadoop fs -ls /user/<LABUSER>/jobdir
hadoop fs -cat /user/<LABUSER>/jobdir/part-m-00000



mysql --host=zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com --user=root --password=Aditya908
use <LABUSER>;

insert into custij values(6,'rani','bangalore','cash');
insert into custij values(7,'vinu','hyderabad','cash');
select * from custij;
quit 


sqoop job --exec injob   =====give password as Aditya908

hadoop fs -ls /user/<LABUSER>/jobdir
hadoop fs -cat /user/<LABUSER>/jobdir/part-m-00001


cat /home/<LABUSER>/.sqoop/metastore.db.script | grep 'last.value'

Tasks
Task 1 ---

Cloudera Folks

mysql -uroot -pcloudera
create database db1;
use db1;
create table avt(id int,name varchar(100),location varchar(100));
insert into avt values(1,'sai','chennai');
insert into avt values(2,'zeyo','hyderabad');
insert into avt values(3,'vasu','chennai');
insert into avt values(4,'rani','hyderabad');
quit
sqoop import --connect jdbc:mysql://localhost/db1 --username root --password cloudera --table avt --m 1 --target-dir /user/cloudera/pardir --as-parquetfile

hadoop fs -ls /user/cloudera/pardir


Lab Folks

mysql --host=zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com --user=root --password=Aditya908
create database <LABUSER>;
use <LABUSER>;
create table avt(id int,name varchar(100),location varchar(100));
insert into avt values(1,'sai','chennai');
insert into avt values(2,'zeyo','hyderabad');
insert into avt values(3,'vasu','chennai');
insert into avt values(4,'rani','hyderabad');
quit
sqoop import --connect jdbc:mysql://zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com/<LABUSER> --username root --password Aditya908 --table avt --m 1 --target-dir /user/<LABUSER>/pardir --as-parquetfile





Task 2 --- SQL Scenario -----


Cloudera

mysql -uroot -pcloudera
use db1;
select * from avt;
select id,name from avt;
select * from avt where location='chennai';
select * from avt where location='hyderabad';


Lab

mysql --host=zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com --user=root --password=Aditya908
use <LABUSER>;
select * from avt;
select id,name from avt;
select * from avt where location='chennai';
select * from avt where location='hyderabad';

sqoop import --connect jdbc:mysql://zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com/itv004068 --username root --password Aditya908 --table cust --m 1 --where "location='chennai' and mode='cash'" --columns name,location --delete-target-dir --target-dir /user/itv004068/chennaidir


hadoop fs -ls /user/itv004068/chennaidir
hadoop fs -cat /user/itv004068/chennaidir/part-m-00000

2 task
sqoop import --connect jdbc:mysql://zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com/zdb --username root --password Aditya908 --query "select a.*,b.location,b.mode from c1 a join c2 b on a.id=b.id where \$CONDITIONS" --m 1  --delete-target-dir --target-dir /user/itv004068/joindir


hadoop fs -ls /user/itv004068/joindir
hadoop fs -cat /user/itv004068/joindir/part-m-00000


=====================
Lab Incremental Folks
=====================

mysql --host=zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com --user=root --password=Aditya908
create database itv004068;
use itv004068;
create table custi(id int,name varchar(100),location varchar(100),mode  varchar(100));

insert into custi values(1,'zeyo','chennai','cash');
insert into custi values(2,'hema','hyderabad','credit');
insert into custi values(3,'haas','chennai','credit');
insert into custi values(4,'vasu','bangalore','cash');
insert into custi values(5,'ravi','hyderabad','cash');
select * from custi;
quit 

sqoop import --connect jdbc:mysql://zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com/itv004068 --username root --password Aditya908 --table custi --m 1 --delete-target-dir --target-dir /user/itv004068/indir

hadoop fs -ls /user/itv004068/indir
hadoop fs -cat /user/itv004068/indir/part-m-00000



=====
add data
=====

mysql --host=zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com --user=root --password=Aditya908

use itv004068;

insert into custi values(6,'rani','bangalore','cash');
insert into custi values(7,'vinu','hyderabad','cash');
select * from custi;
quit


sqoop import --connect jdbc:mysql://zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com/itv004068 --username root --password Aditya908 --table custi --m 1 --target-dir /user/itv004068/indir --incremental append --check-column id --last-value 5

hadoop fs -ls /user/itv004068/indir
hadoop fs -cat /user/itv004068/indir/part-m-00001

sqoop import --connect jdbc:mysql://zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com/itv004068 --username root --password Aditya908 --table cust --m 1 --where "location='chennai' and mode='cash'" --columns name,location --delete-target-dir --target-dir /user/itv004068/chennaidir


hadoop fs -ls /user/itv004068/chennaidir
hadoop fs -cat /user/itv004068/chennaidir/part-m-00000



======================
Lab Job
======================

mysql --host=zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com --user=root --password=Aditya908
create database itv004068;
use itv004068;
create table custij(id int,name varchar(100),location varchar(100),mode  varchar(100));

insert into custij values(1,'zeyo','chennai','cash');
insert into custij values(2,'hema','hyderabad','credit');
insert into custij values(3,'haas','chennai','credit');
insert into custij values(4,'vasu','bangalore','cash');
insert into custij values(5,'ravi','hyderabad','cash');
select * from custij;
quit


sqoop job --create injob -- import --connect jdbc:mysql://zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com/itv004068 --username root --password Aditya908 --table custij --m 1 --target-dir /user/itv004068/jobdir --incremental append --check-column id --last-value 0
 
sqoop job --list     

sqoop job --exec injob    =====give password as Aditya908

hadoop fs -ls /user/itv004068/jobdir
hadoop fs -cat /user/itv004068/jobdir/part-m-00000



mysql --host=zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com --user=root --password=Aditya908
use itv004068;

insert into custij values(6,'rani','bangalore','cash');
insert into custij values(7,'vinu','hyderabad','cash');
select * from custij;
quit 


sqoop job --exec injob   =====give password as Aditya908

hadoop fs -ls /user/itv004068/jobdir
hadoop fs -cat /user/itv004068/jobdir/part-m-00001


cat /home/itv004068/.sqoop/metastore.db.script | grep 'last.value'

----------
01-10-2022
----------
Task 1

==============
*Cloudera Password File*
==============

cd
echo -n cloudera>pfile
mysql -uroot -pcloudera

create database datap;
use datap;

create table intab (id int,name varchar(100),amt int);
insert into intab values(1,'zeyo',4);
insert into intab values(2,'sai',5);
insert into intab values(3,'vasu',7);
select * from intab;
quit


sqoop job --create pjob -- import --connect jdbc:mysql://localhost/datap --username root --password-file file:///home/cloudera/pfile --m 1 --table intab --target-dir /user/cloudera/datap --incremental append --check-column id --last-value 0

sqoop job --list
sqoop job --exec pjob                 //-- it will not ask the password


hadoop fs -ls /user/cloudera/datap 
hadoop fs -cat /user/cloudera/datap/part-m-00000
mysql -uroot -pcloudera
use datap;
insert into intab values(4,'rani',5);
insert into intab values(5,'hema',7);
select * from intab;
quit


sqoop job --exec pjob			//-- it will not ask the password


hadoop fs -ls /user/cloudera/datap
hadoop fs -cat /user/cloudera/datap/part-m-00001

==============
*lab Password File*
==============

cd
echo -n Aditya908>pfile
mysql --host=zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com --user=root --password=Aditya908
create database itv004068;
use itv004068;

create table intab (id int,name varchar(100),amt int);
insert into intab values(1,'zeyo',4);
insert into intab values(2,'sai',5);
insert into intab values(3,'vasu',7);
select * from intab;
quit

sqoop job --create pjob -- import --connect jdbc:mysql://zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com/itv004068 --username root --password-file file:///home/itv004068/pfile --m 1 --table intab --target-dir /user/itv004068/datap --incremental append --check-column id --last-value 0


sqoop job --list
sqoop job --exec pjob			//-- it will not ask the password


hadoop fs -ls /user/itv004068/datap 
hadoop fs -cat /user/itv004068/datap/part-m-00000
mysql --host=zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com --user=root --password=Aditya908
use itv004068;
insert into intab values(4,'rani',5);
insert into intab values(5,'hema',7);
select * from intab;
quit


sqoop job --exec pjob


hadoop fs -ls /user/itv004068/datap
hadoop fs -cat /user/itv004068/datap/part-m-00001
-------
Task 2
--------
Cloudera Folks
==============

mysql -uroot -pcloudera

create database datap1;
use datap1;

create table intab (id int,name varchar(100),amt int);
insert into intab values(1,'zeyo',4);
insert into intab values(2,'sai',5);
insert into intab values(3,'vasu',7);
select * from intab;
quit

sqoop import -Dfs.s3a.access.key=AKIAV6SZ74UIVD7ZWL4U -Dfs.s3a.secret.key=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 -Dfs.s3a.endpoint=s3.ap-south-1.amazonaws.com --connect jdbc:mysql://localhost/datap1 --username root --password cloudera --table intab --m 1  --target-dir  s3a://skbuck/itv004068
---------
Lab Folks
---------
cd
rm -rf awscli-bundle.zip
curl https://s3.amazonaws.com/aws-cli/awscli-bundle-1.16.188.zip -o awscli-bundle.zip
unzip awscli-bundle.zip
./awscli-bundle/install -i /home/itv004068/aws -b /home/itv004068/bin/aws
aws=/home/itv004068/bin/aws
cd
rm -rf .aws
mkdir .aws
cd .aws
wget https://skbuck.s3.amazonaws.com/credentials
cd
======
To validate data
======
aws s3 ls s3://skbuck/

Cloudera To validate data

cd
rm -rf .aws
mkdir .aws
cd .aws
wget https://skbuck.s3.amazonaws.com/credentials
cd

======
To validate data
======
aws s3 ls s3://skbuck/


===================
02-10-2022 12 class
===================
=====================
Cloudera File formats
=====================

cd
mysql -uroot -pcloudera
create database add1;
use add1;
create table ttab(id int,name varchar(100),amount int);
insert into ttab values(1,'zeyo',40);
insert into ttab values(2,'vasu',50);
insert into ttab values(3,'rani',70);
select * from ttab;
quit

sqoop import --connect jdbc:mysql://localhost/add1 --username root --password cloudera --table ttab --m 1 --delete-target-dir --target-dir /user/cloudera/pdata --as-parquetfile

hadoop fs -ls  /user/cloudera/pdata/
hadoop fs -cat /user/cloudera/pdata/*

===============
Lab File format
===============

mysql --host=zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com --user=root --password=Aditya908
create database itv004068;
use itv004068;
create table ttab(id int,name varchar(100),amount int);
insert into ttab values(1,'zeyo',40);
insert into ttab values(2,'vasu',50);
insert into ttab values(3,'rani',70);
select * from ttab;
quit


sqoop import --connect jdbc:mysql://zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com/itv004068 --username root --password Aditya908 --table ttab --m 1 --delete-target-dir --target-dir /user/itv004068/pdata --as-parquetfile


hadoop fs -ls /user/itv004068/pdata
hadoop fs -cat /user/itv004068/pdata/

'Task 1 --- AVRO imports
=========================
Cloudera Folks

cd
mysql -uroot -pcloudera
create database add1;
use add1;
create table ttab2(id int,name varchar(100),amount int);
insert into ttab2 values(1,'zeyo',40);
insert into ttab2 values(2,'vasu',50);
insert into ttab2 values(3,'rani',70);
select * from ttab2;
quit

sqoop import --connect jdbc:mysql://localhost/add1 --username root --password cloudera --table ttab2 --m 1 --delete-target-dir --target-dir /user/cloudera/adata --as-avrodatafile

hadoop fs -ls /user/cloudera/adata
hadoop fs -cat /user/cloudera/adata/*

=========
Lab Folks
=========

mysql --host=zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com --user=root --password=Aditya908
create database itv004068;
use itv004068;
create table ttab2(id int,name varchar(100),amount int);
insert into ttab2 values(1,'zeyo',40);
insert into ttab2 values(2,'vasu',50);
insert into ttab2 values(3,'rani',70);
select * from ttab2;
quit

sqoop import  -Dmapreduce.job.user.classpath.first=true --connect jdbc:mysql://zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com/itv004068 --username root --password Aditya908 --table ttab --m 1 --delete-target-dir --target-dir /user/itv004068/adata --as-avrodatafile


hadoop fs -ls /user/itv004068/adata
hadoop fs -cat /user/itv004068/adata/*




Task 2 --- Take Notes or In paint
=================================
Put in paint or Nodes  (Compression and Notes)


==============
15-10-2022
==============
Cloudera Folks
==============
Type hive and enter

create database zeyodb;
!hadoop fs -ls /user/hive/warehouse;
use zeyodb;
create table atab(id int);
!hadoop fs -ls /user/hive/warehouse/zeyodb.db;

========= 
Lab Folks
=========

hive -e "SET hive.metastore.warehouse.dir = /user/itv004068/warehouse;drop database itv004068 cascade;create database itv004068;use itv004068;create table atab(id int);"


hadoop fs -ls  /user/itv004068/warehouse
hadoop fs -ls /user/itv004068/warehouse/itv004068.db

========
Cloudera
========
cd
echo 1,zeyo>data.csv
echo 2,vish>>data.csv

hive

create database if not exists zeyodb;
use zeyodb;
create table datatab (id int,name string) row format delimited fields terminated by ',';
load data local inpath '/home/cloudera/data.csv' into table datatab;
select * from datatab;
=========
Lab Folks
=========
cd
echo 1,zeyo>data.csv
echo 2,vish>>data.csv

hive -e "SET hive.metastore.warehouse.dir = /user/<itv004068>/warehouse;create database if not exists itv004068;use itv004068;
create table datatab (id int,name string) row format delimited fields terminated by ',';load data local inpath '/home/itv004068/data.csv' into table datatab;select * from datatab;"



======
Task 1-
======
==============
Cloudera Folks
==============
cd
echo 1,zeyo>data.csv
echo 2,vish>>data.csv
hadoop fs -put data.csv /user/cloudera/

hive

create database if not exists zeyodb1;

use zeyodb1;

create table datatab (id int,name string) row format delimited fields terminated by ',';
load data  inpath '/user/cloudera/data.csv' into table datatab;

select * from datatab;

=========
Lab Folks
=========
cd
echo 1,zeyo>data.csv
echo 2,vish>>data.csv

hive -e "SET hive.metastore.warehouse.dir = /user/itv004068/warehouse;create database if not exists itv004068;use itv004068;
create table datatab1 (id int,name string) row format delimited fields terminated by ',';load data  inpath '/user/itv004068/data.csv' into table datatab1;select * from datatab1;"

ERROR
=================
Updated Lab Folks
=================
cd
echo 1,zeyo>data.csv
echo 2,vish>>data.csv

hadoop fs -put data.csv /user/itv004068/

hive -e "SET hive.metastore.warehouse.dir = /user/itv004068/warehouse;create database if not exists itv004068;use itv004068;
create table datatab1 (id int,name string) row format delimited fields terminated by ',';load data  inpath '/user/itv004068/data.csv' into table datatab1;select * from datatab1;"

ERROR

=================
Updated Lab Folks
=================
cd
echo 1,zeyo>data.csv
echo 2,vish>>data.csv

hadoop fs -put data.csv /user/itv004068/

hive -e "SET hive.metastore.warehouse.dir = /user/itv004068/warehouse;create database if not exists itv004068;use itv004068;drop table datatab1;
create table datatab1 (id int,name string) row format delimited fields terminated by ',';load data  inpath '/user/itv004068/data.csv' into table datatab1;select * from datatab1;"

insert overwrite local directory '/home/carter/staging' row format delimited fields terminated by ',' select * from hugetable;

==========
16-10-2022
==========


==============
Cloudera Folks
==============

cd
echo 1,sai>data.csv
echo 2,zeyo>>data.csv
hadoop fs -mkdir /user/cloudera/ddir
hadoop fs -put -f data.csv /user/cloudera/ddir/
hadoop fs -mkdir /user/cloudera/rdir

hive

create table ltab(id int,name string) row format delimited fields terminated by ',' location '/user/cloudera/ddir';

select * from ltab;

create table rtab(id int,name string) row format delimited fields terminated by ',' location '/user/cloudera/rdir';

insert into rtab select * from ltab where id>1;

select * from rtab;

!hadoop fs -ls /user/cloudera/rdir;
!hadoop fs -cat /user/cloudera/rdir/*;

=========
Lab Folks
=========

cd
echo 1,sai>data.csv
echo 2,zeyo>>data.csv
hadoop fs -mkdir /user/itv004068/ddir
hadoop fs -put -f data.csv /user/itv004068/ddir/
hadoop fs -mkdir /user/itv004068/rdir





hive -e "SET hive.metastore.warehouse.dir = /user/itv004068/warehouse;create database if not exists itv004068;use itv004068;
create table ltab(id int,name string) row format delimited fields terminated by ',' location '/user/itv004068/ddir';select * from ltab;create table rtab(id int,name string) row format delimited fields terminated by ',' location '/user/itv004068/rdir';insert into rtab select * from ltab where id>1;select * from rtab;*"

hadoop fs -ls /user/itv004068/rdir
hadoop fs -cat /user/itv004068/rdir/

==============
Cloudera Folks
==============

cd
echo 1,sai>data.csv
echo 2,zeyo>>data.csv
hadoop fs -mkdir /user/cloudera/mdir
hadoop fs -mkdir /user/cloudera/edir
hadoop fs -put data.csv /user/cloudera/mdir
hadoop fs -put data.csv /user/cloudera/edir





hive

create table mtab(id int,name string) row format delimited fields terminated by ',' location '/user/cloudera/mdir';

select * from mtab;

create external table etab(id int,name string) row format delimited fields terminated by ',' location '/user/cloudera/edir';

select * from etab;

!hadoop fs -ls /user/cloudera/;
drop table mtab;
drop table etab;
!hadoop fs -ls /user/cloudera/;


=========
Lab Folks
=========

cd
echo 1,sai>data.csv
echo 2,zeyo>>data.csv
hadoop fs -mkdir /user/itv004068/mdir
hadoop fs -mkdir /user/itv004068/edir
hadoop fs -put data.csv /user/itv004068/mdir
hadoop fs -put data.csv /user/itv004068/edir

hive -e "SET hive.metastore.warehouse.dir = /user/itv004068/warehouse;create database if not exists itv004068;use itv004068;create table mtab(id int,name string) row format delimited fields terminated by ',' location '/user/itv004068/mdir';select * from mtab;create external table etab(id int,name string) row format delimited fields terminated by ',' location '/user/itv004068/edir';select * from etab;drop table mtab;drop table etab;"


hadoop fs -ls /user/itv004068/
hadoop fs -ls /user/itv004068/

Thank you for your business! We look forward to working with you again.


========
Task 1 ----
========
Cloudera Folks

cd 
echo 1,sai>data.csv
echo 2,zeyo>>data.csv
hadoop fs -mkdir /user/cloudera/emdir
hadoop fs -put data.csv /user/cloudera/emdir


hive

create database tcheck;
use tcheck;

create  table mtab1(id int,name string) row format delimited fields terminated by ',' location '/user/cloudera/emdir';

create external table etab1(id int,name string) row format delimited fields terminated by ',' location '/user/cloudera/emdir';

drop table etab1;
select * from mtab1 ;    ----> u will see the data

create external table etab1(id int,name string) row format delimited fields terminated by ',' location '/user/cloudera/emdir';

drop table mtab1;
select * from etab1 ;    ----> u will not see the data

!hadoop fs -ls /user/cloudera/; 	-----> U will not see the directory emdir

=========
Lab Folks
=========

cd 
echo 1,sai>data.csv
echo 2,zeyo>>data.csv
hadoop fs -mkdir /user/itv004068/emdir
hadoop fs -put data.csv /user/itv004068/emdir


hive -e "SET hive.metastore.warehouse.dir = /user/<itv004068>/warehouse;create database if not exists itv004068;use itv004068;
create  table mtab1(id int,name string) row format delimited fields terminated by ',' location '/user/itv004068/emdir';create external table etab1(id int,name string) row format delimited fields terminated by ',' location '/user/itv004068/emdir';drop table etab1;select * from mtab1 ;create external table etab1(id int,name string) row format delimited fields terminated by ',' location '/user/itv004068/emdir'; drop table mtab1;select * from etab1 ;"


!hadoop fs -ls /user/itv004068/;    -----> U will not see the directory emdir

============
SQL Scenario
============

=======
Task 2 ------
=======

mysql -uroot -pcloudera

create database prac1;
use prac1;


create table mtab (id int,name varchar(100),amount int);
insert into mtab values(1,'zeyo',40);
insert into mtab values(2,'hema',40);
insert into mtab values(3,'ravi',5);
insert into mtab values(4,'vasu',4);
insert into mtab values(5,'vani',7);
insert into mtab values(6,'rani',1);
insert into mtab values(7,'rita',4);
insert into mtab values(8,'riya',4);
insert into mtab values(9,'raj',4);
insert into mtab values(10,'siji',3);
insert into mtab values(11,'roni',5);
insert into mtab values(12,'visu',5);


select count(1) from mtab;
select count(1) from mtab where id>3;
select sum(amount) as total from mtab;




=========
Lab Folks
=========

mysql --host=zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com --user=root --password=Aditya908

create database itv004068;
use itv004068;

create table mtab4 (id int,name varchar(100),amount int);
insert into mtab4 values(1,'zeyo',40);
insert into mtab4 values(2,'hema',40);
insert into mtab4 values(3,'ravi',5);
insert into mtab4 values(4,'vasu',4);
insert into mtab4 values(5,'vani',7);
insert into mtab4 values(6,'rani',1);
insert into mtab4 values(7,'rita',4);
insert into mtab4 values(8,'riya',4);
insert into mtab4 values(9,'raj',4);
insert into mtab4 values(10,'siji',3);
insert into mtab4 values(11,'roni',5);
insert into mtab4 values(12,'visu',5);
select * from mtab4;

select count(1) from mtab4;
select count(1) from mtab4 where id>3;
select sum(amount) as total from mtab4;

==========
22-10-2022
==========

=================
Cloudera Folks -- static load
=================
cd
echo 1,Sai,I>INDTxns.csv
echo 2,zeyo,I>>INDTxns.csv
echo 3,Hema,K>UKTxnx.csv
echo 4,ravi,K>>UKTxnx.csv
echo 5,Jai,S>USTxns.csv
echo 6,Swathi,S>>USTxns.csv



hive

create database if not exists pdb;
use pdb;
create table parttab(id int,name string,chk string) partitioned by (country string) row format delimited fields terminated by ',' location '/user/cloudera/pdir';


load data local inpath '/home/cloudera/INDTxns.csv' into table parttab partition(country='INDIA');
load data local inpath '/home/cloudera/USTxns.csv' into table parttab partition(country='USA');
load data local inpath '/home/cloudera/UKTxnx.csv' into table parttab partition(country='UK');

select * from parttab;

!hadoop fs -ls /user/cloudera/pdir/;

!hadoop fs -rmr /user/cloudera/pdir/*;
=================
lab Folks -- Static Load
=================
echo 1,Sai,I>INDTxns.csv
echo 2,zeyo,I>>INDTxns.csv
echo 3,Hema,K>UKTxnx.csv
echo 4,ravi,K>>UKTxnx.csv
echo 5,Jai,S>USTxns.csv
echo 6,Swathi,S>>USTxns.csv



hive -e "set hive.metastore.warehouse.dir=/user/itv004068/warehouse;create database if not exists itv004068;use itv004068;drop table parttab;create table parttab(id int,name string,chk string) partitioned by (country string) row format delimited fields terminated by ',' location '/user/itv004068/pdir';load data local inpath '/home/itv004068/INDTxns.csv' into table parttab partition(country='INDIA');load data local inpath '/home/itv004068/USTxns.csv' into table parttab partition(country='USA');load data local inpath '/home/itv004068/UKTxnx.csv' into table parttab partition(country='UK')"


hadoop fs -ls /user/itv004068/pdir
hadoop fs -ls /user/itv004068/pdir/country=INDIA

=================
Cloudera Folks -- static insert
=================
cd

echo 1,Sai,I,IND>allcountry.csv
echo 2,zeyo,I,IND>>allcountry.csv
echo 3,Hema,K,UK>>allcountry.csv
echo 4,Gomathi,K,UK>>allcountry.csv
echo 5,Jai,S,US>>allcountry.csv
echo 6,Swathi,S,US>>allcountry.csv

hive

create database if not exists pdb;
use pdb;
create table sitab(id int,name string,chk string) partitioned by (country string) row format delimited fields terminated by ',' location '/user/cloudera/sidir';

create table srctab(id int,name string,chk string,country string) row format delimited fields terminated by ',' location '/user/cloudera/sdir';

load data local inpath '/home/cloudera/allcountry.csv' into table srctab;


insert into sitab partition(country='USA') select id,name,chk from srctab where country='US';

!hadoop fs -ls /user/cloudera/sdir;
=================
Lab Folks -- static insert
=================

cd

echo 1,Sai,I,IND>allcountry.csv
echo 2,zeyo,I,IND>>allcountry.csv
echo 3,Hema,K,UK>>allcountry.csv
echo 4,Gomathi,K,UK>>allcountry.csv
echo 5,Jai,S,US>>allcountry.csv
echo 6,Swathi,S,US>>allcountry.csv

hive -e "set hive.metastore.warehouse.dir=/user/itv004068/warehouse;create database if not exists itv004068;use itv004068;drop table sitab;create table sitab(id int,name string,chk string) partitioned by (country string) row format delimited fields terminated by ',' location '/user/itv004068/sidir';create table srctab(id int,name string,chk string,country string) row format delimited fields terminated by ',' location '/user/itv004068/sdir';load data local inpath '/home/itv004068/allcountry.csv' into table srctab;insert into sitab partition(country='USA') select id,name,chk from srctab where country='US';"

hadoop fs -ls /user/itv004068/sdir


==========
23-10-2022
==========


=================
Cloudera Folks
=================
cd

echo 1,Sai,I,IND>allcountry.csv
echo 2,zeyo,I,IND>>allcountry.csv
echo 3,Hema,K,UK>>allcountry.csv
echo 4,Gomathi,K,UK>>allcountry.csv
echo 5,Jai,S,US>>allcountry.csv
echo 6,Swathi,S,US>>allcountry.csv

hive

create database if not exists pdb;
use pdb;
create table dyntab(id int,name string,chk string) partitioned by (country string) row format delimited fields terminated by ',' location '/user/cloudera/dyndir';

create table sttab(id int,name string,chk string,country string) row format delimited fields terminated by ',' location '/user/cloudera/stdir';

load data local inpath '/home/cloudera/allcountry.csv' into table sttab;

set hive.exec.dynamic.partition.mode=nonstrict;
insert into dyntab partition(country) select id,name,chk,country from sttab;


=================
Lab Folks
=================
cd

echo 1,Sai,I,IND>allcountry.csv
echo 2,zeyo,I,IND>>allcountry.csv
echo 3,Hema,K,UK>>allcountry.csv
echo 4,Gomathi,K,UK>>allcountry.csv
echo 5,Jai,S,US>>allcountry.csv
echo 6,Swathi,S,US>>allcountry.csv


hive -e "set hive.metastore.warehouse.dir=/user/itv004068/warehouse;create database if not exists itv004068;use itv004068;create table dyntab(id int,name string,chk string) partitioned by (country string) row format delimited fields terminated by ',' location '/user/itv004068/dyndir';create table sttab(id int,name string,chk string,country string) row format delimited fields terminated by ',' location '/user/itv004068/stdir';load data local inpath '/home/itv004068/allcountry.csv' into table sttab;set hive.exec.dynamic.partition.mode=nonstrict;insert into dyntab partition(country) select id,name,chk,country from sttab;"


hadoop fs -ls /user/itv004068/dyndir


====================================
Verify and read data Cloudera Folks
====================================

hadoop fs -ls /user/cloudera/dyndir
hadoop fs -ls /user/cloudera/dyndir/country=IND
hadoop fs -ls /user/cloudera/dyndir/country=US
hadoop fs -ls /user/cloudera/dyndir/country=UK
hadoop fs -cat /user/cloudera/dyndir/country=IND/*
hadoop fs -cat /user/cloudera/dyndir/country=US/*
hadoop fs -cat /user/cloudera/dyndir/country=UK/*


====================================
Verify and read data <itv004068> Folks
====================================

hadoop fs -ls /user/itv004068/dyndir
hadoop fs -ls /user/itv004068/dyndir/country=IND
hadoop fs -ls /user/itv004068/dyndir/country=US
hadoop fs -ls /user/itv004068/dyndir/country=UK
hadoop fs -cat /user/itv004068/dyndir/country=IND/*
hadoop fs -cat /user/itv004068/dyndir/country=US/*
hadoop fs -cat /user/itv004068/dyndir/country=UK/*

==========================
Task  1 --- Sub partitions
==========================

==================
Sub partitions-- Cloudera Folks
==================

============
Task 1 -----
============

cd

echo 1,Sai,I,IND,cash>allc.csv
echo 2,zeyo,I,IND,credit>>allc.csv
echo 3,Hema,K,UK,cash>>allc.csv
echo 4,Gomathi,K,UK,credit>>allc.csv
echo 5,Jai,S,US,cash>>allc.csv
echo 6,Swathi,S,US,credit>>allc.csv
echo 7,Sai,I,IND,credit>>allc.csv
echo 8,zeyo,I,IND,cash>>allc.csv
echo 9,Hema,K,UK,credit>>allc.csv
echo 10,Gomathi,K,UK,cash>>allc.csv
echo 11,Jai,S,US,credit>>allc.csv
echo 12,Swathi,S,US,cash>>allc.csv

create table srcs(id int,name string,chk string,country string,spendby string) row format delimited fields terminated by ',' location '/user/cloudera/srcd';

load data local inpath '/home/cloudera/allc.csv' into table srcs;


create table tars(id int,name string,chk string) partitioned by (country string,spendby string) row format delimited fields terminated by ',' location '/user/cloudera/tard';

set hive.exec.dynamic.partition.mode=nonstrict;

insert into tars partition (country,spendby) select id,name,chk,country,spendby from srcs;

!hadoop fs -ls /user/cloudera/tard;




==================
Sub partitions-- Lab Folks
==================



cd

echo 1,Sai,I,IND,cash>allc.csv
echo 2,zeyo,I,IND,credit>>allc.csv
echo 3,Hema,K,UK,cash>>allc.csv
echo 4,Gomathi,K,UK,credit>>allc.csv
echo 5,Jai,S,US,cash>>allc.csv
echo 6,Swathi,S,US,credit>>allc.csv
echo 7,Sai,I,IND,credit>>allc.csv
echo 8,zeyo,I,IND,cash>>allc.csv
echo 9,Hema,K,UK,credit>>allc.csv
echo 10,Gomathi,K,UK,cash>>allc.csv
echo 11,Jai,S,US,credit>>allc.csv
echo 12,Swathi,S,US,cash>>allc.csv



hive -e "set hive.metastore.warehouse.dir=/user/itv004068/warehouse;set hive.exec.dynamic.partition.mode=nonstrict;create database if not exists itv004068;use itv004068;create table srcs(id int,name string,chk string,country string,spendby string) row format delimited fields terminated by ',' location '/user/itv004068/srcd';load data local inpath '/home/itv004068/allc.csv' into table srcs;create table tars(id int,name string,chk string) partitioned by (country string,spendby string) row format delimited fields terminated by ',' location '/user/itv004068/tard';insert into tars partition (country,spendby) select id,name,chk,country,spendby from srcs;"

hadoop fs -ls /user/itv004068/tard

============
SQL Scenario
============

=============
Task 2 ------
=============

mysql -uroot -pcloudera

create database prac2;
use prac2;


create table mtab (id int,name varchar(100),amount int);
insert into mtab values(1,'zeyo',40);
insert into mtab values(2,'hema',40);
insert into mtab values(3,'ravi',5);
insert into mtab values(4,'vasu',4);
insert into mtab values(5,'vani',7);
insert into mtab values(6,'rani',1);
insert into mtab values(7,'rita',4);
insert into mtab values(8,'riya',4);
insert into mtab values(9,'raj',4);
insert into mtab values(10,'siji',3);
insert into mtab values(11,'roni',5);
insert into mtab values(12,'visu',5);

select *,current_date from mtab;
select *,current_timestamp from mtab;
create table mtabnew as select * from mtab where id>5;
select * from mtabnew;
delete from mtabnew;
select * from mtabnew;
insert into mtabnew select * from mtab where id<5;
select * from mtabnew;






=========
Lab Folks
=========

mysql --host=zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com --user=root --password=Aditya908

create database itv004068;
use itv004068;

drop table mtab4;

create table mtab4 (id int,name varchar(100),amount int);
insert into mtab4 values(1,'zeyo',40);
insert into mtab4 values(2,'hema',40);
insert into mtab4 values(3,'ravi',5);
insert into mtab4 values(4,'vasu',4);
insert into mtab4 values(5,'vani',7);
insert into mtab4 values(6,'rani',1);
insert into mtab4 values(7,'rita',4);
insert into mtab4 values(8,'riya',4);
insert into mtab4 values(9,'raj',4);
insert into mtab4 values(10,'siji',3);
insert into mtab4 values(11,'roni',5);
insert into mtab4 values(12,'visu',5);
select * from mtab4;

select *,current_date from mtab;
select *,current_timestamp from mtab;
create table mtabnew as select * from mtab where id>5;
select * from mtabnew;
delete from mtabnew;
select * from mtabnew;
insert into mtabnew select * from mtab where id<5;
select * from mtabnew;






==========
29-10-2022
==========

hadoop@35.154.148.13


zeyovmnew.ppk-key


=============
Cloudera avro
=============

mysql -uroot -pcloudera

create database dataa;
use dataa;

drop table atab;
create table atab(id int,name varchar(100),amount int);
insert into atab values(1,'rajesh',40);
insert into atab values(2,'vishnu',10);
insert into atab values(3,'rani',60);
select * from atab;

quit;

mkdir avd

cd avd
rm -rf *
sqoop import --connect jdbc:mysql://localhost/dataa --username root --password cloudera --table atab --m 1  --delete-target-dir --target-dir /user/cloudera/adir --as-avrodatafile

hadoop fs -put -f /home/cloudera/avd/atab.avsc /user/cloudera/

hive -e """

create database if not exists adb;
use adb;

drop table atab;

create external table atab ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe' STORED AS AVRO LOCATION '/user/cloudera/adir'  TBLPROPERTIES ('avro.schema.url'='/user/cloudera/atab.avsc'); 

select * from atab """


mysql -uroot -pcloudera
use dataa;
alter table atab drop column name;
insert into atab values(4,90);
insert into atab values(5,20);
select * from atab;

quit

sqoop import --connect jdbc:mysql://localhost/dataa --username root --password cloudera --table atab --m 1 --target-dir /user/cloudera/adir --as-avrodatafile --incremental append --check-column id --last-value 3

hive -e "select * from adb.atab"




========
Lab avro
========

mysql --host=zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com --user=root --password=Aditya908

create database  if not exists  itv004068;
use itv004068;


drop table atab;
create table atab(id int,name varchar(100),amount int);
insert into atab values(1,'rajesh',40);
insert into atab values(2,'vishnu',10);
insert into atab values(3,'rani',60);
select * from atab;

quit;

mkdir avd

cd avd
rm -rf *


sqoop import -Dmapreduce.job.user.classpath.first=true --connect jdbc:mysql://zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com/itv004068 --username root --password Aditya908  --table atab --m 1  --delete-target-dir --target-dir /user/itv004068/adir --as-avrodatafile

hadoop fs -put -f /home/itv004068/avd/atab.avsc /user/itv004068/

hive -e """

create database if not exists itv004068;
use itv004068;

drop table atab;

create external table atab ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe' STORED AS AVRO LOCATION '/user/itv004068/adir'  TBLPROPERTIES ('avro.schema.url'='/user/itv004068/atab.avsc');

select * from atab """


mysql --host=zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com --user=root --password=Aditya908

create database if not exists itv004068;
use itv004068;

alter table atab drop column name;
insert into atab values(4,90);
insert into atab values(5,20);
select * from atab;

quit

sqoop import -Dmapreduce.job.user.classpath.first=true --connect jdbc:mysql://zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com/itv004068 --username root --password Aditya908  --table atab --m 1  --target-dir /user/itv004068/adir --as-avrodatafile --incremental append --check-column id --last-value 3

hive -e "select * from itv004068.atab"












hive -e "set hive.metastore.warehouse.dir=/user/itv004068/warehouse;
create database if not exists itv004068;
use itv004068;
create table dyntab1(id int,name string,chk string) partitioned by (country string) row format delimited fields terminated by ',' location '/user/itv004068/dyndir';

create table sttab1(id int,name string,chk string,country string) row format delimited fields terminated by ',' location '/user/itv004068/stdir';
load data local inpath '/home/itv004068/allcountry.csv' into table sttab1;
set hive.exec.dynamic.partition.mode=nonstrict;

insert into dyntab1 partition(country) select id,name,chk,country from sttab1;"



Cloudera Folks Project


===============
Go to Mysql 
===============

mysql -uroot -pcloudera

Create  database if not exists prodb;
use prodb;

select * from customer_total;

create table customer_src(id int(10),username varchar(100),sub_port varchar(100),host varchar(100),date_time varchar(100),hit_count_val_1 varchar(100),hit_count_val_2 varchar(100),hit_count_val_3 varchar(100),timezone varchar(100),method varchar(100),`procedure` varchar(100),value varchar(100),sub_product varchar(100),web_info varchar(100),status_code varchar(100));


insert into customer_src select * From customer_total where id>=301 and id<=330;

quit
=============================
Edge Node
=============================

echo -n cloudera>/home/cloudera/passfile
rm -rf /home/cloudera/avsrcdir
mkdir /home/cloudera/avsrcdir
cd /home/cloudera/avsrcdir

sqoop job --delete inpjob
sqoop job --create inpjob -- import --connect jdbc:mysql://localhost/prodb --username root --password-file file:///home/cloudera/passfile -m 1 --table customer_src  --target-dir /user/cloudera/customer_stage_loc --incremental append --check-column id --last-value 0 --as-avrodatafile
sqoop job --list
sqoop job --exec inpjob

hadoop fs -mkdir /user/cloudera/avscdirpro
hadoop fs -put /home/cloudera/avsrcdir/customer_src.avsc /user/cloudera/avscdirpro


====================================
Hive shell
====================================

hive
create database prodb;
use prodb;

create  table customer_src   ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe' STORED AS AVRO LOCATION '/user/cloudera/customer_stage_loc'  TBLPROPERTIES ('avro.schema.url'='/user/cloudera/avscdirpro/customer_src.avsc');

select * from customer_src;





Lab Folks

===============
Go to Mysql 
===============


mysql -u nyse_user -h ms.itversity.com -pitversity

use nyse_export;

create table customer_src_itv004068(id int(10),username varchar(100),sub_port varchar(100),host varchar(100),date_time varchar(100),hit_count_val_1 varchar(100),hit_count_val_2 varchar(100),hit_count_val_3 varchar(100),timezone varchar(100),method varchar(100),`procedure` varchar(100),value varchar(100),sub_product varchar(100),web_info varchar(100),status_code varchar(100));


insert into customer_src_itv004068 select * From customer_total where id>=301 and id<=330;

quit
=============================
Edge Node
=============================

echo -n itversity>/home/itv004068/passfile
rm -rf /home/itv004068/avsrcdir
mkdir /home/itv004068/avsrcdir
cd /home/itv004068/avsrcdir

sqoop import -Dmapreduce.job.user.classpath.first=true --connect jdbc:mysql://ms.itversity.com/nyse_export --username nyse_user --password-file file:///home/itv004068/passfile -m 1 --table customer_src_itv004068 --target-dir /user/itv004068/customer_stage_loc --incremental append --check-column id --last-value 0 --as-avrodatafile


hadoop fs -mkdir /user/itv004068/avscdirpro
hadoop fs -put /home/itv004068/avsrcdir/customer_src_itv004068.avsc /user/itv004068/avscdirpro


====================================
Hive shell
====================================

hive
SET hive.metastore.warehouse.dir = /user/itv004068/warehouse;
create database prodb_itv004068;
use prodb_itv004068;

create  table customer_src   ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe' STORED AS AVRO LOCATION '/user/itv004068/customer_stage_loc'  TBLPROPERTIES ('avro.schema.url'='/user/itv004068/avscdirpro/customer_src_itv004068.avsc');

select * from customer_src;   === U will see the data

1) Verify you have customer_total table
2) create customer_src table like customer_total
3) Insert into customser_src from customer_total where id>300 and id<350
4) create a password file your edge Node == (echo -n cloudera>passfile)
5)Create a directory in Edge Node 
6) Go inside that directory
7) create sqoop job for customer_src table with id last value 0 along with password file
8)Execute the Job
9) Verify whether data got imported
10) AVSC file should have generated locally-- Copy that to HDFS
11) Go inside Hive shell and create database prodb;
12) Now create Hive table on top of Raw data with AVSC FILE as reference



===========
05-11-2022
===========

Downloads

Windows ----- java installation

https://drive.google.com/file/d/1vGKX-r6SzT6ND-NQ0co6vKUD61Xd2_zl/view?usp=sharing

Windows ----- Scala IDE

http://downloads.typesafe.com/scalaide-pack/4.7.0-vfinal-oxygen-212-20170929/scala-SDK-4.7.0-vfinal-2.12-win32.win32.x86_64.zip

Windows ----- Intellij Download

https://www.jetbrains.com/idea/download/download-thanks.html?platform=windows&code=IIC

Windows  ---- WinUtils Download

https://github.com/steveloughran/winutils/raw/master/hadoop-2.7.1/bin/winutils.exe

Spark Download ----MAC/Windows/Ubuntu

https://archive.apache.org/dist/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.6.tgz


MAC Users --- Intellij Download

https://www.jetbrains.com/idea/download/download-thanks.html?platform=mac&code=IIC


Ubuntu Users -- Intellij Download

https://www.jetbrains.com/idea/download/download-thanks.html?platform=linux&code=IIC


==========
06-11-2022
==========

Task 1 --- Print Hello world in your Eclipse / Intellij


Try Eclipse video First.. If not working
Straight away go ahead with IntellIj

Eclipse

https://youtu.be/aLe9Yjm_1Jw

Intellij  (Ubuntu/MAC)

https://youtu.be/_2jC-I1EKz4


Task 2 ---- You have to create 5 Projects Like that --- Eclipse/ Intellij




(Optional)

Task 3 ---- In the Project. Do the insert statement


insert into projdb.customer_target_tab partition (current_day,year,month,day) select id, username,sub_port,host,date_time,hit_count_val_1,hit_count_val_2,hit_count_val_3,timezone,method,procedure,value,sub_product,web_info,status_code,current_date,year(from_unixtime(unix_timestamp(date_time,'dd/MMM/yyyy:HH:mm:ss Z'),'yyyy-MM-dd')) , MONTH(from_unixtime(unix_timestamp(date_time,'dd/MMM/yyyy:HH:mm:ss Z'),'yyyy-MM-dd')) , DAY(from_unixtime(unix_timestamp(date_time,'dd/MMM/yyyy:HH:mm:ss Z'),'yyyy-MM-dd'))from customer_src where not(upper(web_info) like'%JAKARTA%');

!hadoop fs -ls /user/cloudera/customer_target_tab;
!hadoop fs -ls /user/cloudera/customer_target_tab/current_day=2022-07-29;
!hadoop fs -ls /user/cloudera/customer_target_tab/current_day=2022-07-29/year=2011;





set hive.exec.max.dynamic.partitions=1000;
set hive.exec.dynamic.partition.mode=nonstrict;

insert into prodb_itv004068.customer_target_tab partition (current_day,year,month,day) select id, username,sub_port,host,date_time,hit_count_val_1,hit_count_val_2,hit_count_val_3,timezone,method,'dummy' as proc,value,sub_product,web_info,status_code,current_date,year(from_unixtime(unix_timestamp(date_time,'dd/MMM/yyyy:HH:mm:ss Z'),'yyyy-MM-dd')) , MONTH(from_unixtime(unix_timestamp(date_time,'dd/MMM/yyyy:HH:mm:ss Z'),'yyyy-MM-dd')) , DAY(from_unixtime(unix_timestamp(date_time,'dd/MMM/yyyy:HH:mm:ss Z'),'yyyy-MM-dd'))from customer_src where not(upper(web_info) like'%JAKARTA%');


!hadoop fs -ls /user/itv004068/customer_target_tab;







Task 4 ---- (optional) ---- What is diff between. RowNumer,Rank and DenseRank



==========
12-11-2022
==========
Gonna be crucial one .. Having your eclipse/Intellij on or below URL. ( anyone )

https://www.tutorialspoint.com/compile_scala_online.php
https://www.jdoodle.com/compile-scala-online/
https://onecompiler.com/scala

[10:23 AM, 11/12/2022] Sai Aditya Big Data Mentor: Mail --

Boss --- Sai Hope you received the Laptop

I already kept a small data all.csv in your windows Laptop

C:/data/all.csv

As a beginner . First Can you read this data using spark in eclipse and print it


Sai --- Zaharia , Can you please help me. How to read this data file and show it through Spark


Zaharia --- Sai, In spark you can read data using two Methods


textFile Method 
csv Method



Sai --- Zaharia, Where,how Can I utilize this method


Zaharia -- You can see or call this methods from two classes

textFile   ------> SparkContext Class
csv        ------> SparkSession Class

Sai   --  Zaharia, Where how ,we can see SparkContext and SparkSession Class

Zaharia --

textFile  ------> SparkContext    ----> spark-core.jar
csv       ------> SparkSession    ----> spark-sql.jar


Sai  --- Got good Understanding

How can you use this jars,class,method to read a file 



Add the jars
Initialize class
Call Method to read a file



Zaharia ---- Its 5 Step process

Open Eclipse ,create project with Scala Nature   --- Done
Add Spark Jars to the project (spark-core,spark-sql) --- Done
Call the classes SparkContext and SparkSession
Inside Code Give powers of SparkContext and SparkSession to a variable
Call the Methods from that variable


package pack

object obj {

def main(args:Array[String]):Unit={
    
    
    println("========Started=======")
    
    val a = 2
    
    println(a)
    
    val b = "zeyo"
    
    println(b)
    
    
  }



package pack

object obj {
  
  def main(args:Array[String]):Unit={
    
    
    println("========Started=======")
    
    val a = 2
    
    println(a)
    
    val b = "zeyo"
    
    println(b)
    
    
  }
  
  
}


https://www.tutorialspoint.com/compile_scala_online.php
https://www.jdoodle.com/compile-scala-online/
https://onecompiler.com/scala


package pack

object project7 {
  
  def main(args:Array[String]):Unit={
    
    println("=======Started=========")
    
    val a=2
    
    println(a)
    
    val b="zeyo"
    
    println(b)
    
    val lis = List(1,2,3,4)
    
    println(lis)
    
    println("======indv print=====")
    
    lis.foreach(println)
  
  }

}




Task 1 -----

 def main(args:Array[String]):Unit={
    
    println("========Started=======")
    
    val a = 2
    
    println(a)
    
    val b = "zeyo"
    
    println(b)
    
     val lis = List(1,2,3,4,5)
     
     println(lis)
     
     println("=======raw list====")
     
     lis.foreach(println)
     val plis = lis.filter( x => x>2)
     
     println("=====processed List===")
     
     plis.foreach(println)
     
  }
  
  
  
  ==========
  13-11-2022
  ==========
  
  
  -------
  Task1
  -------
  package pack

  object project9 {
  
  def main(args:Array[String]):Unit={
    
   println("======Started=======")
   
   val lis = List(4,7,21,50)
   
   println("======raw list======")
   
   println(lis)
   
   val proclis = lis.filter ( x => x < 20)
   
   println("=====proc List=====")
   
   println(proclis)
     
    
  }
  
}



 -------
  Task2
 -------
 
 package pack

object project10 {
  
  def main(args:Array[String]):Unit={
    
   println("======Started=======")
   
   println
   
   val lis = List(1,2,3,4)
   
   println("======raw list======")
   
   lis.foreach(println)
   
   val mullis = lis.map(x => x*2)
   
   println("=====proc List=====")
   
   mullis.foreach(println)
     
    
  }
  
}


-------
Task3
-------

package pack

object obj {
  
  def main(args:Array[String]):Unit={
    
    println("========Started=======")
    
    val listr = List("zeyobron","analytics","zeyo")
    
    println("=====raw List====")
    
    listr.foreach(println)
    
    val mapstr = listr.map( x  =>  x.concat( ",sai" ))
    
     println("=====concat List====")
    
    mapstr.foreach(println)
    
    val repstr = listr.map( x => x.replace("zeyo", "tera"))
    
     println("=====replace List====")
    
    repstr.foreach(println)
   
         
  }

}


-----
Task4
-----
package pack

object project11 {
  
  def main(args:Array[String]):Unit={
    
    println("========Started=======")
    
    val listr = List("zeyobron","analytics","zeyo")
    
    println("=====raw List====")
    
    listr.foreach(println)
    
    val mapstr = listr.map( x  =>  x.concat( ",siva" ))
    
     println("=====concat List====")
    
    mapstr.foreach(println)
    
    val repstr = listr.map( x => x.replace("zeyo", "tera"))
    
     println("=====replace List====")
    
    repstr.foreach(println)
   
         
  }

}



-------
Task 1 --- 
-------
Task 1 -- Solution


package pack

object obj {
  
  def main(args:Array[String]):Unit={
    
    println("========Started=======")
    
    val lisstr= List( "A~B"  ,  "C~D"  ,  "E~F"   )
    
    println("=====raw List====")
    
    lisstr.foreach(println)

    val flatstr = lisstr.flatMap( x  =>  x.split("~") )
    
    println("======flat List ====")
    
    flatstr.foreach(println)
    
  }

}



Complete Flat Map


Task 2 -----

Cloudera Folks

mysql -uroot -pcloudera

create database prac5;
use prac5;


create table mtab (id int,name varchar(100),amount int);
insert into mtab values(10,'zeyo',40);
insert into mtab values(20,'hema',40);
insert into mtab values(30,'ravi',5);
insert into mtab values(40,'vasu',4);

select * , row_number() from mtab;


Lab Folks

mysql --host=zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com --user=root --password=Aditya908

create database itv004068;
use itv004068;

create table mtab (id int,name varchar(100),amount int);
insert into mtab values(10,'zeyo',40);
insert into mtab values(20,'hema',40);
insert into mtab values(30,'ravi',5);
insert into mtab values(40,'vasu',4);

select * , row_number() from mtab;



Task 2 --- Solution ( *Lab and Cloudera )*

--------------
Cloudera Folks
--------------

mysql -uroot -pcloudera

create database prac5;
use prac5;


create table mtab (id int,name varchar(100),amount int);
insert into mtab values(10,'zeyo',40);
insert into mtab values(20,'hema',40);
insert into mtab values(30,'ravi',5);
insert into mtab values(40,'vasu',4);

set @row_number := 0;
select *,(@row_number:=@row_number + 1) AS num  from mtab;

---------
Lab Folks
---------

mysql --host=zeyod2.czgbk94pezaa.ap-south-1.rds.amazonaws.com --user=root --password=Aditya908

create database itv004068;
use itv004068;

create table mtab (id int,name varchar(100),amount int);
insert into mtab values(10,'zeyo',40);
insert into mtab values(20,'hema',40);
insert into mtab values(30,'ravi',5);
insert into mtab values(40,'vasu',4);

set @row_number := 0;
select *,(@row_number:=@row_number + 1) AS num  from mtab;


Optional Task ---- 



val liststr = List("Amazon-Jeff-America",
	           "Microsoft-BillGates-America",
		   "TCS-TATA-india",
		    "Reliance-Ambani-india"
			)

Filter elements Contains "india"
From that result Flatten with -
From that Result replace "india" with "local"
From that Result contact ",zeyo"





package pack

object obj {

	def main(args:Array[String]):Unit={

			println("========Started=======")

			val liststr = List("Amazon-Jeff-America",
					              "Microsoft-BillGates-America",
					              "TCS-TATA-india",
					              "Reliance-Ambani-india"
					                )

			println("=====raw List====")

			liststr.foreach(println)

			val filstr = liststr.filter( x => x.contains("india"))
			
			println("=====filter List====")

			filstr.foreach(println)
			
			println("=====flatmap List====")
			
			val flatdata = filstr.flatMap( x => x.split("-"))
			
			flatdata.foreach(println)
			
			
			
			println("=====replace List====")
			
			val repdata = flatdata.map( x => x.replace("india","local"))
			
			repdata.foreach(println)
			
			
			println("===concat list =====")
			
			
			val mapdata = repdata.map( x => x.concat(",sai"))
			
			mapdata.foreach(println)
			

	}

}


============
19-11-2022
============

package pack


object obj {

	def main(args:Array[String]):Unit={


			println("Hello World")

			println("========Started=======")


			val liststr = List("Amazon-Jeff-America",
					"Microsoft-BillGates-America",
					"TCS-TATA-india",
					"Reliance-Ambani-india"
					)

			println
			println("========raw List=======")
			liststr.foreach(println)
			println
			//Filter elements Contains "india"


			val indlis = liststr.filter( x =>x.contains("india"))
			println("========India List=======")
			indlis.foreach(println)
			println




			//From that result Flatten with -


			val flatdata = indlis.flatMap( x => x.split("-"))
			println("========flat List=======")
			flatdata.foreach(println)
			println



			//From that Result replace "india" with "local"


			val repdata = flatdata.map( x => x.replace("india","local"))
			println("========repdata List=======")
			repdata.foreach(println)
			println




			//From that Result contact ",zeyo"

			val mapdata = repdata.map( x => x + ",zeyo")
			println("========mapdata List=======")
			mapdata.foreach(println)
			println


	}


}



val liststr=
			            List(
					            "State->TamilNadu~City->Chennai",
					            "State->Karnataka~City->Bangalore",
					            "State->Telangana~City->Hyderabad"
					          )
							  
							  
							  
							  
https://www.tutorialspoint.com/compile_scala_online.php
https://www.jdoodle.com/compile-scala-online/
https://onecompiler.com/scala




========raw list=======

State->TamilNadu~City->Chennai
State->Karnataka~City->Bangalore
State->Telangana~City->Hyderabad

========flatdata list=======

State->TamilNadu
City->Chennai
State->Karnataka
City->Bangalore
State->Telangana
City->Hyderabad

========statedata list=======

State->TamilNadu
State->Karnataka
State->Telangana

========citydata list=======

City->Chennai
City->Bangalore
City->Hyderabad

========finalstate list=======

TamilNadu
Karnataka
Telangana

========finalcity list=======

Chennai
Bangalore
Hyderabad




val liststr=
			            List(
					            "State->TamilNadu~City->Chennai",
					            "State->Karnataka~City->Bangalore",
					            "State->Telangana~City->Hyderabad"
					          )
					
			println("========raw list=======")
			println
			liststr.foreach(println)
			
			val flatdata = liststr.flatMap( x=>x.split("~"))

			println("========flatdata list=======")
			flatdata.foreach(println)
			println		
			
			val statedata = flatdata.filter( x => x.contains("State"))
			println("========statedata list=======")
			println
			statedata.foreach(println)
			
			val citydata = flatdata.filter( x => x.contains("City"))
			println("========citydata list=======")
			println
			citydata.foreach(println)
			
			
			val finalstate = statedata.map( x => x.replace("State->",""))

			println("========finalstate list=======")
			println
			finalstate.foreach(println)
			
			val finalcity = citydata.map( x => x.replace("City->",""))
			
			println("========finalcity list=======")
			println
			finalcity.foreach(println)
			
			
			
			
Add Jars to the Project
Give import Statements
Initialize Conf and Context
Read the File and Process it

package pack


import org.apache.spark.SparkContext
import org.apache.spark.SparkConf


object obj {

	def main(args:Array[String]):Unit={


			val conf = new SparkConf().setAppName("First").setMaster("local[*]")
			
			val sc = new SparkContext(conf)
			sc.setLogLevel("ERROR")
			
			
			
			val data= sc.textFile("file:///C:/data/scdata.txt")
			
			println("===== Raw Rdd=======")
			println
			data.foreach(println)
			println
			
			
			val flatdata= data.flatMap( x => x.split("~"))
			
			println("===== flatdata Rdd=======")
			println
			flatdata.foreach(println)
			println
			
			
			
			val statedata = flatdata.filter( x => x.toLowerCase().contains("state"))
			
			println("===== statedata Rdd=======")
			println
			statedata.foreach(println)
			println
			
			
			val citydata = flatdata.filter( x => x.contains("City"))
			
			println("===== citydata Rdd=======")
			println
			citydata.foreach(println)
			println
			
			
			
			val finalstate = statedata.map( x => x.replace("State->", ""))
			
      println("===== finalstate Rdd=======")
			println
			finalstate.foreach(println)
			println
			
			
			
			val finalcity = citydata.map( x => x.replace("City->", ""))
			
      println("===== finalcity Rdd=======")
			println
			finalcity.foreach(println)
			println
			
			
			finalstate.coalesce(1).saveAsTextFile("file:///C:/data/statedata")
			
			
			
			
			
			
			
			
			
			
			
	}


}



val liststr= List(
		"BigData-Spark-Hive",
		"Spark-Hadoop-Hive",
		"Sqoop-Hive-Spark",
		"Sqoop-BD-Hive"
		)

Expected Result

Tech->BigData Trainer->Sai
Tech->Spark Trainer->Sai
Tech->Hive Trainer->Sai
Tech->Hadoop Trainer->Sai
Tech->Sqoop Trainer->Sai
Tech->BigData Trainer->Sai

FlatMap with -
Remove duplicates using distinct
Prefix "Tech->" and suffix " Trainer->Sai"
Replace BD with BigData



===============
package pack

import org.apache.spark.SparkContext

import org.apache.spark.SparkConf

object project13 {
  def main(args:Array[String]):Unit={

val liststr= List(
		"BigData-Spark-Hive",
		"Spark-Hadoop-Hive",
		"Sqoop-Hive-Spark",
		"Sqoop-BD-Hive"
		)
			
		println("====== Raw List=====")
		
		liststr.foreach(println)
		
		println
		
		val flatlis = liststr.flatMap(x => x.split("-")).distinct
		
		println("====== Flattened List=====")
    
    flatlis.foreach(println)
    
    println
    
    val concatlis = flatlis.map(x => "Tech->" + x + "Trainer->Sai")
		
		println("====== Prefix & Suffix List=====")
    
    concatlis.foreach(println)
    
    println
    
    val finallis = concatlis.map(x => x.replace("BD","BigData"))
    
    println("====== Final List=====")
    
    finallis.foreach(println)
    
    println
			
			}


}

==============



host --ms.itversity.com
username -- hr_user
password -- itversity


To connect mysql

mysql -u hr_user -h ms.itversity.com -pitversity


DB to use

use hr_export;





===========
20-11-2022
===========
First read this Data  Done
Filter the Rows which has length>200 Done
Flatten the data with ,Done
Remove hyphon (-) from all the flatten Rows Done
Concat ,zeyo for each string ,Done
Write the results to a file

package pack

import org.apache.spark._

object obj {
	def main(args:Array[String]):Unit={

			val conf = new SparkConf().setAppName("first").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val data = sc.textFile("file:///C:/data/usdata.csv")

					println("=====Raw data==== ")
					data.take(10).foreach(println)
					println
					
					val fildata = data.filter( x => x.length() > 200)
					println("=====fildata data==== ")
					fildata.foreach(println)
					println
			
					val flatdata = fildata.flatMap( x => x.split(","))
					println("=====flatdata data==== ")
					flatdata.foreach(println)
					println
					
					val repdata = flatdata.map( x => x.replace("-",""))
					println("=====repdata data==== ")
					repdata.foreach(println)
					println
					
					val condata = repdata.map( x => x+ ",zeyo")
					println("=====condata data==== ")
					condata.foreach(println)
					println
	}
}


For rdd every row is a element
Column based processing is possible by default
Mapsplit
define case class
impose case class
Filter with Column itself
Dataframe conversion ( schemardd.toDF() )
df.write.parquet("")


package pack
import org.apache.spark._
import org.apache.spark.sql.SparkSession

object obj {
  case class zeyoschema(id:String, category:String,product:String,mode:String)
  
	def main(args:Array[String]):Unit={
			val conf = new SparkConf().setAppName("first").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")
					
					val spark = SparkSession.builder().getOrCreate()
					import spark.implicits._
					val data = sc.textFile("file:///C:/data/datatxns.txt")
					println("===raw data===")
					println
					data.foreach(println)
					println
					val gymdata = data.filter( x => x.contains("Gymnastics"))
					
					println("===Row gymdata ===")
					println
					gymdata.foreach(println)
					println

					val mapsplit = data.map( x => x.split(",")) 
					val schemardd = mapsplit.map( x => zeyoschema(x(0),x(1),x(2),x(3))) 
					val finalfilter = schemardd.filter( x   =>  x.product.contains("Gymnastics")) 
					
					println("===Column gymdata ===")
					finalfilter.foreach(println)
					println
					val df = finalfilter.toDF()
					println("==Dataframe ===")
					df.show()
	}
}


===========
Task 1 ---
===========
Read datatxns.txt
do the map split
create a case class (id,category,product,mode)
Impose case class
Filter product contains Gymnastics and id>20

Task 2 (Optional) ----


Cloudera

mysql -uroot -pcloudera

create database scen,
use scen;


CREATE TABLE sales_table (  
    Employee_Name VARCHAR(45) NOT NULL,  
    Year INT NOT NULL,  
    Country VARCHAR(45) NOT NULL,  
    Product VARCHAR(45) NOT NULL,  
    Sale DECIMAL(12,2) NOT NULL,  
    PRIMARY KEY(Employee_Name, Year)    
);  



INSERT INTO sales_table VALUES  
('Stephen', 2017, 'India', 'Laptop', 10000),    
('Stephen', 2018, 'India', 'Laptop', 15000),    
('Stephen', 2019, 'India', 'TV', 20000),    
('Bob', 2017, 'US', 'Computer', 15000),    
('Bob', 2018, 'US', 'Computer', 10000),    
('Bob', 2019, 'US', 'TV', 20000),    
('Mandy', 2017, 'Canada', 'Mobile', 20000),    
('Mandy', 2018, 'Canada', 'Calculator', 1500),    
('Mandy', 2019, 'Canada', 'Mobile', 25000);  


select * from sales_table;

==================
Find the Total sale of each employee
==================

 select employee_name,sum(Sale) as total from sales_table group by Employee_Name;

==================
Find the Total sale of each employee for each product
==================

select employee_name,sum(Sale),product as total from sales_table group by Employee_Name,product;

==================
Find the Maximum sale by an each employee
==================

select employee_name,max(Sale) as max_sale  from sales_table group by employee_name;

==================
Find the average of sale by an each employee
==================

select employee_name,avg(Sale) as max_sale  from sales_table group by employee_name;




===============
lab
===============

mysql -u nyse_user -h ms.itversity.com -pitversity


use nyse_export;




CREATE TABLE sales_table_itv004068 (  
    Employee_Name VARCHAR(45) NOT NULL,  
    Year INT NOT NULL,  
    Country VARCHAR(45) NOT NULL,  
    Product VARCHAR(45) NOT NULL,  
    Sale DECIMAL(12,2) NOT NULL,  
    PRIMARY KEY(Employee_Name, Year)    
);  



INSERT INTO sales_table_itv004068 VALUES  
('Stephen', 2017, 'India', 'Laptop', 10000),    
('Stephen', 2018, 'India', 'Laptop', 15000),    
('Stephen', 2019, 'India', 'TV', 20000),    
('Bob', 2017, 'US', 'Computer', 15000),    
('Bob', 2018, 'US', 'Computer', 10000),    
('Bob', 2019, 'US', 'TV', 20000),    
('Mandy', 2017, 'Canada', 'Mobile', 20000),    
('Mandy', 2018, 'Canada', 'Calculator', 1500),    
('Mandy', 2019, 'Canada', 'Mobile', 25000);  



==================
Find the Total sale of each employee
==================

 select employee_name,sum(Sale) as total from sales_table_itv004068 group by Employee_Name;

==================
Find the Total sale of each employee for each product
==================
select employee_name,sum(Sale),product as total from sales_table_itv004068 group by Employee_Name,product;

==================
Find the Maximum sale by an each employee
==================
select employee_name,max(Sale) as max_sale  from sales_table_itv004068 group by employee_name;

==================
Find the average of sale by an each employee
==================

select employee_name,avg(Sale) as max_sale  from sales_table_itv004068 group by employee_name;




========
task1
========
object project13 {
  
 def main(args:Array[String]):Unit={
   
   val conf = new SparkConf().setAppName("first").setMaster("local[*]")
     
      val sc = new SparkContext(conf)
      
     sc.setLogLevel("ERROR")
      
      val data = sc.textFile("file:///D:/data/usdata.csv")
  
			println("=====Raw data======")
			
			data.take(10).foreach(println)
			
			println
			
			val fildata = data.filter( x => x.length()>200)
			
			println("=====fildata data======")
			
			fildata.foreach(println)
			
			println
			
			val flatdata = fildata.flatMap( x => x.split(","))
			
			println("=====flatdata data======")
			
			flatdata.foreach(println)
			
			println
			
			val repdata = flatdata.map( x => x.replace("-",""))
			
			println("======repdata data======")
			
			repdata.foreach(println)
			
			println
			
			val condata = repdata.map( x => x + ",zeyo")
			
			println("======condata data======")
			
			condata.foreach(println)
			
			println
				
			}


}

==========================
package pack
import org.apache.spark._
import org.apache.spark.sql.SparkSession

object project13 {
  case class zeyoschema(id:String, category:String,product:String,mode:String)
  
	def main(args:Array[String]):Unit={
			val conf = new SparkConf().setAppName("first").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")
					
					val spark = SparkSession.builder().getOrCreate()
					import spark.implicits._
					val data = sc.textFile("file:///D:/data/datatxns.txt")
					println("===raw data===")
					println
					data.foreach(println)
					println
					val gymdata = data.filter( x => x.contains("Gymnastics"))
					
					println("===Row gymdata ===")
					println
					gymdata.foreach(println)
					println

					val mapsplit = data.map( x => x.split(",")) 
					val schemardd = mapsplit.map( x => zeyoschema(x(0),x(1),x(2),x(3))) 
					val finalfilter = schemardd.filter( x   =>  x.product.contains("Gymnastics")) 
					
					println("===Column gymdata ===")
					finalfilter.foreach(println)
					println
					val df = finalfilter.toDF()
					println("==Dataframe ===")
					df.show()
	}
}

======================
Assignment Task 1
======================
package pack
import org.apache.spark._
import org.apache.spark.sql.SparkSession

object project13 {
  case class zeyoschema(id:String, category:String,product:String,mode:String)
  
	def main(args:Array[String]):Unit={
			val conf = new SparkConf().setAppName("first").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")
					
					val spark = SparkSession.builder().getOrCreate()
					import spark.implicits._
					val data = sc.textFile("file:///D:/data/datatxns.txt")
					println("===raw data===")
					println
					data.foreach(println)
					println
					val gymdata = data.filter( x => x.contains("Gymnastics"))
					
					println("===Row gymdata ===")
					println
					gymdata.foreach(println)
					println

					val mapsplit = data.map( x => x.split(",")) 
					val schemardd = mapsplit.map( x => zeyoschema(x(0),x(1),x(2),x(3))) 
					val finalfilter = schemardd.filter( x   =>  x.product.contains("Gymnastics") && x.id.toInt>20) 
					
					println
	}
}

=============
26-11-2022
=============

package pack

import org.apache.spark._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._
import org.apache.spark.sql._

object obj {

case class zeyoschema(
		id:Int, 
		category:String,
		product:String,
		mode:String
		)

def main(args:Array[String]):Unit={
		val conf = new SparkConf().setAppName("first").setMaster("local[*]")
				val sc = new SparkContext(conf)
				sc.setLogLevel("ERROR")

				val spark = SparkSession.builder.getOrCreate()
				import spark.implicits._


				// Using Schema rdd

				println("====Schema rdd output=====")
				println

				val data = sc.textFile("file:///C:/data/datatxns.txt")

				data.foreach(println)


				val mapsplit = data.map( x => x.split(","))

				val schemardd = mapsplit.map( x => zeyoschema(x(0).toInt,x(1),x(2),x(3)))

				val filterdata = schemardd.filter( x=>

				x.product.contains("Gymnastics") &&
				x.id.toInt>20



						)
				println
				filterdata.foreach(println)

				val df = filterdata.toDF()

				println
				df.show()


				//df.write.parquet("file:///C:/data/schemawrite")




				println("=====row rdd output====")


				val rdata = sc.textFile("file:///C:/data/datatxns.txt")

				println
				rdata.foreach(println)

				val rmapsplit = rdata.map( x => x.split(","))


				val rowrdd = rmapsplit.map( x => Row(x(0),x(1),x(2),x(3)))

				val rfilterdata = rowrdd.filter( x =>

				x(2).toString().contains("Gymnastics") &&
				x(0).toString().toInt>20

						)

				println
				rfilterdata.foreach(println)



				val rschema = StructType(Array(
						StructField("id",StringType),
						StructField("category",StringType),
						StructField("product",StringType),
						StructField("mode", StringType)
						))


			val rdf = spark.createDataFrame(rfilterdata,rschema)
			
			println
			rdf.show()
			
			rdf.write.parquet("file:///C:/data/rowwrite")



  }
}
==========
Task 1 --
==========
Read datatxns.txt as dataframe with csv format

val df = spark
.read
.format("csv")
.load("file:///C:/data/datatxns.txt")
df.show()       
df.createOrReplaceTempView("tab")
val finaldf = spark.sql("select * from tab where _c1='Gymnastics'")
finaldf.show()

==========
Task 2 ---
==========

val df2 = spark
.read
.format("json")
.load("file:///C:/data/devices.json")
df2.show()       
df2.createOrReplaceTempView("jsontab")
val finaldf2 = spark.sql("select * from jsontab where humidity>60")
finaldf2.show()


=============
27-11-2022
=============

package pack

import org.apache.spark._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._
import org.apache.spark.sql._

object project13 {

case class zeyoschema(
		id:Int, 
		category:String,
		product:String,
		mode:String
		)

def main(args:Array[String]):Unit={
		val conf = new SparkConf().setAppName("first").setMaster("local[*]")
				val sc = new SparkContext(conf)
				sc.setLogLevel("ERROR")

				val spark = SparkSession.builder.getOrCreate()
				import spark.implicits._




				val df = spark
				.read
				.format("csv")
				.option("header","true")
				.load("file:///D:/data/usdata.csv")

				println("============================csvdf===========================")
				println
				df.show(5)     
				println


				val jsondf = spark
				.read
				.format("json")
				.load("file:///D:/data/devices.json")

				println("======================jsondf=========================")
				println
				jsondf.show(5)     
				println

				val orcdf = spark
				.read
				.format("orc")
				.load("file:///D:/data/data.orc")

				println("============================orcdf=====================")
				println
				orcdf.show(5)     
				println



				val parquetdf = spark
				.read
				.format("parquet")
				.load("file:///D:/data/part.parquet")

				println("============================parquetdf=============")
				println
				parquetdf.show(5)     
				println
				
				val avrodf = spark
				.read
				.format("avro")
				.load("file:///D:/data/part.avro")

				println("============================avrodf=============")
				println
				avrodf.show(5)     
				println


    }
}



Task 2



package pack

import org.apache.spark._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._
import org.apache.spark.sql._

object obj {

case class zeyoschema(
		id:Int, 
		category:String,
		product:String,
		mode:String
		)

def main(args:Array[String]):Unit={
		val conf = new SparkConf().setAppName("first").setMaster("local[*]")
				val sc = new SparkContext(conf)
				sc.setLogLevel("ERROR")

				val spark = SparkSession.builder.getOrCreate()
				import spark.implicits._




				val df = spark
				.read
				.format("csv")
				.option("header","true")
				.load("file:///C:/semidata/usdata.csv")

				println("============================csvdf===========================")
				println
				df.sho


package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._

object obj {


	def main(args:Array[String]):Unit={


			val conf = new SparkConf().setMaster("local[*]").setAppName("first")

					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder.getOrCreate()
					import spark.implicits._

					
					val sqldf = spark
					            .read
					            .format("jdbc")
					            .option("url","jdbc:mysql://database-1.cwv8krqq8b83.ap-south-1.rds.amazonaws.com/zeyodb")
					            .option("driver","com.mysql.jdbc.Driver")
					            .option("dbtable","cashdata")
					            .option("user","root")
					            .option("password","Aditya908")
					            .load
					       
					
				sqldf.show()	
				
					

	}
}


Task 1 ---

Read devices.json as json file
df.createOrReplaceTempView("jsontab")
val finaldf=spark.sql("select *,row_number() from jsontab")


package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._

object project13 {


	def main(args:Array[String]):Unit={


			val conf = new SparkConf().setMaster("local[*]").setAppName("first")

					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder.getOrCreate()
					import spark.implicits._

					
					val df = spark.read.format(source = "json").load(path="file:///D:/data/devices.json")
					
					df.createOrReplaceTempView(viewName = "jsontab")
					
					val finaldf = spark.sql("select * from jsontab")
				
					finaldf.show()	
				
	}
}


Task 2 --- (If you have winutils solve it in doubts Session,
EXIT CODE =1156424535-SOMETHING)
Read usdata.csv as csv
Write as parquet
Write as json
write as orc
write as avro (add the Jar)

package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._

object project13 {


	def main(args:Array[String]):Unit={


			val conf = new SparkConf().setMaster("local[*]").setAppName("first")

					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder.getOrCreate()
					import spark.implicits._

					
					val usdatadf = spark
					               .read
					               .format("csv")
					               .option("header","true")
					               .load("file:///D:/data/usdata.csv")
					println("=========csvdf=========")
					println
					usdatadf.show()
					println
					
				val parquetdf = usdatadf
					               .write
					               .format("parquet")
					               .option("header","true")
					               .save("file:///D:/write data/usdata_parquet")
					println("=========usdata_parquet file Write as parquet=========")
					println
					usdatadf.show()
					println	
					
				val jsondf = usdatadf
					               .write
					               .format("json")
					               .option("header","true")
					               .save("file:///D:/write data/usdata_json")
					println("=========usdata_json file Write as json=========")
					println
					usdatadf.show()
					println
					
				val orcdf = usdatadf
					               .write
					               .format("orc")
					               .option("header","true")
					               .save("file:///D:/write data/usdata_orc")
					println("=========usdata_orc file Write as orc=========")
					println
					usdatadf.show()
					println
					
				val avrodf = usdatadf
					               .write
					               .format("avro")
					               .option("header","true")
					               .save("file:///D:/write data/usdata_avro")
					println("=========usdata_avro file Write as avro=========")
					println
					usdatadf.show()
					println
	}
}




Optional Task ----


SQOOP --


HIVE  --

=============
03-12-2022
=============

Test each Mode separately

val df = spark.read.format("json").load("file:///C:/data/devices.json")

val finaldf = df.filter("humidity>40")

==========
Run it  ---- It would be successful
==========
finaldf.write.format("csv").mode("error").save("file:///C:/data/dhumid")


==========
Run it  --- It will through Error
==========

finaldf.write.format("csv").mode("error").save("file:///C:/data/dhumid")

==========
Run it  --- it will append the data
==========

finaldf.write.format("csv").mode("append").save("file:///C:/data/dhumid")




==========
Run it  --- it will overwrite the data
==========

finaldf.write.format("csv").mode("overwrite").save("file:///C:/data/dhumid")

==========
Run it  --- it will ignore the data
==========

finaldf.write.format("csv").mode("ignore").save("file:///C:/data/dhumid")





package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._

object obj {


	def main(args:Array[String]):Unit={


			val conf = new SparkConf().setMaster("local[*]").setAppName("first")

					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder.getOrCreate()
					import spark.implicits._

					val df = spark
					          .read
					          .format("xml")
					          .option("rowtag","book")
					          .load("file:///C:/data/book.xml")
					
					df.show()
					
					
	}
}


===========
04-12-2022
===========

package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._

object project13 {


	def main(args:Array[String]):Unit={


			val conf = new SparkConf().setMaster("local[*]").setAppName("first")
					.set("fs.s3a.access.key","AKIASSGV4N5GIVX6MPVE")
					.set("fs.s3a.secret.key","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")



					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder.config(conf).getOrCreate()

					import spark.implicits._

					
					
					val df = spark
					.read
					.format("json")
					.load("s3a://zdevb/cashdata")	

					df.show()


					val finaldf = df.filter("category='Team Sports'")


					finaldf.show()


	}
}

======
Task 2
======
package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._

object obj {


	def main(args:Array[String]):Unit={


			val conf = new SparkConf().setMaster("local[*]").setAppName("first")



					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")


					val spark = SparkSession.builder.config(conf).getOrCreate()

					import spark.implicits._

					val dschema = StructType(Array(
							StructField("id",StringType,true),
							StructField("name",StringType,true),
							StructField("chk",StringType,true),
							StructField("country", StringType, true)
							))


					val df = spark
					.read
					.format("csv")
					.schema(dschema)
					.load("file:///C:/data/allcountry1.csv")

					df.show() 
					
					
					
					df.write
					      .format("csv")
					      .partitionBy("country")
					      .mode("overwrite")
					      .save("file:///C:/data/countrypart")
					
					

	}
}



Task 1 ----

Read dt.txt as a csv with header true
Create a temp View tdf
Find a way to design a query if spendby =cash then 1 else 0 as status


Task 2 ----

For the same tempview
Find the total sum of each category using SQL and order by it with Category

Task 3 ----


Create an sql in such a way that instead of tdate,I should have year in the column with column name as year -- Split operation


Task 4 ----

Using dataframe

df.dropDuplicates("category").show()


package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._

object project13 {


	def main(args:Array[String]):Unit={


			val conf = new SparkConf().setMaster("local[*]").setAppName("first")

			          
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")


					val spark = SparkSession.builder.config(conf).getOrCreate()

					import spark.implicits._

					
					val df = spark
					          .read
					          .format("csv")
					          .option("header","true")
					          .load("file:///D:/data/dt.txt")
					
					df.show()
					
					//Task 1
						
					df.createOrReplaceTempView("tdf")
					
					val df1= spark.sql("""select *,case 
					    when spendby='cash' 
					    then 1 
					    else 0 end as status 
					    from tdf""")
					println("========Task 1========")
					df1.show()
					
					
					// Task 2 
						
					val df2 = spark.sql("""select category,
					    sum(amount) as total 
					    from tdf 
					    group by category 
					    order by category""")
					println("========Task 2========")
					df2.show()
					
					
					// Task 3 
					
					val df3 = spark.sql("""select id,
					     split(tdate,'-')[2] as year,
					     amount,
					     category,
					     product,
					     spendby from tdf""")
					println("========Task 3========")
					df3.show()
					
					
				  // Task 4  
				
					
			   val df4= df.dropDuplicates("category")
					
			   println("========Task 4========")
			   df4.show()
					
					
					
	}
}

+--------+----------+------+-----------+-------------+-------+
|      id|     tdate|amount|   category|      product|spendby|
+--------+----------+------+-----------+-------------+-------+
|00000000|06-26-2011|   200|   Exercise|GymnasticsPro|   cash|
|00000001|05-26-2011|   300|   Exercise|Weightlifting| credit|
|00000002|06-01-2011|   100|   Exercise|GymnasticsPro|   cash|
|00000003|06-05-2011|   100| Gymnastics|        Rings| credit|
|00000004|12-17-2011|   300|Team Sports|        Field|   cash|
|00000005|02-14-2011|   200| Gymnastics|         Ring|   cash|
+--------+----------+------+-----------+-------------+-------+

========Task 1========
+--------+----------+------+-----------+-------------+-------+------+
|      id|     tdate|amount|   category|      product|spendby|status|
+--------+----------+------+-----------+-------------+-------+------+
|00000000|06-26-2011|   200|   Exercise|GymnasticsPro|   cash|     1|
|00000001|05-26-2011|   300|   Exercise|Weightlifting| credit|     0|
|00000002|06-01-2011|   100|   Exercise|GymnasticsPro|   cash|     1|
|00000003|06-05-2011|   100| Gymnastics|        Rings| credit|     0|
|00000004|12-17-2011|   300|Team Sports|        Field|   cash|     1|
|00000005|02-14-2011|   200| Gymnastics|         Ring|   cash|     1|
+--------+----------+------+-----------+-------------+-------+------+

========Task 2========
+-----------+-----+
|   category|total|
+-----------+-----+
|   Exercise|600.0|
| Gymnastics|300.0|
|Team Sports|300.0|
+-----------+-----+

========Task 3========
+--------+----+------+-----------+-------------+-------+
|      id|year|amount|   category|      product|spendby|
+--------+----+------+-----------+-------------+-------+
|00000000|2011|   200|   Exercise|GymnasticsPro|   cash|
|00000001|2011|   300|   Exercise|Weightlifting| credit|
|00000002|2011|   100|   Exercise|GymnasticsPro|   cash|
|00000003|2011|   100| Gymnastics|        Rings| credit|
|00000004|2011|   300|Team Sports|        Field|   cash|
|00000005|2011|   200| Gymnastics|         Ring|   cash|
+--------+----+------+-----------+-------------+-------+

========Task 4========
+--------+----------+------+-----------+-------------+-------+
|      id|     tdate|amount|   category|      product|spendby|
+--------+----------+------+-----------+-------------+-------+
|00000003|06-05-2011|   100| Gymnastics|        Rings| credit|
|00000004|12-17-2011|   300|Team Sports|        Field|   cash|
|00000000|06-26-2011|   200|   Exercise|GymnasticsPro|   cash|
+--------+----------+------+-----------+-------------+-------+

==========
10-12-2022
==========

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._

Read dt.txt using header true as csv
df1 == filter spendby=cash
df2 == filter category=Exercise and spendby=cash
df3 == filter category=Exercise or spendby=casht
df4 == filter category =Exercise Gymnastics

===============
package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._

object obj {


	def main(args:Array[String]):Unit={


			val conf = new SparkConf().setMaster("local[*]").setAppName("first")


					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")


					val spark = SparkSession
					.builder
					.config(conf)
					.getOrCreate()

					import spark.implicits._


					val df = spark
			        		.read
					        .format("csv")
					        .option("header","true")
					        .load("file:///C:/data/dt.txt")
					df.show()
					println
					println("===Equals Filter===")
					println
					
					val df1 = df.filter(
					          col("spendby") === "cash"
					        )
					
					df1.show()
					println
					println("===Multi Column and Filter===")
					println
					val df2 = df.filter(
					            col("category")==="Exercise"
					            &&
					            col("spendby")=== "cash"
					        )
					df2.show()        
					
					println
					println("===Multi Column or Filter===")
					println
					val df3 = df.filter(
					            col("category")==="Exercise"
					            ||
					            col("spendby")=== "cash"					
					        )
					df3.show()      
					
					println
					println("===Multi value Filter===")
					println
					val df4 = df.filter(
					                  col("category") isin ("Exercise","Gymnastics")
					                  )
					
					df4.show()
					
					val df7 = df.filter(   
		  ! ( col("category") === "Gymnastics" )    
			)
					df7.show()
					val df8 = df.filter( 
		  !   (	col("category") isin ("Gymnastics" ,"Exercise")  )
		)

					df8.show()



	}
}
==============================


package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._

object project13 {


	def main(args:Array[String]):Unit={


			val conf = new SparkConf().setMaster("local[*]").setAppName("first")


					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")


					val spark = SparkSession
					.builder
					.config(conf)
					.getOrCreate()

					import spark.implicits._


					val df = spark
			        		.read
					        .format("csv")
					        .option("header","true")
					        .load("file:///D:/data/dt.txt")
					df.show()
					println
					println("===Equals Filter===")
					println
					
					val df1 = df.filter(
					          col("spendby") === "cash"
					        )
					
					df1.show()
					println
					println("===Multi Column and Filter===")
					println
					val df2 = df.filter(
					            col("category")==="Exercise"
					            &&
					            col("spendby")=== "cash"
					        )
					df2.show()        
					
					println
					println("===Multi Column or Filter===")
					println
					val df3 = df.filter(
					            col("category")==="Exercise"
					            ||
					            col("spendby")=== "cash"					
					        )
					df3.show()      
					
					println
					println("===Multi value Filter===")
					println
					val df4 = df.filter(
					                  col("category") isin ("Exercise","Gymnastics")
					                  )
					
					df4.show()
					println
					println("===Multi value Filter===")
					println
					val df5 = df.filter(   
		  ! ( col("category") === "Gymnastics" )    
			)
          df5.show()
          
          println
					println("===Multi value Filter===")
					println
          val df6 = df.filter( 
		  !   (	col("category") isin ("Gymnastics" ,"Exercise")  )
		  )

          df6.show()	


	}
}

Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
22/12/10 10:06:27 INFO SparkContext: Running Spark version 2.4.7
22/12/10 10:06:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
22/12/10 10:06:28 INFO SparkContext: Submitted application: first
22/12/10 10:06:28 INFO SecurityManager: Changing view acls to: dell
22/12/10 10:06:28 INFO SecurityManager: Changing modify acls to: dell
22/12/10 10:06:28 INFO SecurityManager: Changing view acls groups to: 
22/12/10 10:06:28 INFO SecurityManager: Changing modify acls groups to: 
22/12/10 10:06:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dell); groups with view permissions: Set(); users  with modify permissions: Set(dell); groups with modify permissions: Set()
22/12/10 10:06:30 INFO Utils: Successfully started service 'sparkDriver' on port 64835.
22/12/10 10:06:30 INFO SparkEnv: Registering MapOutputTracker
22/12/10 10:06:30 INFO SparkEnv: Registering BlockManagerMaster
22/12/10 10:06:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/12/10 10:06:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/12/10 10:06:30 INFO DiskBlockManager: Created local directory at C:\Users\dell\AppData\Local\Temp\blockmgr-6ac07dfa-2709-42f7-8bbc-0a0c4926f257
22/12/10 10:06:30 INFO MemoryStore: MemoryStore started with capacity 866.4 MB
22/12/10 10:06:30 INFO SparkEnv: Registering OutputCommitCoordinator
22/12/10 10:06:30 INFO Utils: Successfully started service 'SparkUI' on port 4040.
22/12/10 10:06:30 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://Dell:4040
22/12/10 10:06:30 INFO Executor: Starting executor ID driver on host localhost
22/12/10 10:06:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 64858.
22/12/10 10:06:30 INFO NettyBlockTransferService: Server created on Dell:64858
22/12/10 10:06:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/12/10 10:06:31 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, Dell, 64858, None)
22/12/10 10:06:31 INFO BlockManagerMasterEndpoint: Registering block manager Dell:64858 with 866.4 MB RAM, BlockManagerId(driver, Dell, 64858, None)
22/12/10 10:06:31 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, Dell, 64858, None)
22/12/10 10:06:31 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, Dell, 64858, None)
+--------+----------+------+-----------+-------------+-------+
|      id|     tdate|amount|   category|      product|spendby|
+--------+----------+------+-----------+-------------+-------+
|00000000|06-26-2011|   200|   Exercise|GymnasticsPro|   cash|
|00000001|05-26-2011|   300|   Exercise|Weightlifting| credit|
|00000002|06-01-2011|   100|   Exercise|GymnasticsPro|   cash|
|00000003|06-05-2011|   100| Gymnastics|        Rings| credit|
|00000004|12-17-2011|   300|Team Sports|        Field|   cash|
|00000005|02-14-2011|   200| Gymnastics|         Ring|   cash|
+--------+----------+------+-----------+-------------+-------+


===Equals Filter===

+--------+----------+------+-----------+-------------+-------+
|      id|     tdate|amount|   category|      product|spendby|
+--------+----------+------+-----------+-------------+-------+
|00000000|06-26-2011|   200|   Exercise|GymnasticsPro|   cash|
|00000002|06-01-2011|   100|   Exercise|GymnasticsPro|   cash|
|00000004|12-17-2011|   300|Team Sports|        Field|   cash|
|00000005|02-14-2011|   200| Gymnastics|         Ring|   cash|
+--------+----------+------+-----------+-------------+-------+


===Multi Column and Filter===

+--------+----------+------+--------+-------------+-------+
|      id|     tdate|amount|category|      product|spendby|
+--------+----------+------+--------+-------------+-------+
|00000000|06-26-2011|   200|Exercise|GymnasticsPro|   cash|
|00000002|06-01-2011|   100|Exercise|GymnasticsPro|   cash|
+--------+----------+------+--------+-------------+-------+


===Multi Column or Filter===

+--------+----------+------+-----------+-------------+-------+
|      id|     tdate|amount|   category|      product|spendby|
+--------+----------+------+-----------+-------------+-------+
|00000000|06-26-2011|   200|   Exercise|GymnasticsPro|   cash|
|00000001|05-26-2011|   300|   Exercise|Weightlifting| credit|
|00000002|06-01-2011|   100|   Exercise|GymnasticsPro|   cash|
|00000004|12-17-2011|   300|Team Sports|        Field|   cash|
|00000005|02-14-2011|   200| Gymnastics|         Ring|   cash|
+--------+----------+------+-----------+-------------+-------+


===Multi value Filter===

+--------+----------+------+----------+-------------+-------+
|      id|     tdate|amount|  category|      product|spendby|
+--------+----------+------+----------+-------------+-------+
|00000000|06-26-2011|   200|  Exercise|GymnasticsPro|   cash|
|00000001|05-26-2011|   300|  Exercise|Weightlifting| credit|
|00000002|06-01-2011|   100|  Exercise|GymnasticsPro|   cash|
|00000003|06-05-2011|   100|Gymnastics|        Rings| credit|
|00000005|02-14-2011|   200|Gymnastics|         Ring|   cash|
+--------+----------+------+----------+-------------+-------+


===Multi value Filter===

+--------+----------+------+-----------+-------------+-------+
|      id|     tdate|amount|   category|      product|spendby|
+--------+----------+------+-----------+-------------+-------+
|00000000|06-26-2011|   200|   Exercise|GymnasticsPro|   cash|
|00000001|05-26-2011|   300|   Exercise|Weightlifting| credit|
|00000002|06-01-2011|   100|   Exercise|GymnasticsPro|   cash|
|00000004|12-17-2011|   300|Team Sports|        Field|   cash|
+--------+----------+------+-----------+-------------+-------+


===Multi value Filter===

+--------+----------+------+-----------+-------+-------+
|      id|     tdate|amount|   category|product|spendby|
+--------+----------+------+-----------+-------+-------+
|00000004|12-17-2011|   300|Team Sports|  Field|   cash|
+--------+----------+------+-----------+-------+-------+

===========================
val df = spark
			        		.read
					        .format("csv")
					        .option("header","true")
					        .load("file:///C:/data/dt.txt")
					df.show()
				
				
					
					val df1 = df.selectExpr(
					                  "id",
					                  "split(tdate,'-')[2] as year",
					                  "amount",
					                  "UPPER(category) as category",
					                  "product",
					                  "spendby",
					                  "case when spendby='cash' then 0 else 1 end as status"
					            )
					
					df1.show()
					df1.printSchema()
======================================

===========================
10-12-2022 Assignment task1
===========================

package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._

object project13 {


	def main(args:Array[String]):Unit={


			val conf = new SparkConf().setMaster("local[*]").setAppName("first")


					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")


					val spark = SparkSession
					.builder
					.config(conf)
					.getOrCreate()

					import spark.implicits._


					val df = spark
			        		.read
					        .format("csv")
					        .option("header","true")
					        .load("file:///D:/data/dt.txt")
					df.show()
				
				
					println("====== Task1 ======") 
					val df1 = df.selectExpr(
					                  "id",
					                  "split (tdate,'-')[2] as year",
					                  "amount",
					                  "UPPER(category) as category",
					                  "product",
					                  "spendby",
					                  "case when spendby='cash' then 0 else 1 end as status"
					            )
					
					df1.show()
					df1.printSchema()
					
					println
					println("==== Change_date_format ====")
					println
					val df2 = df.selectExpr(exprs = "*",
            """date_format
              (to_date(tdate,'MM-dd-yyyy'),
              'yyyy-MM-dd') as Change_date_format"""
            )
					df2.show()
	}
}


+--------+----------+------+-----------+-------------+-------+
|      id|     tdate|amount|   category|      product|spendby|
+--------+----------+------+-----------+-------------+-------+
|00000000|06-26-2011|   200|   Exercise|GymnasticsPro|   cash|
|00000001|05-26-2011|   300|   Exercise|Weightlifting| credit|
|00000002|06-01-2011|   100|   Exercise|GymnasticsPro|   cash|
|00000003|06-05-2011|   100| Gymnastics|        Rings| credit|
|00000004|12-17-2011|   300|Team Sports|        Field|   cash|
|00000005|02-14-2011|   200| Gymnastics|         Ring|   cash|
+--------+----------+------+-----------+-------------+-------+

====== Task1 ======
+--------+----+------+-----------+-------------+-------+------+
|      id|year|amount|   category|      product|spendby|status|
+--------+----+------+-----------+-------------+-------+------+
|00000000|2011|   200|   EXERCISE|GymnasticsPro|   cash|     0|
|00000001|2011|   300|   EXERCISE|Weightlifting| credit|     1|
|00000002|2011|   100|   EXERCISE|GymnasticsPro|   cash|     0|
|00000003|2011|   100| GYMNASTICS|        Rings| credit|     1|
|00000004|2011|   300|TEAM SPORTS|        Field|   cash|     0|
|00000005|2011|   200| GYMNASTICS|         Ring|   cash|     0|
+--------+----+------+-----------+-------------+-------+------+

root
 |-- id: string (nullable = true)
 |-- year: string (nullable = true)
 |-- amount: string (nullable = true)
 |-- category: string (nullable = true)
 |-- product: string (nullable = true)
 |-- spendby: string (nullable = true)
 |-- status: integer (nullable = false)


==== Change_date_format ====

+--------+----------+------+-----------+-------------+-------+------------------+
|      id|     tdate|amount|   category|      product|spendby|Change_date_format|
+--------+----------+------+-----------+-------------+-------+------------------+
|00000000|06-26-2011|   200|   Exercise|GymnasticsPro|   cash|        2011-06-26|
|00000001|05-26-2011|   300|   Exercise|Weightlifting| credit|        2011-05-26|
|00000002|06-01-2011|   100|   Exercise|GymnasticsPro|   cash|        2011-06-01|
|00000003|06-05-2011|   100| Gymnastics|        Rings| credit|        2011-06-05|
|00000004|12-17-2011|   300|Team Sports|        Field|   cash|        2011-12-17|
|00000005|02-14-2011|   200| Gymnastics|         Ring|   cash|        2011-02-14|
+--------+----------+------+-----------+-------------+-------+------------------+


=============
11-12-2022
=============

package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._

object project13 {


	def main(args:Array[String]):Unit={


			val conf = new SparkConf().setMaster("local[*]").setAppName("first")


					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")


					val spark = SparkSession
					.builder
					.config(conf)
					.getOrCreate()

					import spark.implicits._


					val df = spark
			        		.read
					        .format("csv")
					        .option("header","true")
					        .load("file:///D:/data/dt.txt")
					df.show()
				
				
					val df1 = df.withColumn("year",expr("split(tdate,'-')[2]"))
					
					.withColumnRenamed("tdate","year")
					
					.withColumn("status",expr("case when spendby = 'cash' then 1 else 0 end"))
					
					df1.show()
	}
}



+--------+----------+------+-----------+-------------+-------+
|      id|     tdate|amount|   category|      product|spendby|
+--------+----------+------+-----------+-------------+-------+
|00000000|06-26-2011|   200|   Exercise|GymnasticsPro|   cash|
|00000001|05-26-2011|   300|   Exercise|Weightlifting| credit|
|00000002|06-01-2011|   100|   Exercise|GymnasticsPro|   cash|
|00000003|06-05-2011|   100| Gymnastics|        Rings| credit|
|00000004|12-17-2011|   300|Team Sports|        Field|   cash|
|00000005|02-14-2011|   200| Gymnastics|         Ring|   cash|
+--------+----------+------+-----------+-------------+-------+

+--------+----------+------+-----------+-------------+-------+----+------+
|      id|      year|amount|   category|      product|spendby|year|status|
+--------+----------+------+-----------+-------------+-------+----+------+
|00000000|06-26-2011|   200|   Exercise|GymnasticsPro|   cash|2011|     1|
|00000001|05-26-2011|   300|   Exercise|Weightlifting| credit|2011|     0|
|00000002|06-01-2011|   100|   Exercise|GymnasticsPro|   cash|2011|     1|
|00000003|06-05-2011|   100| Gymnastics|        Rings| credit|2011|     0|
|00000004|12-17-2011|   300|Team Sports|        Field|   cash|2011|     1|
|00000005|02-14-2011|   200| Gymnastics|         Ring|   cash|2011|     1|
+--------+----------+------+-----------+-------------+-------+----+------+

=========
Join Code
=========
 val df1 = spark.read.format("csv")
                  .option("header","true")
                  .load("file:///C:/data/j11.csv")
				
				df1.show()
				
				
				val df2 = spark.read.format("csv")
                  .option("header","true")
                  .load("file:///C:/data/j22.csv")
				
				df2.show()



				println
				println("===inner join===")
				println
				
				val innerdf = df1.join(  df2,  Seq("id") , "inner")
				
				
				innerdf.show()
				
				
				println
				println("===left join===")
				println
				
				val leftdf = df1.join(  df2,  Seq("id") , "left")
				
				
				leftdf.show()
				
				
				
				println
				println("===right join===")
				println
				
				val rightdf = df1.join(  df2,  Seq("id") , "right")
				
				
				rightdf.show()
				
				
				
				
				println
				println("===full join===")
				println
				
				val fulldf = df1.join(  df2,  Seq("id") , "full")
				              .orderBy("id")
				
				
				fulldf.show()
				
				
			val df2 = spark.read.format("csv")
                  .option("header","true")
                  .load("file:///C:/data/j22.csv")
				
				df2.show()
				
				
				
				
				val finaldf = df1.join(df2,Seq("id"),"left_anti")
				
				finaldf.show()
				
				
				
				
				
				
				
				
				
				
			/*	val listids = df2
				              .select("id")
				              .rdd
				              .map( x => x.mkString(","))
				              .collect()
				              .toList
				
				println(listids)
				
				
				
				val finaldf = df1.filter( ! (col("id").isin(listids: _*)))
				
				
				finaldf.show()
				*/

===========
Task 1 ---

Practise all the Joins

Task 2 ---

val crossdf=df1.crossJoin(df2)
crossdf.show()



Task 3 --- Optional

Analyse uber case (Dataset and Requirement)


Task 4 ----Optional

 If possible Perform all the joins using SQL tempview
===========




======================
Assignment Tasks 1 & 2
======================

package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._

object project13 {


	def main(args:Array[String]):Unit={


			val conf = new SparkConf().setMaster("local[*]").setAppName("first")


					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")


					val spark = SparkSession
					.builder
					.config(conf)
					.getOrCreate()

					import spark.implicits._


					val df1 = spark
			        		.read
					        .format("csv")
					        .option("header","true")
					        .load("file:///D:/data/j11.csv")
					df1.show()
					
					val df2 = spark
			        		.read
					        .format("csv")
					        .option("header","true")
					        .load("file:///D:/data/j22.csv")
					df2.show()
				  
				
				println
				println("======Left join======")
			  val leftdf = df1.join(df2,Seq("id"),"left")
			  leftdf.show()
			  
			  println
				println("======Inner join======")
			  val innerdf = df1.join(df2,Seq("id"),"inner")
			  innerdf.show()
			  
			  println
				println("======Right join======")
			  val rightdf = df1.join(df2,Seq("id"),"right")
			  rightdf.show()
			  
			  
			  println
				println("======Full join======")
			  val fulldf = df1.join(df2,Seq("id"),"full")
			                .orderBy("id")
			  fulldf.show()
			  
			  println
				println("======Left_anti join======")
			  val finaldf = df1.join(df2,Seq("id"),"left_anti")
			  leftdf.show()
			  
			  println
				println("======Cross join======")
			  val crossdf = df1.crossJoin(df2)
			  crossdf.show()
	
	}
}

Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
22/12/15 11:34:40 INFO SparkContext: Running Spark version 2.4.7
22/12/15 11:34:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
22/12/15 11:34:40 INFO SparkContext: Submitted application: first
22/12/15 11:34:40 INFO SecurityManager: Changing view acls to: dell
22/12/15 11:34:40 INFO SecurityManager: Changing modify acls to: dell
22/12/15 11:34:40 INFO SecurityManager: Changing view acls groups to: 
22/12/15 11:34:40 INFO SecurityManager: Changing modify acls groups to: 
22/12/15 11:34:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dell); groups with view permissions: Set(); users  with modify permissions: Set(dell); groups with modify permissions: Set()
22/12/15 11:34:42 INFO Utils: Successfully started service 'sparkDriver' on port 64445.
22/12/15 11:34:42 INFO SparkEnv: Registering MapOutputTracker
22/12/15 11:34:42 INFO SparkEnv: Registering BlockManagerMaster
22/12/15 11:34:42 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/12/15 11:34:42 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/12/15 11:34:42 INFO DiskBlockManager: Created local directory at C:\Users\dell\AppData\Local\Temp\blockmgr-53f191c0-c919-4e35-ac36-5951567c3bbc
22/12/15 11:34:42 INFO MemoryStore: MemoryStore started with capacity 866.4 MB
22/12/15 11:34:42 INFO SparkEnv: Registering OutputCommitCoordinator
22/12/15 11:34:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.
22/12/15 11:34:42 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://Dell:4040
22/12/15 11:34:42 INFO Executor: Starting executor ID driver on host localhost
22/12/15 11:34:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 64471.
22/12/15 11:34:43 INFO NettyBlockTransferService: Server created on Dell:64471
22/12/15 11:34:43 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/12/15 11:34:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, Dell, 64471, None)
22/12/15 11:34:43 INFO BlockManagerMasterEndpoint: Registering block manager Dell:64471 with 866.4 MB RAM, BlockManagerId(driver, Dell, 64471, None)
22/12/15 11:34:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, Dell, 64471, None)
22/12/15 11:34:43 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, Dell, 64471, None)
+---+----+
| id|name|
+---+----+
|  1| raj|
|  2|ravi|
|  3| sai|
|  5|rani|
+---+----+

+---+-------+
| id|product|
+---+-------+
|  1|cookies|
|  3| mobile|
|  7| laptop|
+---+-------+


======Left join======
+---+----+-------+
| id|name|product|
+---+----+-------+
|  1| raj|cookies|
|  2|ravi|   null|
|  3| sai| mobile|
|  5|rani|   null|
+---+----+-------+


======Inner join======
+---+----+-------+
| id|name|product|
+---+----+-------+
|  1| raj|cookies|
|  3| sai| mobile|
+---+----+-------+


======Right join======
+---+----+-------+
| id|name|product|
+---+----+-------+
|  1| raj|cookies|
|  3| sai| mobile|
|  7|null| laptop|
+---+----+-------+


======Full join======
+---+----+-------+
| id|name|product|
+---+----+-------+
|  1| raj|cookies|
|  2|ravi|   null|
|  3| sai| mobile|
|  5|rani|   null|
|  7|null| laptop|
+---+----+-------+


======Left_anti join======
+---+----+-------+
| id|name|product|
+---+----+-------+
|  1| raj|cookies|
|  2|ravi|   null|
|  3| sai| mobile|
|  5|rani|   null|
+---+----+-------+


======Cross join======
+---+----+---+-------+
| id|name| id|product|
+---+----+---+-------+
|  1| raj|  1|cookies|
|  2|ravi|  1|cookies|
|  3| sai|  1|cookies|
|  5|rani|  1|cookies|
|  1| raj|  3| mobile|
|  2|ravi|  3| mobile|
|  3| sai|  3| mobile|
|  5|rani|  3| mobile|
|  1| raj|  7| laptop|
|  2|ravi|  7| laptop|
|  3| sai|  7| laptop|
|  5|rani|  7| laptop|
+---+----+---+-------+


============
Task3

package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._

object project13 {


	def main(args:Array[String]):Unit={


			val conf = new SparkConf().setMaster("local[*]").setAppName("first")


					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")


					val spark = SparkSession
					.builder
					.config(conf)
					.getOrCreate()

					import spark.implicits._

			  
			  val df = spark.read.format("csv")
			           .option("header","true")
			           .load("file:///D:/data/uber.csv")
			           df.show()
			  
			 df.createOrReplaceTempView("df1temp")
			  
			 val df2 = spark.sql("""select dispatching_base_number,date,
			                 sum(active_vehicles) as AV From df1temp
			                 group by dispatching_base_number,date
			                 order by AV desc""")
			 println("====== Aggregate Query ======")
			 df2.show()
	}
}

Result:
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
22/12/16 02:01:38 INFO SparkContext: Running Spark version 2.4.7
22/12/16 02:01:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
22/12/16 02:01:38 INFO SparkContext: Submitted application: first
22/12/16 02:01:39 INFO SecurityManager: Changing view acls to: dell
22/12/16 02:01:39 INFO SecurityManager: Changing modify acls to: dell
22/12/16 02:01:39 INFO SecurityManager: Changing view acls groups to: 
22/12/16 02:01:39 INFO SecurityManager: Changing modify acls groups to: 
22/12/16 02:01:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dell); groups with view permissions: Set(); users  with modify permissions: Set(dell); groups with modify permissions: Set()
22/12/16 02:01:40 INFO Utils: Successfully started service 'sparkDriver' on port 61756.
22/12/16 02:01:40 INFO SparkEnv: Registering MapOutputTracker
22/12/16 02:01:40 INFO SparkEnv: Registering BlockManagerMaster
22/12/16 02:01:40 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/12/16 02:01:40 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/12/16 02:01:40 INFO DiskBlockManager: Created local directory at C:\Users\dell\AppData\Local\Temp\blockmgr-f61355f2-faca-4463-9ef9-c392899be237
22/12/16 02:01:40 INFO MemoryStore: MemoryStore started with capacity 866.4 MB
22/12/16 02:01:40 INFO SparkEnv: Registering OutputCommitCoordinator
22/12/16 02:01:40 INFO Utils: Successfully started service 'SparkUI' on port 4040.
22/12/16 02:01:40 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://Dell:4040
22/12/16 02:01:40 INFO Executor: Starting executor ID driver on host localhost
22/12/16 02:01:41 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61779.
22/12/16 02:01:41 INFO NettyBlockTransferService: Server created on Dell:61779
22/12/16 02:01:41 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/12/16 02:01:41 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, Dell, 61779, None)
22/12/16 02:01:41 INFO BlockManagerMasterEndpoint: Registering block manager Dell:61779 with 866.4 MB RAM, BlockManagerId(driver, Dell, 61779, None)
22/12/16 02:01:41 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, Dell, 61779, None)
22/12/16 02:01:41 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, Dell, 61779, None)
+-----------------------+--------+---------------+-----+
|dispatching_base_number|    date|active_vehicles|trips|
+-----------------------+--------+---------------+-----+
|                 B02512|1/1/2017|            190| 1132|
|                 B02765|1/1/2017|            225| 1765|
|                 B02764|1/1/2017|           3427|29421|
|                 B02682|1/1/2017|            945| 7679|
|                 B02617|1/1/2017|           1228| 9537|
|                 B02598|1/1/2017|            870| 6903|
|                 B02598|1/2/2017|            785| 4768|
|                 B02617|1/2/2017|           1137| 7065|
|                 B02512|1/2/2017|            175|  875|
|                 B02682|1/2/2017|            890| 5506|
|                 B02765|1/2/2017|            196| 1001|
|                 B02764|1/2/2017|           3147|19974|
|                 B02765|1/3/2017|            201| 1526|
|                 B02617|1/3/2017|           1188|10664|
|                 B02598|1/3/2017|            818| 7432|
|                 B02682|1/3/2017|            915| 8010|
|                 B02512|1/3/2017|            173| 1088|
|                 B02764|1/3/2017|           3215|29729|
|                 B02512|1/4/2017|            147|  791|
|                 B02682|1/4/2017|            812| 5621|
+-----------------------+--------+---------------+-----+
only showing top 20 rows

====== Aggregate Query ======
+-----------------------+---------+------+
|dispatching_base_number|     date|    AV|
+-----------------------+---------+------+
|                 B02764|2/13/2017|4395.0|
|                 B02764|2/20/2017|4384.0|
|                 B02764|2/27/2017|4253.0|
|                 B02764| 2/6/2017|4170.0|
|                 B02764|2/12/2017|4137.0|
|                 B02764|2/14/2017|4129.0|
|                 B02764|1/30/2017|4124.0|
|                 B02764|2/19/2017|4110.0|
|                 B02764|2/26/2017|4101.0|
|                 B02764| 2/5/2017|4093.0|
|                 B02764|1/23/2017|4040.0|
|                 B02764|2/21/2017|3981.0|
|                 B02764|1/16/2017|3975.0|
|                 B02764|2/24/2017|3965.0|
|                 B02764|1/29/2017|3959.0|
|                 B02764|2/28/2017|3952.0|
|                 B02764|1/31/2017|3947.0|
|                 B02764|2/25/2017|3934.0|
|                 B02764|1/22/2017|3889.0|
|                 B02764| 2/4/2017|3856.0|
+-----------------------+---------+------+
only showing top 20 rows


================
17-12-2022
================

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.functions.upper
import org.apache.spark.sql.catalyst.expressions.Upper
-------------------------------------------------------
package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.functions.upper
import org.apache.spark.sql.catalyst.expressions.Upper

import org.apache.spark.sql.functions._

object obj {


	def main(args:Array[String]):Unit={


			val conf = new SparkConf().setMaster("local[*]").setAppName("first")


					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")


					val spark = SparkSession
					.builder
					.config(conf)
					.getOrCreate()

					import spark.implicits._

       
				val df1 = spark.read.format("csv")
                  .option("header","true")
                  .load("file:///C:/data/dt.txt")
				df1.show()
				val finaldf  = df1
				                .groupBy("category")
				                .agg(
				                    sum("amount")
				                    .cast(IntegerType)
				                    .as("total")
				                    ,
				                    count("amount")
				                    .as("cnt")
				                    ) 		
				             
				finaldf.show()
				
				
				val finaldf1  = df1
				                .groupBy("category","spendby")
				                .agg(
				                    sum("amount")
				                    .cast(IntegerType)
				                    .as("total")
				                    ,
				                    count("amount")
				                    .as("cnt")
				                    ) 		
				finaldf1.show()
				
				
			
	}
	
	
	
	
}

---------------------------------------------------------------------
package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.functions.upper
import org.apache.spark.sql.catalyst.expressions.Upper

import org.apache.spark.sql.functions._

object project13 {


	def main(args:Array[String]):Unit={


			val conf = new SparkConf().setMaster("local[*]").setAppName("first")


					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")


					val spark = SparkSession
					.builder
					.config(conf)
					.getOrCreate()

					import spark.implicits._

       
				val df1 = spark.read.format("csv")
                  .option("header","true")
                  .load("file:///D:/data/dt.txt")
				df1.show()
				val finaldf  = df1
				                .groupBy("category")
				                .agg(
				                    sum("amount")
				                    .cast(IntegerType)
				                    .as("total")
				                    ,
				                    count("amount")
				                    .as("cnt")
				                    ) 		
				             
				finaldf.show()
				
				
				val finaldf1  = df1
				                .groupBy("category","spendby")
				                .agg(
				                    sum("amount")
				                    .cast(IntegerType)
				                    .as("total")
				                    ,
				                    count("amount")
				                    .as("cnt")
				                    ) 		
				finaldf1.show()
				
	}
	
	
	
}
Out put
============
+--------+----------+------+-----------+-------------+-------+
|      id|     tdate|amount|   category|      product|spendby|
+--------+----------+------+-----------+-------------+-------+
|00000000|06-26-2011|   200|   Exercise|GymnasticsPro|   cash|
|00000001|05-26-2011|   300|   Exercise|Weightlifting| credit|
|00000002|06-01-2011|   100|   Exercise|GymnasticsPro|   cash|
|00000003|06-05-2011|   100| Gymnastics|        Rings| credit|
|00000004|12-17-2011|   300|Team Sports|        Field|   cash|
|00000005|02-14-2011|   200| Gymnastics|         Ring|   cash|
+--------+----------+------+-----------+-------------+-------+

+-----------+-----+---+
|   category|total|cnt|
+-----------+-----+---+
| Gymnastics|  300|  2|
|Team Sports|  300|  1|
|   Exercise|  600|  3|
+-----------+-----+---+

+-----------+-------+-----+---+
|   category|spendby|total|cnt|
+-----------+-------+-----+---+
| Gymnastics|   cash|  200|  1|
|   Exercise|   cash|  300|  2|
|   Exercise| credit|  300|  1|
|Team Sports|   cash|  300|  1|
| Gymnastics| credit|  100|  1|
+-----------+-------+-----+---+

----------------------------------------------------------------

========================================
----------------------------------------
Revision data

package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.functions.upper
import org.apache.spark.sql.catalyst.expressions.Upper
import org.apache.spark.sql.functions._

object obj {


case class schema(txnno:String,
		txndate:String,
		custno:String,
		amount:String,
		category:String,
		product:String,
		city:String,
		state:String,
		spendby:String)

def main(args:Array[String]):Unit={



		val conf = new SparkConf().setAppName("Revision").setMaster("local[*]")
				val sc = new SparkContext(conf)
				sc.setLogLevel("ERROR")



				val liscol = List("txnno","txndate","custno","amount","category","product","city","state","spendby")









				val spark = SparkSession.builder().getOrCreate()
				import spark.implicits._

				println
				println("======schema df=======")
				println

				val file1 = sc.textFile("file:///C:/data/revdata/file1.txt")

				val gymdata = file1.filter( x => x.contains("Gymnastics") )

				val mapsplit = gymdata.map( x => x.split(","))

				val schemardd = mapsplit.map( x => schema(x(0),x(1),x(2),x(3),x(4),x(5),x(6),x(7),x(8)))

				val filterschemardd = schemardd.filter( x=> x.product.contains("Gymnastics"))

				//filterschemardd.take(10).foreach(println)

				val schemadf = filterschemardd.toDF()

				schemadf.show(5)



				println
				println("====== rowdf======")
				println


				val file2 = sc.textFile("file:///C:/data/revdata/file2.txt")

				val mapsplit1 = file2.map( x => x.split(","))

				val rowrdd = mapsplit1.map( x => Row(x(0),x(1),x(2),x(3),x(4),x(5),x(6),x(7),x(8)))

				val simpleSchema = StructType(Array(
						StructField("txnno",StringType,true),
						StructField("txndate",StringType,true),
						StructField("custno",StringType,true),
						StructField("amount", StringType, true),
						StructField("category", StringType, true),
						StructField("product", StringType, true),
						StructField("city", StringType, true),
						StructField("state", StringType, true),
						StructField("spendby", StringType, true)
						))


				val rowdf = spark.createDataFrame(rowrdd, simpleSchema)

				rowdf.show(5)


				println
				println("====== file 3 csvdf======")
				println




				val csvdf = spark
				.read
				.format("csv")
				.option("header","true")
				.load("file:///C:/data/revdata/file3.txt")


				csvdf.show(5)           




				println
				println("====== file 4 jsondf======")
				println

				val jsondf = spark
				.read
				.format("json")
				.load("file:///C:/data/revdata/file4.json")
				.select(liscol.map(col): _*)



				jsondf.show(5)

				println
				println("====== file 5 parquetdf======")
				println

				val parquetdf = spark
				.read
				.load("file:///C:/data/revdata/file5.parquet")

				parquetdf.show(5)


				println
				println("====== file 6 xmldf======")
				println


				val xmldf = spark
				.read
				.format("xml")
				.option("rowtag","txndata")
				.load("file:///C:/data/revdata/file6")
				.select(liscol.map(col): _*)

				xmldf.show(6)


				println
				println("====== union df======")
				println

				val uniondf = schemadf
				.union(rowdf)
				.union(csvdf)
				.union(jsondf)
				.union(parquetdf)
				.union(xmldf)


				uniondf.show(5)

				println(uniondf.count)


				println
				println("====== proc df======")
				println


				val procdf = uniondf.withColumn("txndate", expr("split(txndate,'-')[2]"))
				.withColumnRenamed("txndate","year")
				.withColumn("status",expr("case when spendby='cash' then 1 else 0 end"))
				.filter( col("txnno") > 50000 )

				procdf.show(5)                   



				println
				println("====== agg df======")
				println



				val aggdf = procdf.groupBy("category")
				.agg(sum("amount").as("total"))

				aggdf.show()


				println
				println("====== write df======")
				println



				/*uniondf
				.write
				.format("avro").mode("append")
				.partitionBy("category")
				.save("file:///C:/data/uniondfrev")*/

				println("====data written ===")





}




}
-----------------------------------------
=========================================

===============
Spark Revision 
===============

package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.functions.upper
import org.apache.spark.sql.catalyst.expressions.Upper
import org.apache.spark.sql.functions._

object project13 {


case class schema(txnno:String,
		txndate:String,
		custno:String,
		amount:String,
		category:String,
		product:String,
		city:String,
		state:String,
		spendby:String)

def main(args:Array[String]):Unit={



		val conf = new SparkConf().setAppName("Revision").setMaster("local[*]")
				val sc = new SparkContext(conf)
				sc.setLogLevel("ERROR")



				val liscol = List("txnno","txndate","custno","amount","category","product","city","state","spendby")


				val spark = SparkSession.builder().getOrCreate()
				import spark.implicits._

				println
				println("======schema df=======")
				println

				val file1 = sc.textFile("file:///D:/data/revdata/file1.txt")

				val gymdata = file1.filter( x => x.contains("Gymnastics") )

				val mapsplit = gymdata.map( x => x.split(","))

				val schemardd = mapsplit.map( x => schema(x(0),x(1),x(2),x(3),x(4),x(5),x(6),x(7),x(8)))

				val filterschemardd = schemardd.filter( x=> x.product.contains("Gymnastics"))

				//filterschemardd.take(10).foreach(println)

				val schemadf = filterschemardd.toDF()

				schemadf.show(5)



				println
				println("====== rowdf======")
				println


				val file2 = sc.textFile("file:///D:/data/revdata/file2.txt")

				val mapsplit1 = file2.map( x => x.split(","))

				val rowrdd = mapsplit1.map( x => Row(x(0),x(1),x(2),x(3),x(4),x(5),x(6),x(7),x(8)))

				val simpleSchema = StructType(Array(
						StructField("txnno",StringType,true),
						StructField("txndate",StringType,true),
						StructField("custno",StringType,true),
						StructField("amount", StringType, true),
						StructField("category", StringType, true),
						StructField("product", StringType, true),
						StructField("city", StringType, true),
						StructField("state", StringType, true),
						StructField("spendby", StringType, true)
						))


				val rowdf = spark.createDataFrame(rowrdd, simpleSchema)

				rowdf.show(5)


				println
				println("====== file 3 csvdf======")
				println


				val csvdf = spark
				.read
				.format("csv")
				.option("header","true")
				.load("file:///D:/data/revdata/file3.txt")

				csvdf.show(5)           




				println
				println("====== file 4 jsondf======")
				println

				val jsondf = spark
				.read
				.format("json")
				.load("file:///D:/data/revdata/file4.json")
				.select(liscol.map(col): _*)


				jsondf.show(5)

			
				println
				println("====== file 5 parquetdf======")
				println

				val parquetdf = spark
				.read
				.load("file:///D:/data/revdata/file5.parquet")

				parquetdf.show(5)


				println
				println("====== file 6 xmldf======")
				println

				val xmldf = spark
				.read
				.format("xml")
				.option("rowtag","txndata")
				.load("file:///D:/data/revdata/file6")
				.select(liscol.map(col): _*)

				xmldf.show(6)


				println
				println("====== union df======")
				println

				val uniondf = schemadf
				.union(rowdf)
				.union(csvdf)
				.union(jsondf)
				.union(parquetdf)
				.union(xmldf)


				uniondf.show(5)

				println(uniondf.count)


				println
				println("====== proc df======")
				println

				val procdf = uniondf.withColumn("txndate", expr("split(txndate,'-')[2]"))
				.withColumnRenamed("txndate","year")
				.withColumn("status",expr("case when spendby='cash' then 1 else 0 end"))
				.filter( col("txnno") > 50000 )

				procdf.show(5)                   



				println
				println("====== agg df======")
				println

				val aggdf = procdf.groupBy("category")
				.agg(sum("amount").as("total"))

				aggdf.show()


				println
				println("====== write df======")
				println



				 uniondf
				.write
				.format("avro").mode("append")
				.partitionBy("category")
				.save("file:///D:/data/uniondfrev")

				println("====data written ====")

				
				val df1 = spark
			        		.read
					        .format("csv")
					        .option("header","true")
					        .load("file:///D:/data/j11.csv")
					df1.show()
					
					val df2 = spark
			        		.read
					        .format("csv")
					        .option("header","true")
					        .load("file:///D:/data/j22.csv")
					df2.show()
				  
				
				println
				println("======Left join======")
			  val leftdf = df1.join(df2,Seq("id"),"left")
			  leftdf.show()
			  
			  println
				println("======Inner join======")
			  val innerdf = df1.join(df2,Seq("id"),"inner")
			  innerdf.show()
			  
			  println
				println("======Right join======")
			  val rightdf = df1.join(df2,Seq("id"),"right")
			  rightdf.show()
			  
			  
			  println
				println("======Full join======")
			  val fulldf = df1.join(df2,Seq("id"),"full")
			                .orderBy("id")
			  fulldf.show()
			  
			  println
				println("======Left_anti join======")
			  val finaldf = df1.join(df2,Seq("id"),"left_anti")
			  leftdf.show()
			  
			  println
				println("======Cross join======")
			  val crossdf = df1.crossJoin(df2)
			  crossdf.show()
			  
			  
			  println
				println("======Left Semi join======")
			  val left_semidf = df1.join(df2,Seq("id"),"left_semi")
			  left_semidf.show()
				
    }


}

======================================
-----------------Out put--------------
======schema df=======

+--------+----------+-------+------+----------+--------------------+----------------+---------+-------+
|   txnno|   txndate| custno|amount|  category|             product|            city|    state|spendby|
+--------+----------+-------+------+----------+--------------------+----------------+---------+-------+
|00000003|06-05-2011|4002199|198.19|Gymnastics|    Gymnastics Rings|       Milwaukee|Wisconsin| credit|
|00000023|05-02-2011|4007596|099.50|Gymnastics|    Gymnastics Rings|     Springfield| Illinois| credit|
|00000082|01-16-2011|4001098|021.23|Gymnastics|    Gymnastics Rings|           Tampa|  Florida|   cash|
|00000087|06-05-2011|4001050|089.56|Gymnastics|     Gymnastics Mats|West Valley City|     Utah| credit|
|00000111|03-05-2011|4000401|173.56|Gymnastics|Gymnastics Protec...|        Portland|   Oregon| credit|
+--------+----------+-------+------+----------+--------------------+----------------+---------+-------+
only showing top 5 rows


====== rowdf======

+--------+----------+-------+------+--------------------+--------------------+----------+-------------+-------+
|   txnno|   txndate| custno|amount|            category|             product|      city|        state|spendby|
+--------+----------+-------+------+--------------------+--------------------+----------+-------------+-------+
|00021035|08-06-2011|4008418|161.62|  Exercise & Fitness|Cardio Machine Ac...|   Gresham|       Oregon| credit|
|00021036|05-20-2011|4001083|162.04|Outdoor Play Equi...|    Outdoor Playsets|Saint Paul|    Minnesota| credit|
|00021037|05-18-2011|4004039|162.40|          Gymnastics|       Pommel Horses|Pittsburgh| Pennsylvania| credit|
|00021038|06-29-2011|4004018|067.71|  Outdoor Recreation|     Riding Scooters|   Gresham|       Oregon| credit|
|00021039|05-16-2011|4008593|147.31|  Outdoor Recreation|   Deck Shuffleboard| Cambridge|Massachusetts| credit|
+--------+----------+-------+------+--------------------+--------------------+----------+-------------+-------+
only showing top 5 rows


====== file 3 csvdf======

+--------+----------+-------+------+--------------+--------------------+-----------+----------+-------+
|   txnno|   txndate| custno|amount|      category|             product|       city|     state|spendby|
+--------+----------+-------+------+--------------+--------------------+-----------+----------+-------+
|00047931|03-22-2011|4000263|041.89|   Team Sports|             Curling|     Dayton|      Ohio| credit|
|00047932|05-03-2011|4001699|169.15|       Jumping|Trampoline Access...|   New York|  New York| credit|
|00047933|06-16-2011|4000001|032.52|Racquet Sports|              Tennis|Los Angeles|California| credit|
|00047934|01-09-2011|4002860|124.31|  Water Sports|             Surfing|   Columbia|  Missouri| credit|
|00047935|12-27-2011|4001211|150.54|       Puzzles|      Jigsaw Puzzles|Los Angeles|California| credit|
+--------+----------+-------+------+--------------+--------------------+-----------+----------+-------+
only showing top 5 rows


====== file 4 jsondf======

+--------+----------+-------+------+------------------+--------------------+----------+----------+-------+
|   txnno|   txndate| custno|amount|          category|             product|      city|     state|spendby|
+--------+----------+-------+------+------------------+--------------------+----------+----------+-------+
|00060747|09-06-2011|4002143|074.68|     Winter Sports|            Sledding|Long Beach|California| credit|
|00060748|09-17-2011|4000219|138.63|           Jumping|         Pogo Sticks| Vancouver|Washington| credit|
|00060749|10-02-2011|4009381|173.42|       Team Sports|             Cricket|   Everett|Washington| credit|
|00060750|02-08-2011|4007813|155.85|Exercise & Fitness|Weightlifting Mac...|   Buffalo|  New York| credit|
|00060751|09-09-2011|4009254|101.17|      Indoor Games|            Foosball|Long Beach|California| credit|
+--------+----------+-------+------+------------------+--------------------+----------+----------+-------+
only showing top 5 rows


====== file 5 parquetdf======

+--------+----------+-------+------+--------------------+-------------------+-----------+----------+-------+
|   txnno|   txndate| custno|amount|            category|            product|       city|     state|spendby|
+--------+----------+-------+------+--------------------+-------------------+-----------+----------+-------+
|00075990|10-27-2011|4004658|063.14|        Water Sports| Whitewater Rafting|Springfield|  Illinois| credit|
|00075991|11-23-2011|4009784|005.46|Outdoor Play Equi...|   Outdoor Playsets| Cincinnati|      Ohio|   cash|
|00075992|04-20-2011|4000281|175.44|  Exercise & Fitness|Abdominal Equipment| Long Beach|California| credit|
|00075993|05-12-2011|4005096|061.08|         Team Sports|       Cheerleading| Montgomery|   Alabama| credit|
|00075994|12-16-2011|4002516|005.20|  Outdoor Recreation|      Rock Climbing|Chattanooga| Tennessee|   cash|
+--------+----------+-------+------+--------------------+-------------------+-----------+----------+-------+
only showing top 5 rows


====== file 6 xmldf======

+-----+----------+-------+------+------------------+---------------+-------------+----------+-------+
|txnno|   txndate| custno|amount|          category|        product|         city|     state|spendby|
+-----+----------+-------+------+------------------+---------------+-------------+----------+-------+
|88973|10-23-2011|4000564|180.42|     Winter Sports|    Snowshoeing|     Portland|    Oregon| credit|
|88974|11-24-2011|4004615| 76.45|Exercise & Fitness|Cardio Machines|  Kansas City|    Kansas| credit|
|88975|04-14-2011|4009978|187.67|     Winter Sports|    Snowshoeing|     New York|  New York| credit|
|88976|04-11-2011|4003490|196.06|      Indoor Games|     Air Hockey|        Boise|     Idaho| credit|
|88977|04-02-2011|4007867| 36.07|       Team Sports|     Basketball|Coral Springs|   Florida|   cash|
|88978|02-12-2011|4006836| 40.78|        Gymnastics|   Springboards|    Santa Ana|California|   cash|
+-----+----------+-------+------+------------------+---------------+-------------+----------+-------+
only showing top 6 rows


====== union df======

+--------+----------+-------+------+----------+--------------------+----------------+---------+-------+
|   txnno|   txndate| custno|amount|  category|             product|            city|    state|spendby|
+--------+----------+-------+------+----------+--------------------+----------------+---------+-------+
|00000003|06-05-2011|4002199|198.19|Gymnastics|    Gymnastics Rings|       Milwaukee|Wisconsin| credit|
|00000023|05-02-2011|4007596|099.50|Gymnastics|    Gymnastics Rings|     Springfield| Illinois| credit|
|00000082|01-16-2011|4001098|021.23|Gymnastics|    Gymnastics Rings|           Tampa|  Florida|   cash|
|00000087|06-05-2011|4001050|089.56|Gymnastics|     Gymnastics Mats|West Valley City|     Utah| credit|
|00000111|03-05-2011|4000401|173.56|Gymnastics|Gymnastics Protec...|        Portland|   Oregon| credit|
+--------+----------+-------+------+----------+--------------------+----------------+---------+-------+
only showing top 5 rows

75534

====== proc df======

+--------+----+-------+------+------------------+------------------+-------------+----------+-------+------+
|   txnno|year| custno|amount|          category|           product|         city|     state|spendby|status|
+--------+----+-------+------+------------------+------------------+-------------+----------+-------+------+
|00050001|2011|4004079|189.90|             Games|Poker Chips & Sets|   Cincinnati|      Ohio| credit|     0|
|00050002|2011|4009271|092.19|       Team Sports|      Cheerleading|   Richmond  |  Virginia| credit|     0|
|00050003|2011|4004362|051.91|Exercise & Fitness|      Foam Rollers|  Westminster|  Colorado| credit|     0|
|00050004|2011|4001893|039.72|      Water Sports|          Swimming|San Francisco|California| credit|     0|
|00050005|2011|4004986|065.25|      Water Sports|          Wetsuits|  St. Louis  |  Missouri| credit|     0|
+--------+----+-------+------+------------------+------------------+-------------+----------+-------+------+
only showing top 5 rows


====== agg df======

+--------------------+------------------+
|            category|             total|
+--------------------+------------------+
|          Gymnastics| 311946.1299999998|
|       Winter Sports| 298795.6600000001|
|             Jumping|176760.88999999998|
|         Team Sports| 572796.0100000001|
|          Air Sports| 98705.80000000003|
|        Indoor Games|259715.26999999993|
|               Games|          339716.3|
|Outdoor Play Equi...| 263790.9900000002|
|        Water Sports|496075.46000000014|
|             Puzzles|57356.020000000004|
|  Outdoor Recreation| 796503.0499999998|
|      Racquet Sports|151250.82000000007|
|       Combat Sports|         155792.32|
|             Dancing|39585.350000000006|
|  Exercise & Fitness| 691531.6700000004|
+--------------------+------------------+


====== write df======

====data written ====
+---+----+
| id|name|
+---+----+
|  1| raj|
|  2|ravi|
|  3| sai|
|  5|rani|
+---+----+

+---+-------+
| id|product|
+---+-------+
|  1|cookies|
|  3| mobile|
|  7| laptop|
+---+-------+


======Left join======
+---+----+-------+
| id|name|product|
+---+----+-------+
|  1| raj|cookies|
|  2|ravi|   null|
|  3| sai| mobile|
|  5|rani|   null|
+---+----+-------+


======Inner join======
+---+----+-------+
| id|name|product|
+---+----+-------+
|  1| raj|cookies|
|  3| sai| mobile|
+---+----+-------+


======Right join======
+---+----+-------+
| id|name|product|
+---+----+-------+
|  1| raj|cookies|
|  3| sai| mobile|
|  7|null| laptop|
+---+----+-------+


======Full join======
+---+----+-------+
| id|name|product|
+---+----+-------+
|  1| raj|cookies|
|  2|ravi|   null|
|  3| sai| mobile|
|  5|rani|   null|
|  7|null| laptop|
+---+----+-------+


======Left_anti join======
+---+----+-------+
| id|name|product|
+---+----+-------+
|  1| raj|cookies|
|  2|ravi|   null|
|  3| sai| mobile|
|  5|rani|   null|
+---+----+-------+


======Cross join======
+---+----+---+-------+
| id|name| id|product|
+---+----+---+-------+
|  1| raj|  1|cookies|
|  2|ravi|  1|cookies|
|  3| sai|  1|cookies|
|  5|rani|  1|cookies|
|  1| raj|  3| mobile|
|  2|ravi|  3| mobile|
|  3| sai|  3| mobile|
|  5|rani|  3| mobile|
|  1| raj|  7| laptop|
|  2|ravi|  7| laptop|
|  3| sai|  7| laptop|
|  5|rani|  7| laptop|
+---+----+---+-------+


======Left Semi join======
+---+----+
| id|name|
+---+----+
|  1| raj|
|  3| sai|
+---+----+

--------------------------------------
======================================

==========================
Task 1 -----

Flatten pic.json
--------------------
package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.functions.upper
import org.apache.spark.sql.catalyst.expressions.Upper
import org.apache.spark.sql.functions._

object project13 {



def main(args:Array[String]):Unit={



		val conf = new SparkConf().setAppName("Revision").setMaster("local[*]")
				val sc = new SparkContext(conf)
				sc.setLogLevel("ERROR")




				val spark = SparkSession.builder().getOrCreate()
				import spark.implicits._

			
				println("====== Task1 Pic.json ======")
				val df = spark
				          .read
				          .format("json")
				          .option("multiline","true")
				          .load("file:///D:/data/pic.json")
				
				df.show()
				df.printSchema()
	
				
				val flatdf = df.select(
				                        "id",
				                        "type",
				                        "name",
				                        "image.height",
				                        "image.url",
				                        "image.width"
				                      
				                        )

				          
				flatdf.show()
				flatdf.printSchema()
				
				/*val flatdf = df.select(
				    
				                    "Years",
				                    "address.permanentAddress",
				                    "address.temporaryAddress",
				                    "org",
				                    "trainer"				
				
				
				            )
				
				
				flatdf.show()
				flatdf.printSchema()*/
				
		
				
      }

}
-------------
Out put:
--------
====== Task1 Pic.json ======
+---+--------------------+---------+-----+
| id|               image|     name| type|
+---+--------------------+---------+-----+
|000|[200, images/0001...|Non cream|donut|
+---+--------------------+---------+-----+

root
 |-- id: string (nullable = true)
 |-- image: struct (nullable = true)
 |    |-- height: long (nullable = true)
 |    |-- url: string (nullable = true)
 |    |-- width: long (nullable = true)
 |-- name: string (nullable = true)
 |-- type: string (nullable = true)

+---+-----+---------+------+---------------+-----+
| id| type|     name|height|            url|width|
+---+-----+---------+------+---------------+-----+
|000|donut|Non cream|   200|images/0001.jpg|  200|
+---+-----+---------+------+---------------+-----+

root
 |-- id: string (nullable = true)
 |-- type: string (nullable = true)
 |-- name: string (nullable = true)
 |-- height: long (nullable = true)
 |-- url: string (nullable = true)
 |-- width: long (nullable = true)
 ========
 Method-2
 ========
 package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.functions.upper
import org.apache.spark.sql.catalyst.expressions.Upper
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window

object project13 {



def main(args:Array[String]):Unit={



		val conf = new SparkConf().setAppName("Revision").setMaster("local[*]")
				val sc = new SparkContext(conf)
				sc.setLogLevel("ERROR")




				val spark = SparkSession.builder().getOrCreate()
				import spark.implicits._


		val df = spark.read.format("json").option("multiline","true").load("file:///D:/data/pic.json")
		
		df.show()
		df.printSchema()
		
		val flatdf = df.withColumn("height",expr("image.height"))
		               .withColumn("url",expr("image.url"))
		               .withColumn("width",expr("image.width"))
		               .drop("image")
		
		flatdf.show()
		flatdf.printSchema()
		
    }

}
------------
Out put:
-------
+---+--------------------+---------+-----+
| id|               image|     name| type|
+---+--------------------+---------+-----+
|000|[200, images/0001...|Non cream|donut|
+---+--------------------+---------+-----+

root
 |-- id: string (nullable = true)
 |-- image: struct (nullable = true)
 |    |-- height: long (nullable = true)
 |    |-- url: string (nullable = true)
 |    |-- width: long (nullable = true)
 |-- name: string (nullable = true)
 |-- type: string (nullable = true)

+---+---------+-----+------+---------------+-----+
| id|     name| type|height|            url|width|
+---+---------+-----+------+---------------+-----+
|000|Non cream|donut|   200|images/0001.jpg|  200|
+---+---------+-----+------+---------------+-----+

root
 |-- id: string (nullable = true)
 |-- name: string (nullable = true)
 |-- type: string (nullable = true)
 |-- height: long (nullable = true)
 |-- url: string (nullable = true)
 |-- width: long (nullable = true)
=======================================================


Task 2 -- Uber UseCase

Find the Maximum number of Trips along with Day for each dispatching base number
=================================================================================
package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.functions.upper
import org.apache.spark.sql.catalyst.expressions.Upper
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window

object project13 {



def main(args:Array[String]):Unit={



		val conf = new SparkConf().setAppName("Revision").setMaster("local[*]")
				val sc = new SparkContext(conf)
				sc.setLogLevel("ERROR")




				val spark = SparkSession.builder().getOrCreate()
				import spark.implicits._


				
				val df = spark
				          .read
				          .format("csv")
				          .option("header","true")
				          .load("file:///D:/data/uber.csv")

				
				
				val finaldf = df.withColumn("Day",expr("DATE_FORMAT(TO_DATE(DATE,'MM/dd/yyyy'),'EEE')"))
				                .groupBy("dispatching_base_number","Day")
				                .agg(sum("trips").alias("Sum"))
				                .withColumn("rnk",rank.over(Window.partitionBy("dispatching_base_number").orderBy("Sum")))
				                .filter("rnk=1")
				                .drop("rnk")
				                
				 finaldf.show()               
				               
				                    
				}


}
----------------
Out put:
--------
+-----------------------+---+--------+
|dispatching_base_number|Day|     Sum|
+-----------------------+---+--------+
|                 B02512|Wed| 10487.0|
|                 B02598|Thu| 60882.0|
|                 B02682|Thu| 74939.0|
|                 B02765|Thu| 21974.0|
|                 B02617|Thu| 80591.0|
|                 B02764|Thu|214116.0|
+-----------------------+---+--------+
======================================================================




Task 3 --- Gold Video
==========================



=======================
24-12-2022
=======================
Handson Task1
-------------
package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.functions.upper
import org.apache.spark.sql.catalyst.expressions.Upper
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window

object project13 {



def main(args:Array[String]):Unit={



		val conf = new SparkConf().setAppName("Revision").setMaster("local[*]")
				val sc = new SparkContext(conf)
				sc.setLogLevel("ERROR")




				val spark = SparkSession.builder().getOrCreate()
				import spark.implicits._


		val df = spark.read.format("json").option("multiline","true").load("file:///D:/data/place.json")
		
		df.show()
		df.printSchema()
		
		val flattendf = df.select(
		                    "place",
		                    "user.address.*",
		                    "user.name"
		                  )
		
		flattendf.show()
		flattendf.printSchema()
		
    }

}
-------------------------------
Out put:
--------
+---------+--------------------+
|    place|                user|
+---------+--------------------+
|Hyderabad|[[40, 400209, ash...|
+---------+--------------------+

root
 |-- place: string (nullable = true)
 |-- user: struct (nullable = true)
 |    |-- address: struct (nullable = true)
 |    |    |-- number: string (nullable = true)
 |    |    |-- pin: string (nullable = true)
 |    |    |-- street: string (nullable = true)
 |    |-- name: string (nullable = true)

+---------+------+------+-----------+----+
|    place|number|   pin|     street|name|
+---------+------+------+-----------+----+
|Hyderabad|    40|400209|ashok nagar|zeyo|
+---------+------+------+-----------+----+

root
 |-- place: string (nullable = true)
 |-- number: string (nullable = true)
 |-- pin: string (nullable = true)
 |-- street: string (nullable = true)
 |-- name: string (nullable = true)
 
 
 
 =============================
 2nd Handson Task Complex data
 package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.functions.upper
import org.apache.spark.sql.catalyst.expressions.Upper
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window

object project13 {



def main(args:Array[String]):Unit={



		val conf = new SparkConf().setAppName("Revision").setMaster("local[*]")
				val sc = new SparkContext(conf)
				sc.setLogLevel("ERROR")




				val spark = SparkSession.builder().getOrCreate()
				import spark.implicits._


		val df = spark.read.format("json").option("multiline","true").load("file:///D:/data/address1.json")
		
		df.show()
		df.printSchema()
		
		val flatten2df = df.select(
		                    "age",
		                    "date_of_birth",
		                    "email_address",
		                    "first_name",
		                    "height_cm",
		                    "is_alive",
		                    "last_name",
		                    "address.billing_address.*"
		                  )
		
		flatten2df.show()
		flatten2df.printSchema()
		
    }

}

--------------
Out put:
--------
+--------------------+---+-------------+-------------------+----------+---------+--------+---------+
|             address|age|date_of_birth|      email_address|first_name|height_cm|is_alive|last_name|
+--------------------+---+-------------+-------------------+----------+---------+--------+---------+
|[[502, Main Marke...| 30|         null|rajeev@ezeelive.com|    Rajeev|    185.2|    true|   Sharma|
+--------------------+---+-------------+-------------------+----------+---------+--------+---------+

root
 |-- address: struct (nullable = true)
 |    |-- billing_address: struct (nullable = true)
 |    |    |-- address: string (nullable = true)
 |    |    |-- city: string (nullable = true)
 |    |    |-- postal_code: string (nullable = true)
 |    |    |-- state: string (nullable = true)
 |-- age: long (nullable = true)
 |-- date_of_birth: string (nullable = true)
 |-- email_address: string (nullable = true)
 |-- first_name: string (nullable = true)
 |-- height_cm: double (nullable = true)
 |-- is_alive: boolean (nullable = true)
 |-- last_name: string (nullable = true)

+---+-------------+-------------------+----------+---------+--------+---------+--------------------+-------------------+-----------+-----------+
|age|date_of_birth|      email_address|first_name|height_cm|is_alive|last_name|             address|               city|postal_code|      state|
+---+-------------+-------------------+----------+---------+--------+---------+--------------------+-------------------+-----------+-----------+
| 30|         null|rajeev@ezeelive.com|    Rajeev|    185.2|    true|   Sharma|502, Main Market,...|Vasai Raod, Palghar|     401208|Maharashtra|
+---+-------------+-------------------+----------+---------+--------+---------+--------------------+-------------------+-----------+-----------+

root
 |-- age: long (nullable = true)
 |-- date_of_birth: string (nullable = true)
 |-- email_address: string (nullable = true)
 |-- first_name: string (nullable = true)
 |-- height_cm: double (nullable = true)
 |-- is_alive: boolean (nullable = true)
 |-- last_name: string (nullable = true)
 |-- address: string (nullable = true)
 |-- city: string (nullable = true)
 |-- postal_code: string (nullable = true)
 |-- state: string (nullable = true)
 
 =====================================================
 Address1 json process and Generation:
 ------------------------------------
 package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.functions.upper
import org.apache.spark.sql.catalyst.expressions.Upper
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window

object project13 {
	def main(args:Array[String]):Unit={
			val conf = new SparkConf().setAppName("Revision").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder().getOrCreate()
					import spark.implicits._


					val df = spark
					.read
					.format("json")
					.option("multiline","true")
					.load("file:///D:/data/address1.json")
					df.show()         
					df.printSchema()



					val flattendf = df.select(

							"address.billing_address.*",
							"age",
							"date_of_birth",
							"email_address",
							"first_name",
							"height_cm",
							"is_alive",
							"last_name"

							)


					flattendf.show()
					flattendf.printSchema()



					val complexdf = flattendf.select(

							struct(

									struct(

											col("address"),
											col("city"),
											col("postal_code"),
											col("state")

											).as("billing_address")

									).as("address"),


							col("age"),
							col("date_of_birth"),
							col("email_address"),
							col("first_name"),
							col("height_cm"),
							col("is_alive"),
							col("last_name")


							)


							complexdf.show()
							complexdf.printSchema()


	}

}
--------------------
Out put:
--------
+--------------------+---+-------------+-------------------+----------+---------+--------+---------+
|             address|age|date_of_birth|      email_address|first_name|height_cm|is_alive|last_name|
+--------------------+---+-------------+-------------------+----------+---------+--------+---------+
|[[502, Main Marke...| 30|         null|rajeev@ezeelive.com|    Rajeev|    185.2|    true|   Sharma|
+--------------------+---+-------------+-------------------+----------+---------+--------+---------+

root
 |-- address: struct (nullable = true)
 |    |-- billing_address: struct (nullable = true)
 |    |    |-- address: string (nullable = true)
 |    |    |-- city: string (nullable = true)
 |    |    |-- postal_code: string (nullable = true)
 |    |    |-- state: string (nullable = true)
 |-- age: long (nullable = true)
 |-- date_of_birth: string (nullable = true)
 |-- email_address: string (nullable = true)
 |-- first_name: string (nullable = true)
 |-- height_cm: double (nullable = true)
 |-- is_alive: boolean (nullable = true)
 |-- last_name: string (nullable = true)

+--------------------+-------------------+-----------+-----------+---+-------------+-------------------+----------+---------+--------+---------+
|             address|               city|postal_code|      state|age|date_of_birth|      email_address|first_name|height_cm|is_alive|last_name|
+--------------------+-------------------+-----------+-----------+---+-------------+-------------------+----------+---------+--------+---------+
|502, Main Market,...|Vasai Raod, Palghar|     401208|Maharashtra| 30|         null|rajeev@ezeelive.com|    Rajeev|    185.2|    true|   Sharma|
+--------------------+-------------------+-----------+-----------+---+-------------+-------------------+----------+---------+--------+---------+

root
 |-- address: string (nullable = true)
 |-- city: string (nullable = true)
 |-- postal_code: string (nullable = true)
 |-- state: string (nullable = true)
 |-- age: long (nullable = true)
 |-- date_of_birth: string (nullable = true)
 |-- email_address: string (nullable = true)
 |-- first_name: string (nullable = true)
 |-- height_cm: double (nullable = true)
 |-- is_alive: boolean (nullable = true)
 |-- last_name: string (nullable = true)

+--------------------+---+-------------+-------------------+----------+---------+--------+---------+
|             address|age|date_of_birth|      email_address|first_name|height_cm|is_alive|last_name|
+--------------------+---+-------------+-------------------+----------+---------+--------+---------+
|[[502, Main Marke...| 30|         null|rajeev@ezeelive.com|    Rajeev|    185.2|    true|   Sharma|
+--------------------+---+-------------+-------------------+----------+---------+--------+---------+

root
 |-- address: struct (nullable = false)
 |    |-- billing_address: struct (nullable = false)
 |    |    |-- address: string (nullable = true)
 |    |    |-- city: string (nullable = true)
 |    |    |-- postal_code: string (nullable = true)
 |    |    |-- state: string (nullable = true)
 |-- age: long (nullable = true)
 |-- date_of_birth: string (nullable = true)
 |-- email_address: string (nullable = true)
 |-- first_name: string (nullable = true)
 |-- height_cm: double (nullable = true)
 |-- is_alive: boolean (nullable = true)
 |-- last_name: string (nullable = true)
 
 
===================
Assignment Tasks:
------
Task1:
------
package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.functions.upper
import org.apache.spark.sql.catalyst.expressions.Upper
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window

object project13 {
	def main(args:Array[String]):Unit={
			val conf = new SparkConf().setAppName("Revision").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder().getOrCreate()
					import spark.implicits._


					val df = spark
					.read
					.format("json")
					.option("multiline","true")
					.load("file:///D:/data/jc.json")
					
					df.show()         
					df.printSchema()



					val flattendf = df.select(

							"years",
							"address.user.*",
							"org",
							"trainer"
							)


					flattendf.show()
					flattendf.printSchema()



					val complexdf3 = flattendf.select(
					                                  
					                              col("years"),

							struct(

									struct(

											
											col("age"),
											col("name")).as("user")).as("address"),


							col("org"),
							col("trainer"))
							
					complexdf3.show()
					complexdf3.printSchema()

	    }

}
---------
Out put:
--------
+-----+-----------+--------+-------+
|Years|    address|     org|trainer|
+-----+-----------+--------+-------+
|    4|[[4, zeyo]]|zeyobron|    sai|
+-----+-----------+--------+-------+

root
 |-- Years: long (nullable = true)
 |-- address: struct (nullable = true)
 |    |-- user: struct (nullable = true)
 |    |    |-- age: long (nullable = true)
 |    |    |-- name: string (nullable = true)
 |-- org: string (nullable = true)
 |-- trainer: string (nullable = true)

+-----+---+----+--------+-------+
|years|age|name|     org|trainer|
+-----+---+----+--------+-------+
|    4|  4|zeyo|zeyobron|    sai|
+-----+---+----+--------+-------+

root
 |-- years: long (nullable = true)
 |-- age: long (nullable = true)
 |-- name: string (nullable = true)
 |-- org: string (nullable = true)
 |-- trainer: string (nullable = true)

+-----+-----------+--------+-------+
|years|    address|     org|trainer|
+-----+-----------+--------+-------+
|    4|[[4, zeyo]]|zeyobron|    sai|
+-----+-----------+--------+-------+

root
 |-- years: long (nullable = true)
 |-- address: struct (nullable = false)
 |    |-- user: struct (nullable = false)
 |    |    |-- age: long (nullable = true)
 |    |    |-- name: string (nullable = true)
 |-- org: string (nullable = true)
 |-- trainer: string (nullable = true)
 
 ==============
 24-12-2022
 
Assignment Task2
----------------
package pack

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
import org.apache.spark.{SparkConf, SparkContext}

object project13 {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setMaster("local[*]").setAppName("first")
    val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR")
    val spark = SparkSession.builder.config(conf).getOrCreate()
    val df = spark
      .read
      .format("json")
      .option("multiline", "true")
      .load("file:///D:/data/address2.json")
    println
    println("===== dt data =====")
    println
    
    df.show()
    df.printSchema()
    
    println
    println("======== Flattening data ========")
    println
    
    val flattendf = df
      .withColumn( "billing-address",expr("address.billing_address.address"))
      .withColumn("billing-city",expr("address.billing_address.city"))
      .withColumn("billing-postal_code",expr("address.billing_address.postal_code"))
      .withColumn("billing-state",expr("address.billing_address.state"))
      .withColumn("shipping-address",expr("address.shipping_address.address"))
      .withColumn("shipping-address_city",expr("address.shipping_address.city"))
      .withColumn("shipping-postal_code",expr("address.shipping_address.postal_code"))
      .withColumn("shipping-state",expr("address.shipping_address.state"))
      .drop("address")
     
    flattendf.show()
    flattendf.printSchema()
   
    println
    println("====== Recreating RawData from FlattenData ======")
    println
    
    val rawdf = flattendf.select(
        struct( col("billing-address").as("address"),
                col("billing-city").as("city"),
                col("billing-postal_code").as("postal_code"),
                col("billing-state").as("state")
        ).as("billing_address"),
        struct( col("shipping-address").as("address"),
                col("shipping-address_city").as("city"),
                col("shipping-postal_code").as("postal_code"),
                col("shipping-state").as("state")
        ).as("shipping_address"),
        col("age"),
        col("date_of_birth"),
        col("email_address"),
        col("first_name"),
        col("height_cm"),
        col("is_alive"),
        col("last_name")
    )
    rawdf.show()
    rawdf.printSchema()
  }
}
Out put:
--------
===== dt data =====

+--------------------+---+-------------+-------------------+----------+---------+--------+---------+
|             address|age|date_of_birth|      email_address|first_name|height_cm|is_alive|last_name|
+--------------------+---+-------------+-------------------+----------+---------+--------+---------+
|[[502, Main Marke...| 30|         null|rajeev@ezeelive.com|    Rajeev|    185.2|    true|   Sharma|
+--------------------+---+-------------+-------------------+----------+---------+--------+---------+

root
 |-- address: struct (nullable = true)
 |    |-- billing_address: struct (nullable = true)
 |    |    |-- address: string (nullable = true)
 |    |    |-- city: string (nullable = true)
 |    |    |-- postal_code: string (nullable = true)
 |    |    |-- state: string (nullable = true)
 |    |-- shipping_address: struct (nullable = true)
 |    |    |-- address: string (nullable = true)
 |    |    |-- city: string (nullable = true)
 |    |    |-- postal_code: string (nullable = true)
 |    |    |-- state: string (nullable = true)
 |-- age: long (nullable = true)
 |-- date_of_birth: string (nullable = true)
 |-- email_address: string (nullable = true)
 |-- first_name: string (nullable = true)
 |-- height_cm: double (nullable = true)
 |-- is_alive: boolean (nullable = true)
 |-- last_name: string (nullable = true)


======== Flattening data ========

+---+-------------+-------------------+----------+---------+--------+---------+--------------------+-------------------+-------------------+-------------+--------------------+---------------------+--------------------+--------------+
|age|date_of_birth|      email_address|first_name|height_cm|is_alive|last_name|     billing-address|       billing-city|billing-postal_code|billing-state|    shipping-address|shipping-address_city|shipping-postal_code|shipping-state|
+---+-------------+-------------------+----------+---------+--------+---------+--------------------+-------------------+-------------------+-------------+--------------------+---------------------+--------------------+--------------+
| 30|         null|rajeev@ezeelive.com|    Rajeev|    185.2|    true|   Sharma|502, Main Market,...|Vasai Raod, Palghar|             401208|  Maharashtra|Ezeelive Technolo...|               Mumbai|              400058|   Maharashtra|
+---+-------------+-------------------+----------+---------+--------+---------+--------------------+-------------------+-------------------+-------------+--------------------+---------------------+--------------------+--------------+

root
 |-- age: long (nullable = true)
 |-- date_of_birth: string (nullable = true)
 |-- email_address: string (nullable = true)
 |-- first_name: string (nullable = true)
 |-- height_cm: double (nullable = true)
 |-- is_alive: boolean (nullable = true)
 |-- last_name: string (nullable = true)
 |-- billing-address: string (nullable = true)
 |-- billing-city: string (nullable = true)
 |-- billing-postal_code: string (nullable = true)
 |-- billing-state: string (nullable = true)
 |-- shipping-address: string (nullable = true)
 |-- shipping-address_city: string (nullable = true)
 |-- shipping-postal_code: string (nullable = true)
 |-- shipping-state: string (nullable = true)


====== Recreating RawData from FlattenData ======

+--------------------+--------------------+---+-------------+-------------------+----------+---------+--------+---------+
|     billing_address|    shipping_address|age|date_of_birth|      email_address|first_name|height_cm|is_alive|last_name|
+--------------------+--------------------+---+-------------+-------------------+----------+---------+--------+---------+
|[502, Main Market...|[Ezeelive Technol...| 30|         null|rajeev@ezeelive.com|    Rajeev|    185.2|    true|   Sharma|
+--------------------+--------------------+---+-------------+-------------------+----------+---------+--------+---------+

root
 |-- billing_address: struct (nullable = false)
 |    |-- address: string (nullable = true)
 |    |-- city: string (nullable = true)
 |    |-- postal_code: string (nullable = true)
 |    |-- state: string (nullable = true)
 |-- shipping_address: struct (nullable = false)
 |    |-- address: string (nullable = true)
 |    |-- city: string (nullable = true)
 |    |-- postal_code: string (nullable = true)
 |    |-- state: string (nullable = true)
 |-- age: long (nullable = true)
 |-- date_of_birth: string (nullable = true)
 |-- email_address: string (nullable = true)
 |-- first_name: string (nullable = true)
 |-- height_cm: double (nullable = true)
 |-- is_alive: boolean (nullable = true)
 |-- last_name: string (nullable = true)
 =========================================================
 
 ================
 25-12-2022
 ================
 package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.functions.upper
import org.apache.spark.sql.catalyst.expressions.Upper
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window

object project13 {
	def main(args:Array[String]):Unit={
			val conf = new SparkConf().setAppName("Revision").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder().getOrCreate()
					import spark.implicits._


					val df = spark
					.read
					.format("json")
					.option("multiline","true")
					.load("file:///D:/data/jc1.json")
					
					df.show()         
					df.printSchema()


		
			val flattendf = df.withColumn(
			                            "Students", 
			                            expr("explode(Students)")
			                            )
			                            
			flattendf.show()
			flattendf.printSchema()
			
					
	}

}
----------
Out Put:
--------
+--------------+-----+--------+-------+
|      Students|Years|     org|trainer|
+--------------+-----+--------+-------+
|[ajay, Rajesh]|    4|zeyobron|    sai|
+--------------+-----+--------+-------+

root
 |-- Students: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- Years: long (nullable = true)
 |-- org: string (nullable = true)
 |-- trainer: string (nullable = true)

+--------+-----+--------+-------+
|Students|Years|     org|trainer|
+--------+-----+--------+-------+
|    ajay|    4|zeyobron|    sai|
|  Rajesh|    4|zeyobron|    sai|
+--------+-----+--------+-------+

root
 |-- Students: string (nullable = true)
 |-- Years: long (nullable = true)
 |-- org: string (nullable = true)
 |-- trainer: string (nullable = true)
 =======
 Task 2:
 =======
 package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.functions.upper
import org.apache.spark.sql.catalyst.expressions.Upper
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window

object project13 {
	def main(args:Array[String]):Unit={
			val conf = new SparkConf().setAppName("Revision").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder().getOrCreate()
					import spark.implicits._


					val df = spark
					.read
					.format("json")
					.option("multiline","true")
					.load("file:///D:/data/pets.json")
					
					df.show()         
					df.printSchema()


		
			val flattendf = df.withColumn("Pets",expr("explode(Pets)"))
			                  .withColumn("Permanent address",expr("Address.`Permanent address`"))
			                  .withColumn("Current address",expr("Address.`Current address`"))
			                  .drop("Address")
			                            
			flattendf.show()
			flattendf.printSchema()
			
					
	}

}
---------
Out put:
--------
+---------+-------+--------+----+----------+
|  Address|Boolean|  Mobile|Name|      Pets|
+---------+-------+--------+----+----------+
|[USA, AU]|   true|12345678|Vasu|[Dog, cat]|
+---------+-------+--------+----+----------+

root
 |-- Address: struct (nullable = true)
 |    |-- Permanent address: string (nullable = true)
 |    |-- current Address: string (nullable = true)
 |-- Boolean: boolean (nullable = true)
 |-- Mobile: long (nullable = true)
 |-- Name: string (nullable = true)
 |-- Pets: array (nullable = true)
 |    |-- element: string (containsNull = true)

+-------+--------+----+----+-----------------+---------------+
|Boolean|  Mobile|Name|Pets|Permanent address|Current address|
+-------+--------+----+----+-----------------+---------------+
|   true|12345678|Vasu| Dog|              USA|             AU|
|   true|12345678|Vasu| cat|              USA|             AU|
+-------+--------+----+----+-----------------+---------------+

root
 |-- Boolean: boolean (nullable = true)
 |-- Mobile: long (nullable = true)
 |-- Name: string (nullable = true)
 |-- Pets: string (nullable = true)
 |-- Permanent address: string (nullable = true)
 |-- Current address: string (nullable = true)
 
 ==========
 Task3
 ==========
 package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.functions.upper
import org.apache.spark.sql.catalyst.expressions.Upper
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window
import scala.io.Source

object project13 {
	def main(args:Array[String]):Unit={
			val conf = new SparkConf().setAppName("Revision").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder().getOrCreate()
					import spark.implicits._


					val html = Source.fromURL("https://randomuser.me/api/0.8/?results=10")
					val urldata = html.mkString
					println(urldata)
					val urlrdd = sc.parallelize(List(urldata))

					println("=============rdd ============")
					
					urlrdd.foreach(println)
					val df = spark.read.json(urlrdd)
					df.show()
					df.printSchema()
					
					val procdf = df.withColumn("results", expr("explode(results)"))
					             
					procdf.show()
					procdf.printSchema()
					
						val procdf1 = procdf.select ( 
					                       "results.user.name.first" ,
					                       "results.user.name.last"  , 
					                       "results.user.password"  , 
					                       "results.user.username") 
					
					procdf1.show()
					procdf1.printSchema()
					
					
					
					println(procdf1.rdd.partitions.size)
					
					
					procdf1.coalesce(1).write.format("csv").option("header","true")
					          .mode("overwrite")
					          .save("file:///D:/data/urldata")
					          
					          
					          
					          
					          println("=== data written ====")
					
					
					

	}

}
------------
Out put:
--------
{
    "results": [
        {
            "user": {
                "gender": "female",
                "name": {
                    "title": "mrs",
                    "first": "pebbles",
                    "last": "hofs"
                },
                "location": {
                    "street": "9925 zuilenstraat",
                    "city": "haaksbergen",
                    "state": "noord-brabant",
                    "zip": 16508
                },
                "email": "pebbles.hofs@example.com",
                "username": "ticklishpanda488",
                "password": "lucas1",
                "salt": "pxiivNJ7",
                "md5": "a37666c1398d987cb8d29174c7d1ed20",
                "sha1": "b6aa58b2ee300ba47b02b2ebff2ff7c4df7f5d36",
                "sha256": "2d2b2fe6d4c5358419265aa7af04912225686edc70dd5c65726ef6f7407836a4",
                "registered": 1368268281,
                "dob": 1262576359,
                "phone": "(056)-302-1199",
                "cell": "(297)-567-5308",
                "BSN": "58031527",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/women/20.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/women/20.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/women/20.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "male",
                "name": {
                    "title": "mr",
                    "first": "uilke",
                    "last": "lenferink"
                },
                "location": {
                    "street": "6504 predikherenstraat",
                    "city": "den haag",
                    "state": "friesland",
                    "zip": 19746
                },
                "email": "uilke.lenferink@example.com",
                "username": "greengoose413",
                "password": "beagle",
                "salt": "DpLkVuwP",
                "md5": "f449d844e6b9f2c21d4cf128dcb24aac",
                "sha1": "45459383952272688cd051137da658c832034fae",
                "sha256": "aca85b8586ff4703acc20958e868c2bcefd9fdb5f23abd9e321143a2bed78819",
                "registered": 1392289280,
                "dob": 571835690,
                "phone": "(141)-239-4907",
                "cell": "(945)-758-4109",
                "BSN": "04492764",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/men/82.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/men/82.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/men/82.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "male",
                "name": {
                    "title": "mr",
                    "first": "bert-jan",
                    "last": "wigger"
                },
                "location": {
                    "street": "1073 boothstraat",
                    "city": "katwijk",
                    "state": "drenthe",
                    "zip": 57359
                },
                "email": "bert-jan.wigger@example.com",
                "username": "brownlion541",
                "password": "sugar",
                "salt": "BaVctLtf",
                "md5": "891fb080934291db58d259b36e48194b",
                "sha1": "0cecf485f5df03303f1bd9c176d88d7635fbb722",
                "sha256": "cee87606d82e86862c9f28c7333f0b23ed3e367bcdd42470a8531ebe1114bb54",
                "registered": 1134136280,
                "dob": 320067224,
                "phone": "(623)-474-2054",
                "cell": "(467)-488-9177",
                "BSN": "69854910",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/men/68.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/men/68.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/men/68.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "male",
                "name": {
                    "title": "mr",
                    "first": "joas",
                    "last": "fopma"
                },
                "location": {
                    "street": "1875 achter de dom",
                    "city": "opmeer",
                    "state": "zeeland",
                    "zip": 54768
                },
                "email": "joas.fopma@example.com",
                "username": "yellowdog234",
                "password": "tasha",
                "salt": "fEftGZw0",
                "md5": "86c44aef9c02a1a07ce7092d4b3cb48a",
                "sha1": "5d8acdc2787de686ddb612a7cfbce22f8d313279",
                "sha256": "3ffcb2ca2361b64dc0df095012e8205ee2fd2897362dc2ec17d604d4e36870a8",
                "registered": 1428841629,
                "dob": 335086269,
                "phone": "(854)-542-2844",
                "cell": "(089)-988-4970",
                "BSN": "37929969",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/men/94.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/men/94.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/men/94.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "female",
                "name": {
                    "title": "ms",
                    "first": "morgan",
                    "last": "van rosmalen"
                },
                "location": {
                    "street": "5894 hardebollenstraat",
                    "city": "achtkarspelen",
                    "state": "friesland",
                    "zip": 66565
                },
                "email": "morgan.van rosmalen@example.com",
                "username": "orangebird699",
                "password": "animals",
                "salt": "XnVPhqBV",
                "md5": "27d9fda6eb956edc66e9a2ac1e0fa48d",
                "sha1": "db5c4a415611956635ab980a890fcd1d41e4340c",
                "sha256": "fedbc559ec2c062b114e851c60336fe8b2bed937ddc384efd170b2749680450d",
                "registered": 1279634032,
                "dob": 1259307907,
                "phone": "(477)-727-6327",
                "cell": "(186)-710-6772",
                "BSN": "17338098",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/women/64.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/women/64.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/women/64.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "female",
                "name": {
                    "title": "ms",
                    "first": "danila",
                    "last": "van walsum"
                },
                "location": {
                    "street": "1274 oudkerkhof",
                    "city": "ouder-amstel",
                    "state": "utrecht",
                    "zip": 11232
                },
                "email": "danila.van walsum@example.com",
                "username": "whiteleopard987",
                "password": "yummy",
                "salt": "OoeCG5TF",
                "md5": "ad70f64542cfddf331b5816b04f340f6",
                "sha1": "1e20fa8a706a90986b6e65e2ce899d193edb7f3d",
                "sha256": "c4591a1d23bbd28aaa3c0697a467a608bb0feb046abf9c1af88f8c0de5107544",
                "registered": 1393734223,
                "dob": 400625341,
                "phone": "(691)-096-5492",
                "cell": "(851)-744-4794",
                "BSN": "59373261",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/women/50.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/women/50.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/women/50.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "male",
                "name": {
                    "title": "mr",
                    "first": "mehdi",
                    "last": "best"
                },
                "location": {
                    "street": "1670 springweg",
                    "city": "grave",
                    "state": "noord-brabant",
                    "zip": 93661
                },
                "email": "mehdi.best@example.com",
                "username": "ticklishrabbit230",
                "password": "pudding",
                "salt": "eu2h7Mfg",
                "md5": "98be41765070ba94e18800f217eb18e7",
                "sha1": "67236e0ce635276b265bd5c678dc7a7d5bc5b2bc",
                "sha256": "1d2b0f74e141f3a0a54ad652b1dfc0ff62d3b99fac83a2c68eabeda713049d96",
                "registered": 1389590650,
                "dob": 379856123,
                "phone": "(567)-701-9319",
                "cell": "(289)-352-4008",
                "BSN": "18605181",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/men/93.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/men/93.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/men/93.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "female",
                "name": {
                    "title": "ms",
                    "first": "vibeke",
                    "last": "van ommeren"
                },
                "location": {
                    "street": "7329 blauwkapelseweg",
                    "city": "geldermalsen",
                    "state": "flevoland",
                    "zip": 14278
                },
                "email": "vibeke.van ommeren@example.com",
                "username": "silverkoala542",
                "password": "italia",
                "salt": "O3yizVyH",
                "md5": "f9caf770167f1655d67a7039bafff751",
                "sha1": "1c01a45c66272c2496740b4380719d05e60967db",
                "sha256": "c2fae996f16ba54190444f8215b92d28ed6bd68f2e19c4f21efc8003879e098e",
                "registered": 1123813507,
                "dob": 490953934,
                "phone": "(478)-907-5675",
                "cell": "(479)-851-1273",
                "BSN": "87099052",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/women/1.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/women/1.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/women/1.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "female",
                "name": {
                    "title": "ms",
                    "first": "aisling",
                    "last": "beukelman"
                },
                "location": {
                    "street": "7172 mariaplaats",
                    "city": "sint-michielsgestel",
                    "state": "drenthe",
                    "zip": 58283
                },
                "email": "aisling.beukelman@example.com",
                "username": "organicswan664",
                "password": "ripken",
                "salt": "5nhNfuxb",
                "md5": "354713194556f3ee331e2f8a1700dd89",
                "sha1": "4cad4dfe9c22f000bd65fcc1bf1390fa90bd2b37",
                "sha256": "081c91c13820da8f400e3f927cd3ca2b3b561e51ac2748d747fa2bfe350e901f",
                "registered": 943041912,
                "dob": 850713382,
                "phone": "(511)-620-9971",
                "cell": "(452)-666-0525",
                "BSN": "55972150",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/women/44.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/women/44.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/women/44.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "female",
                "name": {
                    "title": "miss",
                    "first": "marjelle",
                    "last": "kuijper"
                },
                "location": {
                    "street": "4228 houtensepad",
                    "city": "rozendaal",
                    "state": "groningen",
                    "zip": 24082
                },
                "email": "marjelle.kuijper@example.com",
                "username": "lazylion582",
                "password": "sneaky",
                "salt": "mgJyPm1s",
                "md5": "cf4b8d7d13cb4ab787da8db265a97af5",
                "sha1": "d899ce4eb5122a8045dbc702a84e56accf286813",
                "sha256": "ce1ce3fefe395e9de58d9eda89f7720ef5a36652f166a21b297b35162d9ecec7",
                "registered": 1147535883,
                "dob": 1393026637,
                "phone": "(633)-560-3597",
                "cell": "(036)-791-6455",
                "BSN": "76211583",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/women/36.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/women/36.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/women/36.jpg"
                }
            }
        }
    ],
    "nationality": "NL",
    "seed": "d3e8bbcd81fd58620b",
    "version": "0.8"
}
=============rdd ============
{
    "results": [
        {
            "user": {
                "gender": "female",
                "name": {
                    "title": "mrs",
                    "first": "pebbles",
                    "last": "hofs"
                },
                "location": {
                    "street": "9925 zuilenstraat",
                    "city": "haaksbergen",
                    "state": "noord-brabant",
                    "zip": 16508
                },
                "email": "pebbles.hofs@example.com",
                "username": "ticklishpanda488",
                "password": "lucas1",
                "salt": "pxiivNJ7",
                "md5": "a37666c1398d987cb8d29174c7d1ed20",
                "sha1": "b6aa58b2ee300ba47b02b2ebff2ff7c4df7f5d36",
                "sha256": "2d2b2fe6d4c5358419265aa7af04912225686edc70dd5c65726ef6f7407836a4",
                "registered": 1368268281,
                "dob": 1262576359,
                "phone": "(056)-302-1199",
                "cell": "(297)-567-5308",
                "BSN": "58031527",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/women/20.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/women/20.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/women/20.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "male",
                "name": {
                    "title": "mr",
                    "first": "uilke",
                    "last": "lenferink"
                },
                "location": {
                    "street": "6504 predikherenstraat",
                    "city": "den haag",
                    "state": "friesland",
                    "zip": 19746
                },
                "email": "uilke.lenferink@example.com",
                "username": "greengoose413",
                "password": "beagle",
                "salt": "DpLkVuwP",
                "md5": "f449d844e6b9f2c21d4cf128dcb24aac",
                "sha1": "45459383952272688cd051137da658c832034fae",
                "sha256": "aca85b8586ff4703acc20958e868c2bcefd9fdb5f23abd9e321143a2bed78819",
                "registered": 1392289280,
                "dob": 571835690,
                "phone": "(141)-239-4907",
                "cell": "(945)-758-4109",
                "BSN": "04492764",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/men/82.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/men/82.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/men/82.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "male",
                "name": {
                    "title": "mr",
                    "first": "bert-jan",
                    "last": "wigger"
                },
                "location": {
                    "street": "1073 boothstraat",
                    "city": "katwijk",
                    "state": "drenthe",
                    "zip": 57359
                },
                "email": "bert-jan.wigger@example.com",
                "username": "brownlion541",
                "password": "sugar",
                "salt": "BaVctLtf",
                "md5": "891fb080934291db58d259b36e48194b",
                "sha1": "0cecf485f5df03303f1bd9c176d88d7635fbb722",
                "sha256": "cee87606d82e86862c9f28c7333f0b23ed3e367bcdd42470a8531ebe1114bb54",
                "registered": 1134136280,
                "dob": 320067224,
                "phone": "(623)-474-2054",
                "cell": "(467)-488-9177",
                "BSN": "69854910",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/men/68.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/men/68.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/men/68.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "male",
                "name": {
                    "title": "mr",
                    "first": "joas",
                    "last": "fopma"
                },
                "location": {
                    "street": "1875 achter de dom",
                    "city": "opmeer",
                    "state": "zeeland",
                    "zip": 54768
                },
                "email": "joas.fopma@example.com",
                "username": "yellowdog234",
                "password": "tasha",
                "salt": "fEftGZw0",
                "md5": "86c44aef9c02a1a07ce7092d4b3cb48a",
                "sha1": "5d8acdc2787de686ddb612a7cfbce22f8d313279",
                "sha256": "3ffcb2ca2361b64dc0df095012e8205ee2fd2897362dc2ec17d604d4e36870a8",
                "registered": 1428841629,
                "dob": 335086269,
                "phone": "(854)-542-2844",
                "cell": "(089)-988-4970",
                "BSN": "37929969",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/men/94.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/men/94.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/men/94.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "female",
                "name": {
                    "title": "ms",
                    "first": "morgan",
                    "last": "van rosmalen"
                },
                "location": {
                    "street": "5894 hardebollenstraat",
                    "city": "achtkarspelen",
                    "state": "friesland",
                    "zip": 66565
                },
                "email": "morgan.van rosmalen@example.com",
                "username": "orangebird699",
                "password": "animals",
                "salt": "XnVPhqBV",
                "md5": "27d9fda6eb956edc66e9a2ac1e0fa48d",
                "sha1": "db5c4a415611956635ab980a890fcd1d41e4340c",
                "sha256": "fedbc559ec2c062b114e851c60336fe8b2bed937ddc384efd170b2749680450d",
                "registered": 1279634032,
                "dob": 1259307907,
                "phone": "(477)-727-6327",
                "cell": "(186)-710-6772",
                "BSN": "17338098",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/women/64.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/women/64.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/women/64.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "female",
                "name": {
                    "title": "ms",
                    "first": "danila",
                    "last": "van walsum"
                },
                "location": {
                    "street": "1274 oudkerkhof",
                    "city": "ouder-amstel",
                    "state": "utrecht",
                    "zip": 11232
                },
                "email": "danila.van walsum@example.com",
                "username": "whiteleopard987",
                "password": "yummy",
                "salt": "OoeCG5TF",
                "md5": "ad70f64542cfddf331b5816b04f340f6",
                "sha1": "1e20fa8a706a90986b6e65e2ce899d193edb7f3d",
                "sha256": "c4591a1d23bbd28aaa3c0697a467a608bb0feb046abf9c1af88f8c0de5107544",
                "registered": 1393734223,
                "dob": 400625341,
                "phone": "(691)-096-5492",
                "cell": "(851)-744-4794",
                "BSN": "59373261",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/women/50.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/women/50.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/women/50.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "male",
                "name": {
                    "title": "mr",
                    "first": "mehdi",
                    "last": "best"
                },
                "location": {
                    "street": "1670 springweg",
                    "city": "grave",
                    "state": "noord-brabant",
                    "zip": 93661
                },
                "email": "mehdi.best@example.com",
                "username": "ticklishrabbit230",
                "password": "pudding",
                "salt": "eu2h7Mfg",
                "md5": "98be41765070ba94e18800f217eb18e7",
                "sha1": "67236e0ce635276b265bd5c678dc7a7d5bc5b2bc",
                "sha256": "1d2b0f74e141f3a0a54ad652b1dfc0ff62d3b99fac83a2c68eabeda713049d96",
                "registered": 1389590650,
                "dob": 379856123,
                "phone": "(567)-701-9319",
                "cell": "(289)-352-4008",
                "BSN": "18605181",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/men/93.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/men/93.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/men/93.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "female",
                "name": {
                    "title": "ms",
                    "first": "vibeke",
                    "last": "van ommeren"
                },
                "location": {
                    "street": "7329 blauwkapelseweg",
                    "city": "geldermalsen",
                    "state": "flevoland",
                    "zip": 14278
                },
                "email": "vibeke.van ommeren@example.com",
                "username": "silverkoala542",
                "password": "italia",
                "salt": "O3yizVyH",
                "md5": "f9caf770167f1655d67a7039bafff751",
                "sha1": "1c01a45c66272c2496740b4380719d05e60967db",
                "sha256": "c2fae996f16ba54190444f8215b92d28ed6bd68f2e19c4f21efc8003879e098e",
                "registered": 1123813507,
                "dob": 490953934,
                "phone": "(478)-907-5675",
                "cell": "(479)-851-1273",
                "BSN": "87099052",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/women/1.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/women/1.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/women/1.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "female",
                "name": {
                    "title": "ms",
                    "first": "aisling",
                    "last": "beukelman"
                },
                "location": {
                    "street": "7172 mariaplaats",
                    "city": "sint-michielsgestel",
                    "state": "drenthe",
                    "zip": 58283
                },
                "email": "aisling.beukelman@example.com",
                "username": "organicswan664",
                "password": "ripken",
                "salt": "5nhNfuxb",
                "md5": "354713194556f3ee331e2f8a1700dd89",
                "sha1": "4cad4dfe9c22f000bd65fcc1bf1390fa90bd2b37",
                "sha256": "081c91c13820da8f400e3f927cd3ca2b3b561e51ac2748d747fa2bfe350e901f",
                "registered": 943041912,
                "dob": 850713382,
                "phone": "(511)-620-9971",
                "cell": "(452)-666-0525",
                "BSN": "55972150",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/women/44.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/women/44.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/women/44.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "female",
                "name": {
                    "title": "miss",
                    "first": "marjelle",
                    "last": "kuijper"
                },
                "location": {
                    "street": "4228 houtensepad",
                    "city": "rozendaal",
                    "state": "groningen",
                    "zip": 24082
                },
                "email": "marjelle.kuijper@example.com",
                "username": "lazylion582",
                "password": "sneaky",
                "salt": "mgJyPm1s",
                "md5": "cf4b8d7d13cb4ab787da8db265a97af5",
                "sha1": "d899ce4eb5122a8045dbc702a84e56accf286813",
                "sha256": "ce1ce3fefe395e9de58d9eda89f7720ef5a36652f166a21b297b35162d9ecec7",
                "registered": 1147535883,
                "dob": 1393026637,
                "phone": "(633)-560-3597",
                "cell": "(036)-791-6455",
                "BSN": "76211583",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/women/36.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/women/36.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/women/36.jpg"
                }
            }
        }
    ],
    "nationality": "NL",
    "seed": "d3e8bbcd81fd58620b",
    "version": "0.8"
}
+-----------+--------------------+------------------+-------+
|nationality|             results|              seed|version|
+-----------+--------------------+------------------+-------+
|         NL|[[[58031527, (297...|d3e8bbcd81fd58620b|    0.8|
+-----------+--------------------+------------------+-------+

root
 |-- nationality: string (nullable = true)
 |-- results: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- user: struct (nullable = true)
 |    |    |    |-- BSN: string (nullable = true)
 |    |    |    |-- cell: string (nullable = true)
 |    |    |    |-- dob: long (nullable = true)
 |    |    |    |-- email: string (nullable = true)
 |    |    |    |-- gender: string (nullable = true)
 |    |    |    |-- location: struct (nullable = true)
 |    |    |    |    |-- city: string (nullable = true)
 |    |    |    |    |-- state: string (nullable = true)
 |    |    |    |    |-- street: string (nullable = true)
 |    |    |    |    |-- zip: long (nullable = true)
 |    |    |    |-- md5: string (nullable = true)
 |    |    |    |-- name: struct (nullable = true)
 |    |    |    |    |-- first: string (nullable = true)
 |    |    |    |    |-- last: string (nullable = true)
 |    |    |    |    |-- title: string (nullable = true)
 |    |    |    |-- password: string (nullable = true)
 |    |    |    |-- phone: string (nullable = true)
 |    |    |    |-- picture: struct (nullable = true)
 |    |    |    |    |-- large: string (nullable = true)
 |    |    |    |    |-- medium: string (nullable = true)
 |    |    |    |    |-- thumbnail: string (nullable = true)
 |    |    |    |-- registered: long (nullable = true)
 |    |    |    |-- salt: string (nullable = true)
 |    |    |    |-- sha1: string (nullable = true)
 |    |    |    |-- sha256: string (nullable = true)
 |    |    |    |-- username: string (nullable = true)
 |-- seed: string (nullable = true)
 |-- version: string (nullable = true)

+-----------+--------------------+------------------+-------+
|nationality|             results|              seed|version|
+-----------+--------------------+------------------+-------+
|         NL|[[58031527, (297)...|d3e8bbcd81fd58620b|    0.8|
|         NL|[[04492764, (945)...|d3e8bbcd81fd58620b|    0.8|
|         NL|[[69854910, (467)...|d3e8bbcd81fd58620b|    0.8|
|         NL|[[37929969, (089)...|d3e8bbcd81fd58620b|    0.8|
|         NL|[[17338098, (186)...|d3e8bbcd81fd58620b|    0.8|
|         NL|[[59373261, (851)...|d3e8bbcd81fd58620b|    0.8|
|         NL|[[18605181, (289)...|d3e8bbcd81fd58620b|    0.8|
|         NL|[[87099052, (479)...|d3e8bbcd81fd58620b|    0.8|
|         NL|[[55972150, (452)...|d3e8bbcd81fd58620b|    0.8|
|         NL|[[76211583, (036)...|d3e8bbcd81fd58620b|    0.8|
+-----------+--------------------+------------------+-------+

root
 |-- nationality: string (nullable = true)
 |-- results: struct (nullable = true)
 |    |-- user: struct (nullable = true)
 |    |    |-- BSN: string (nullable = true)
 |    |    |-- cell: string (nullable = true)
 |    |    |-- dob: long (nullable = true)
 |    |    |-- email: string (nullable = true)
 |    |    |-- gender: string (nullable = true)
 |    |    |-- location: struct (nullable = true)
 |    |    |    |-- city: string (nullable = true)
 |    |    |    |-- state: string (nullable = true)
 |    |    |    |-- street: string (nullable = true)
 |    |    |    |-- zip: long (nullable = true)
 |    |    |-- md5: string (nullable = true)
 |    |    |-- name: struct (nullable = true)
 |    |    |    |-- first: string (nullable = true)
 |    |    |    |-- last: string (nullable = true)
 |    |    |    |-- title: string (nullable = true)
 |    |    |-- password: string (nullable = true)
 |    |    |-- phone: string (nullable = true)
 |    |    |-- picture: struct (nullable = true)
 |    |    |    |-- large: string (nullable = true)
 |    |    |    |-- medium: string (nullable = true)
 |    |    |    |-- thumbnail: string (nullable = true)
 |    |    |-- registered: long (nullable = true)
 |    |    |-- salt: string (nullable = true)
 |    |    |-- sha1: string (nullable = true)
 |    |    |-- sha256: string (nullable = true)
 |    |    |-- username: string (nullable = true)
 |-- seed: string (nullable = true)
 |-- version: string (nullable = true)

+--------+------------+--------+-----------------+
|   first|        last|password|         username|
+--------+------------+--------+-----------------+
| pebbles|        hofs|  lucas1| ticklishpanda488|
|   uilke|   lenferink|  beagle|    greengoose413|
|bert-jan|      wigger|   sugar|     brownlion541|
|    joas|       fopma|   tasha|     yellowdog234|
|  morgan|van rosmalen| animals|    orangebird699|
| danila|  van walsum|   yummy|  whiteleopard987|
|   mehdi|        best| pudding|ticklishrabbit230|
|  vibeke| van ommeren|  italia|   silverkoala542|
| aisling|   beukelman|  ripken|   organicswan664|
|marjelle|     kuijper|  sneaky|      lazylion582|
+--------+------------+--------+-----------------+

root
 |-- first: string (nullable = true)
 |-- last: string (nullable = true)
 |-- password: string (nullable = true)
 |-- username: string (nullable = true)

4
=== data written ====
===========================================================

============
Task4
============
package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.functions.upper
import org.apache.spark.sql.catalyst.expressions.Upper
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window
import scala.io.Source

object project13 {
	def main(args:Array[String]):Unit={
			val conf = new SparkConf().setAppName("Revision").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder().getOrCreate()
					import spark.implicits._

					
					// -------  URL read

					val html = Source.fromURL("https://randomuser.me/api/0.8/?results=10")
					val urldata = html.mkString
					println(urldata)
					val urlrdd = sc.parallelize(List(urldata))

					println("=============rdd ============")
					
					urlrdd.foreach(println)
					val df = spark.read.json(urlrdd)
					df.show()
					df.printSchema()
					
					
					println("====dataframe conversion=====")
					
					val procdf = df.withColumn("results", expr("explode(results)"))
					             
					procdf.show()
					procdf.printSchema()
					
						val procdf1 = procdf.select ( 
					                       "results.user.name.first" ,
					                       "results.user.name.last"  , 
					                       "results.user.password"  , 
					                       "results.user.username") 
					
					procdf1.show()
					procdf1.printSchema()
					
					
					
					println(procdf1.rdd.partitions.size)
					
					
					println("====csv write===")
					
					procdf1.coalesce(1).write.format("csv").option("header","true")
					          .mode("overwrite")
					          .save("file:///D:/data/urldata")
					          
					          
					          
					          
					          println("=== data written ====")
					
					
					

	}

}

----------
Out put:
--------
{
    "results": [
        {
            "user": {
                "gender": "female",
                "name": {
                    "title": "madame",
                    "first": "andra",
                    "last": "martin"
                },
                "location": {
                    "street": "1035 rue du bon-pasteur",
                    "city": "ballens",
                    "state": "obwalden",
                    "zip": 5006
                },
                "email": "andra.martin@example.com",
                "username": "blueleopard942",
                "password": "illusion",
                "salt": "3OWte03F",
                "md5": "5cb48bf131befe7b9690c4aff3874599",
                "sha1": "afea1d211de8f36c8c083e0cdbf5d0ac52b20fb9",
                "sha256": "c85bc7767b6c91100bb794466c71585e1e660a66394df98066cc27bf6aead470",
                "registered": 1184234889,
                "dob": 16564606,
                "phone": "(876)-578-8680",
                "cell": "(239)-673-5095",
                "AVS": "756.TVFY.SFLS.57",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/women/68.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/women/68.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/women/68.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "male",
                "name": {
                    "title": "monsieur",
                    "first": "fabio",
                    "last": "lemoine"
                },
                "location": {
                    "street": "4635 rue principale",
                    "city": "renens vd 1",
                    "state": "uri",
                    "zip": 4980
                },
                "email": "fabio.lemoine@example.com",
                "username": "orangemeercat822",
                "password": "barfly",
                "salt": "nMKIXWBL",
                "md5": "24f66bdb9d7bd8c24b2ab8d9f67ba138",
                "sha1": "b92997ac90bfae7b3e8abdc0dedf66fe324aa495",
                "sha256": "9ad846c5789aab2456147aa91523afd2e96d5ba7363ac7a45294fcc23c90ec22",
                "registered": 1424225996,
                "dob": 861896285,
                "phone": "(612)-290-9647",
                "cell": "(745)-525-4414",
                "AVS": "756.NVYK.MSJW.77",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/men/49.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/men/49.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/men/49.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "male",
                "name": {
                    "title": "monsieur",
                    "first": "julien",
                    "last": "robert"
                },
                "location": {
                    "street": "8010 rue des abbesses",
                    "city": "essertes",
                    "state": "aargau",
                    "zip": 3973
                },
                "email": "julien.robert@example.com",
                "username": "lazywolf863",
                "password": "cowboys1",
                "salt": "qvQLZpiJ",
                "md5": "1299a48610db94f8ae0f2d6849096e51",
                "sha1": "d6d1194f94aec9a8d8456f2215139c2b77fae473",
                "sha256": "00b6c07c8e48c7fad149014b96c1a4a0841660dde5068235bbbceed0a8584a4d",
                "registered": 1205315712,
                "dob": 223747301,
                "phone": "(396)-695-1895",
                "cell": "(036)-084-0231",
                "AVS": "756.BPRY.LMXI.91",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/men/49.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/men/49.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/men/49.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "female",
                "name": {
                    "title": "madame",
                    "first": "nora",
                    "last": "caron"
                },
                "location": {
                    "street": "4077 rue de l'abb-rousselot",
                    "city": "poliez-pittet",
                    "state": "neuchtel",
                    "zip": 6261
                },
                "email": "nora.caron@example.com",
                "username": "silverleopard906",
                "password": "sssss",
                "salt": "jytBSEwn",
                "md5": "22639964c9f2e2eab32539fe649323e4",
                "sha1": "ecb43ec749536293c98b9e5057b1d692dd587ff8",
                "sha256": "029d178c672bb3d42814b3f6745d3d952f2074f96b41f2597cccde332846d956",
                "registered": 1138057390,
                "dob": 686351428,
                "phone": "(534)-974-0118",
                "cell": "(065)-766-5048",
                "AVS": "756.ZEMG.NJPE.93",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/women/51.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/women/51.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/women/51.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "female",
                "name": {
                    "title": "mademoiselle",
                    "first": "mawenn",
                    "last": "da silva"
                },
                "location": {
                    "street": "1930 avenue jean-jaurs",
                    "city": "crissier",
                    "state": "genve",
                    "zip": 6881
                },
                "email": "mawenn.da silva@example.com",
                "username": "goldentiger200",
                "password": "koolaid",
                "salt": "Qq5lKP4E",
                "md5": "d7baccf8ab0e17c73dccabe7fa68c01f",
                "sha1": "27bb295cdcde14f74a17fa9cd4740d76441399e6",
                "sha256": "c56ba769ee86c438f8e9953128704da8405b1e4f0487f86321ccc9c414a76379",
                "registered": 1109564579,
                "dob": 471375653,
                "phone": "(805)-613-2977",
                "cell": "(035)-080-3589",
                "AVS": "756.TWLW.ITLJ.69",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/women/94.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/women/94.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/women/94.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "female",
                "name": {
                    "title": "madame",
                    "first": "estelle",
                    "last": "hubert"
                },
                "location": {
                    "street": "4472 rue du bon-pasteur",
                    "city": "echandens-denges",
                    "state": "fribourg",
                    "zip": 8107
                },
                "email": "estelle.hubert@example.com",
                "username": "purpleduck360",
                "password": "sluggo",
                "salt": "I2d708uE",
                "md5": "aa49d390db76bbd18a8f2ff8aa8ac419",
                "sha1": "3265fa4200540be90cf8c99d3e242e05b2c1ed20",
                "sha256": "f7f3a5e8cb94a66fe2eb9552ac615dcefd13485e59b24f8b99100b3f88d782ce",
                "registered": 983520140,
                "dob": 386408913,
                "phone": "(401)-803-2498",
                "cell": "(034)-394-3324",
                "AVS": "756.WETD.EOOD.19",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/women/84.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/women/84.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/women/84.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "male",
                "name": {
                    "title": "monsieur",
                    "first": "nal",
                    "last": "hubert"
                },
                "location": {
                    "street": "2626 avenue goerges clmenceau",
                    "city": "epalinges",
                    "state": "bern",
                    "zip": 4032
                },
                "email": "nal.hubert@example.com",
                "username": "bluebear919",
                "password": "ferris",
                "salt": "X43JuHud",
                "md5": "5246cb8f89aaa0b49b07b3c35ac60457",
                "sha1": "354e58bdbf6f34b682772bb1c6000f2c9e03db51",
                "sha256": "d693838e9c21f435c48b94c35bc2be88659de714ffa463a22a3b544d762fd349",
                "registered": 945353605,
                "dob": 688245764,
                "phone": "(725)-007-0596",
                "cell": "(486)-430-1033",
                "AVS": "756.HZPY.QTQH.70",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/men/44.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/men/44.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/men/44.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "female",
                "name": {
                    "title": "madame",
                    "first": "romane",
                    "last": "morel"
                },
                "location": {
                    "street": "9295 place de l'abb-georges-hnocque",
                    "city": "peyres-possens",
                    "state": "appenzell innerrhoden",
                    "zip": 6289
                },
                "email": "romane.morel@example.com",
                "username": "bigkoala295",
                "password": "novifarm",
                "salt": "dP3sBJoC",
                "md5": "66f4730e900ace190ec21cc5a3336005",
                "sha1": "4226a2222efd552f72169421236c4e4fba029c0f",
                "sha256": "1da0a8d08f6e5d071d218fb310af399bd945664a6f6b160d81d0860988749c62",
                "registered": 1184471084,
                "dob": 60659395,
                "phone": "(287)-326-8699",
                "cell": "(686)-791-0597",
                "AVS": "756.YJPX.PAWU.84",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/women/12.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/women/12.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/women/12.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "male",
                "name": {
                    "title": "monsieur",
                    "first": "to",
                    "last": "renaud"
                },
                "location": {
                    "street": "8940 rue des coles",
                    "city": "les cullayes",
                    "state": "appenzell ausserrhoden",
                    "zip": 1025
                },
                "email": "to.renaud@example.com",
                "username": "yellowlion593",
                "password": "active",
                "salt": "yVi2GzOX",
                "md5": "360dbc16d5d03b3757c677a90fb270ab",
                "sha1": "6e6bb974a5c41568ee14ddb798f9c490ab32135b",
                "sha256": "5e753ef9c3bef2abaac07abb6f7fbffa4df27f7e0ef80da9daa49a77868b1975",
                "registered": 970720889,
                "dob": 714254912,
                "phone": "(460)-193-6270",
                "cell": "(420)-039-6049",
                "AVS": "756.HDFZ.JUPZ.47",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/men/83.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/men/83.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/men/83.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "male",
                "name": {
                    "title": "monsieur",
                    "first": "noham",
                    "last": "lecomte"
                },
                "location": {
                    "street": "5696 rue du 8 mai 1945",
                    "city": "epautheyres",
                    "state": "jura",
                    "zip": 1826
                },
                "email": "noham.lecomte@example.com",
                "username": "greenmeercat430",
                "password": "awesome",
                "salt": "l4Rhs4JK",
                "md5": "4c37a76b6c6887549c80e1f3aa14027a",
                "sha1": "c8f3f3d4a6c4b58080a34fc67d7c5dfe334f7997",
                "sha256": "5a2a79b3d8eb10111f71c5d59a27771651126adda239a18ce5354622283bb432",
                "registered": 1437995704,
                "dob": 1022791977,
                "phone": "(900)-243-4218",
                "cell": "(209)-274-3895",
                "AVS": "756.OIAC.TUCR.11",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/men/21.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/men/21.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/men/21.jpg"
                }
            }
        }
    ],
    "nationality": "CH",
    "seed": "e9c150623f05718403",
    "version": "0.8"
}
=============rdd ============
{
    "results": [
        {
            "user": {
                "gender": "female",
                "name": {
                    "title": "madame",
                    "first": "andra",
                    "last": "martin"
                },
                "location": {
                    "street": "1035 rue du bon-pasteur",
                    "city": "ballens",
                    "state": "obwalden",
                    "zip": 5006
                },
                "email": "andra.martin@example.com",
                "username": "blueleopard942",
                "password": "illusion",
                "salt": "3OWte03F",
                "md5": "5cb48bf131befe7b9690c4aff3874599",
                "sha1": "afea1d211de8f36c8c083e0cdbf5d0ac52b20fb9",
                "sha256": "c85bc7767b6c91100bb794466c71585e1e660a66394df98066cc27bf6aead470",
                "registered": 1184234889,
                "dob": 16564606,
                "phone": "(876)-578-8680",
                "cell": "(239)-673-5095",
                "AVS": "756.TVFY.SFLS.57",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/women/68.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/women/68.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/women/68.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "male",
                "name": {
                    "title": "monsieur",
                    "first": "fabio",
                    "last": "lemoine"
                },
                "location": {
                    "street": "4635 rue principale",
                    "city": "renens vd 1",
                    "state": "uri",
                    "zip": 4980
                },
                "email": "fabio.lemoine@example.com",
                "username": "orangemeercat822",
                "password": "barfly",
                "salt": "nMKIXWBL",
                "md5": "24f66bdb9d7bd8c24b2ab8d9f67ba138",
                "sha1": "b92997ac90bfae7b3e8abdc0dedf66fe324aa495",
                "sha256": "9ad846c5789aab2456147aa91523afd2e96d5ba7363ac7a45294fcc23c90ec22",
                "registered": 1424225996,
                "dob": 861896285,
                "phone": "(612)-290-9647",
                "cell": "(745)-525-4414",
                "AVS": "756.NVYK.MSJW.77",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/men/49.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/men/49.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/men/49.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "male",
                "name": {
                    "title": "monsieur",
                    "first": "julien",
                    "last": "robert"
                },
                "location": {
                    "street": "8010 rue des abbesses",
                    "city": "essertes",
                    "state": "aargau",
                    "zip": 3973
                },
                "email": "julien.robert@example.com",
                "username": "lazywolf863",
                "password": "cowboys1",
                "salt": "qvQLZpiJ",
                "md5": "1299a48610db94f8ae0f2d6849096e51",
                "sha1": "d6d1194f94aec9a8d8456f2215139c2b77fae473",
                "sha256": "00b6c07c8e48c7fad149014b96c1a4a0841660dde5068235bbbceed0a8584a4d",
                "registered": 1205315712,
                "dob": 223747301,
                "phone": "(396)-695-1895",
                "cell": "(036)-084-0231",
                "AVS": "756.BPRY.LMXI.91",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/men/49.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/men/49.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/men/49.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "female",
                "name": {
                    "title": "madame",
                    "first": "nora",
                    "last": "caron"
                },
                "location": {
                    "street": "4077 rue de l'abb-rousselot",
                    "city": "poliez-pittet",
                    "state": "neuchtel",
                    "zip": 6261
                },
                "email": "nora.caron@example.com",
                "username": "silverleopard906",
                "password": "sssss",
                "salt": "jytBSEwn",
                "md5": "22639964c9f2e2eab32539fe649323e4",
                "sha1": "ecb43ec749536293c98b9e5057b1d692dd587ff8",
                "sha256": "029d178c672bb3d42814b3f6745d3d952f2074f96b41f2597cccde332846d956",
                "registered": 1138057390,
                "dob": 686351428,
                "phone": "(534)-974-0118",
                "cell": "(065)-766-5048",
                "AVS": "756.ZEMG.NJPE.93",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/women/51.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/women/51.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/women/51.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "female",
                "name": {
                    "title": "mademoiselle",
                    "first": "mawenn",
                    "last": "da silva"
                },
                "location": {
                    "street": "1930 avenue jean-jaurs",
                    "city": "crissier",
                    "state": "genve",
                    "zip": 6881
                },
                "email": "mawenn.da silva@example.com",
                "username": "goldentiger200",
                "password": "koolaid",
                "salt": "Qq5lKP4E",
                "md5": "d7baccf8ab0e17c73dccabe7fa68c01f",
                "sha1": "27bb295cdcde14f74a17fa9cd4740d76441399e6",
                "sha256": "c56ba769ee86c438f8e9953128704da8405b1e4f0487f86321ccc9c414a76379",
                "registered": 1109564579,
                "dob": 471375653,
                "phone": "(805)-613-2977",
                "cell": "(035)-080-3589",
                "AVS": "756.TWLW.ITLJ.69",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/women/94.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/women/94.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/women/94.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "female",
                "name": {
                    "title": "madame",
                    "first": "estelle",
                    "last": "hubert"
                },
                "location": {
                    "street": "4472 rue du bon-pasteur",
                    "city": "echandens-denges",
                    "state": "fribourg",
                    "zip": 8107
                },
                "email": "estelle.hubert@example.com",
                "username": "purpleduck360",
                "password": "sluggo",
                "salt": "I2d708uE",
                "md5": "aa49d390db76bbd18a8f2ff8aa8ac419",
                "sha1": "3265fa4200540be90cf8c99d3e242e05b2c1ed20",
                "sha256": "f7f3a5e8cb94a66fe2eb9552ac615dcefd13485e59b24f8b99100b3f88d782ce",
                "registered": 983520140,
                "dob": 386408913,
                "phone": "(401)-803-2498",
                "cell": "(034)-394-3324",
                "AVS": "756.WETD.EOOD.19",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/women/84.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/women/84.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/women/84.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "male",
                "name": {
                    "title": "monsieur",
                    "first": "nal",
                    "last": "hubert"
                },
                "location": {
                    "street": "2626 avenue goerges clmenceau",
                    "city": "epalinges",
                    "state": "bern",
                    "zip": 4032
                },
                "email": "nal.hubert@example.com",
                "username": "bluebear919",
                "password": "ferris",
                "salt": "X43JuHud",
                "md5": "5246cb8f89aaa0b49b07b3c35ac60457",
                "sha1": "354e58bdbf6f34b682772bb1c6000f2c9e03db51",
                "sha256": "d693838e9c21f435c48b94c35bc2be88659de714ffa463a22a3b544d762fd349",
                "registered": 945353605,
                "dob": 688245764,
                "phone": "(725)-007-0596",
                "cell": "(486)-430-1033",
                "AVS": "756.HZPY.QTQH.70",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/men/44.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/men/44.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/men/44.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "female",
                "name": {
                    "title": "madame",
                    "first": "romane",
                    "last": "morel"
                },
                "location": {
                    "street": "9295 place de l'abb-georges-hnocque",
                    "city": "peyres-possens",
                    "state": "appenzell innerrhoden",
                    "zip": 6289
                },
                "email": "romane.morel@example.com",
                "username": "bigkoala295",
                "password": "novifarm",
                "salt": "dP3sBJoC",
                "md5": "66f4730e900ace190ec21cc5a3336005",
                "sha1": "4226a2222efd552f72169421236c4e4fba029c0f",
                "sha256": "1da0a8d08f6e5d071d218fb310af399bd945664a6f6b160d81d0860988749c62",
                "registered": 1184471084,
                "dob": 60659395,
                "phone": "(287)-326-8699",
                "cell": "(686)-791-0597",
                "AVS": "756.YJPX.PAWU.84",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/women/12.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/women/12.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/women/12.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "male",
                "name": {
                    "title": "monsieur",
                    "first": "to",
                    "last": "renaud"
                },
                "location": {
                    "street": "8940 rue des coles",
                    "city": "les cullayes",
                    "state": "appenzell ausserrhoden",
                    "zip": 1025
                },
                "email": "to.renaud@example.com",
                "username": "yellowlion593",
                "password": "active",
                "salt": "yVi2GzOX",
                "md5": "360dbc16d5d03b3757c677a90fb270ab",
                "sha1": "6e6bb974a5c41568ee14ddb798f9c490ab32135b",
                "sha256": "5e753ef9c3bef2abaac07abb6f7fbffa4df27f7e0ef80da9daa49a77868b1975",
                "registered": 970720889,
                "dob": 714254912,
                "phone": "(460)-193-6270",
                "cell": "(420)-039-6049",
                "AVS": "756.HDFZ.JUPZ.47",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/men/83.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/men/83.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/men/83.jpg"
                }
            }
        },
        {
            "user": {
                "gender": "male",
                "name": {
                    "title": "monsieur",
                    "first": "noham",
                    "last": "lecomte"
                },
                "location": {
                    "street": "5696 rue du 8 mai 1945",
                    "city": "epautheyres",
                    "state": "jura",
                    "zip": 1826
                },
                "email": "noham.lecomte@example.com",
                "username": "greenmeercat430",
                "password": "awesome",
                "salt": "l4Rhs4JK",
                "md5": "4c37a76b6c6887549c80e1f3aa14027a",
                "sha1": "c8f3f3d4a6c4b58080a34fc67d7c5dfe334f7997",
                "sha256": "5a2a79b3d8eb10111f71c5d59a27771651126adda239a18ce5354622283bb432",
                "registered": 1437995704,
                "dob": 1022791977,
                "phone": "(900)-243-4218",
                "cell": "(209)-274-3895",
                "AVS": "756.OIAC.TUCR.11",
                "picture": {
                    "large": "https://randomuser.me/api/portraits/men/21.jpg",
                    "medium": "https://randomuser.me/api/portraits/med/men/21.jpg",
                    "thumbnail": "https://randomuser.me/api/portraits/thumb/men/21.jpg"
                }
            }
        }
    ],
    "nationality": "CH",
    "seed": "e9c150623f05718403",
    "version": "0.8"
}
+-----------+--------------------+------------------+-------+
|nationality|             results|              seed|version|
+-----------+--------------------+------------------+-------+
|         CH|[[[756.TVFY.SFLS....|e9c150623f05718403|    0.8|
+-----------+--------------------+------------------+-------+

root
 |-- nationality: string (nullable = true)
 |-- results: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- user: struct (nullable = true)
 |    |    |    |-- AVS: string (nullable = true)
 |    |    |    |-- cell: string (nullable = true)
 |    |    |    |-- dob: long (nullable = true)
 |    |    |    |-- email: string (nullable = true)
 |    |    |    |-- gender: string (nullable = true)
 |    |    |    |-- location: struct (nullable = true)
 |    |    |    |    |-- city: string (nullable = true)
 |    |    |    |    |-- state: string (nullable = true)
 |    |    |    |    |-- street: string (nullable = true)
 |    |    |    |    |-- zip: long (nullable = true)
 |    |    |    |-- md5: string (nullable = true)
 |    |    |    |-- name: struct (nullable = true)
 |    |    |    |    |-- first: string (nullable = true)
 |    |    |    |    |-- last: string (nullable = true)
 |    |    |    |    |-- title: string (nullable = true)
 |    |    |    |-- password: string (nullable = true)
 |    |    |    |-- phone: string (nullable = true)
 |    |    |    |-- picture: struct (nullable = true)
 |    |    |    |    |-- large: string (nullable = true)
 |    |    |    |    |-- medium: string (nullable = true)
 |    |    |    |    |-- thumbnail: string (nullable = true)
 |    |    |    |-- registered: long (nullable = true)
 |    |    |    |-- salt: string (nullable = true)
 |    |    |    |-- sha1: string (nullable = true)
 |    |    |    |-- sha256: string (nullable = true)
 |    |    |    |-- username: string (nullable = true)
 |-- seed: string (nullable = true)
 |-- version: string (nullable = true)

====dataframe conversion=====
+-----------+--------------------+------------------+-------+
|nationality|             results|              seed|version|
+-----------+--------------------+------------------+-------+
|         CH|[[756.TVFY.SFLS.5...|e9c150623f05718403|    0.8|
|         CH|[[756.NVYK.MSJW.7...|e9c150623f05718403|    0.8|
|         CH|[[756.BPRY.LMXI.9...|e9c150623f05718403|    0.8|
|         CH|[[756.ZEMG.NJPE.9...|e9c150623f05718403|    0.8|
|         CH|[[756.TWLW.ITLJ.6...|e9c150623f05718403|    0.8|
|         CH|[[756.WETD.EOOD.1...|e9c150623f05718403|    0.8|
|         CH|[[756.HZPY.QTQH.7...|e9c150623f05718403|    0.8|
|         CH|[[756.YJPX.PAWU.8...|e9c150623f05718403|    0.8|
|         CH|[[756.HDFZ.JUPZ.4...|e9c150623f05718403|    0.8|
|         CH|[[756.OIAC.TUCR.1...|e9c150623f05718403|    0.8|
+-----------+--------------------+------------------+-------+

root
 |-- nationality: string (nullable = true)
 |-- results: struct (nullable = true)
 |    |-- user: struct (nullable = true)
 |    |    |-- AVS: string (nullable = true)
 |    |    |-- cell: string (nullable = true)
 |    |    |-- dob: long (nullable = true)
 |    |    |-- email: string (nullable = true)
 |    |    |-- gender: string (nullable = true)
 |    |    |-- location: struct (nullable = true)
 |    |    |    |-- city: string (nullable = true)
 |    |    |    |-- state: string (nullable = true)
 |    |    |    |-- street: string (nullable = true)
 |    |    |    |-- zip: long (nullable = true)
 |    |    |-- md5: string (nullable = true)
 |    |    |-- name: struct (nullable = true)
 |    |    |    |-- first: string (nullable = true)
 |    |    |    |-- last: string (nullable = true)
 |    |    |    |-- title: string (nullable = true)
 |    |    |-- password: string (nullable = true)
 |    |    |-- phone: string (nullable = true)
 |    |    |-- picture: struct (nullable = true)
 |    |    |    |-- large: string (nullable = true)
 |    |    |    |-- medium: string (nullable = true)
 |    |    |    |-- thumbnail: string (nullable = true)
 |    |    |-- registered: long (nullable = true)
 |    |    |-- salt: string (nullable = true)
 |    |    |-- sha1: string (nullable = true)
 |    |    |-- sha256: string (nullable = true)
 |    |    |-- username: string (nullable = true)
 |-- seed: string (nullable = true)
 |-- version: string (nullable = true)

+-------+--------+--------+----------------+
|  first|    last|password|        username|
+-------+--------+--------+----------------+
| andra|  martin|illusion|  blueleopard942|
|  fabio| lemoine|  barfly|orangemeercat822|
| julien|  robert|cowboys1|     lazywolf863|
|   nora|   caron|   sssss|silverleopard906|
|mawenn|da silva| koolaid|  goldentiger200|
|estelle|  hubert|  sluggo|   purpleduck360|
|   nal|  hubert|  ferris|     bluebear919|
| romane|   morel|novifarm|     bigkoala295|
|    to|  renaud|  active|   yellowlion593|
|  noham| lecomte| awesome| greenmeercat430|
+-------+--------+--------+----------------+

root
 |-- first: string (nullable = true)
 |-- last: string (nullable = true)
 |-- password: string (nullable = true)
 |-- username: string (nullable = true)

4
====csv write===
=== data written ====
=======================================================

Assignment Tasks:
-----------------
=========================================
Task 1 ---- Flatten users.json completely
=========================================

package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.functions.upper
import org.apache.spark.sql.catalyst.expressions.Upper
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window
import scala.io.Source

object project13 {
	def main(args:Array[String]):Unit={
			val conf = new SparkConf().setAppName("first").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder().getOrCreate()
					import spark.implicits._

					val df = spark.read.format("json").option("multiline","true").load("file:///D:/data/users.json")
					
					df.show()
					df.printSchema()
					
					val df1 = df.withColumn("users",expr("explode(users)"))
					
					df1.show()
					df1.printSchema()
					
					val df2 = df1.select("users.*")
					
					df2.show()
					df2.printSchema()
					
					
					
	      }
}

------------------
Out put:
--------
+--------------------+
|               users|
+--------------------+
|[[krish.lee@learn...|
+--------------------+

root
 |-- users: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- emailAddress: string (nullable = true)
 |    |    |-- firstName: string (nullable = true)
 |    |    |-- lastName: string (nullable = true)
 |    |    |-- phoneNumber: long (nullable = true)
 |    |    |-- userId: long (nullable = true)

+--------------------+
|               users|
+--------------------+
|[krish.lee@learni...|
|[racks.jacson@lea...|
|[denial.roast@lea...|
|[devid.neo@learni...|
|[jone.mac@learnin...|
+--------------------+

root
 |-- users: struct (nullable = true)
 |    |-- emailAddress: string (nullable = true)
 |    |-- firstName: string (nullable = true)
 |    |-- lastName: string (nullable = true)
 |    |-- phoneNumber: long (nullable = true)
 |    |-- userId: long (nullable = true)

+--------------------+---------+--------+-----------+------+
|        emailAddress|firstName|lastName|phoneNumber|userId|
+--------------------+---------+--------+-----------+------+
|krish.lee@learnin...|    Krish|     Lee|     123456|     1|
|racks.jacson@lear...|    racks|  jacson|     123456|     2|
|denial.roast@lear...|   denial|   roast|   33333333|     3|
|devid.neo@learnin...|    devid|     neo|  222222222|     4|
|jone.mac@learning...|     jone|     mac|  111111111|     5|
+--------------------+---------+--------+-----------+------+

root
 |-- emailAddress: string (nullable = true)
 |-- firstName: string (nullable = true)
 |-- lastName: string (nullable = true)
 |-- phoneNumber: long (nullable = true)
 |-- userId: long (nullable = true)

=======================================
Task 2 ----- Revert back the array file

package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.functions.upper
import org.apache.spark.sql.catalyst.expressions.Upper
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window
import scala.io.Source

object project13 {
	def main(args:Array[String]):Unit={
			val conf = new SparkConf().setAppName("first").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder().getOrCreate()
					import spark.implicits._

					val df = spark
					          .read
					          .format("json")
					          .option("multiline","true")
					          .load("file:///D:/data/jc1.json")
					          
					          
					 df.show()
					 df.printSchema()
					          
					                    
					val flattendf = df
					                  .withColumn("Students",expr("explode(Students)"))        
					 
          flattendf.show()
          flattendf.printSchema()

          val complexdf = flattendf.groupBy("Years","org","trainer")
          .agg( collect_list("Students").as("Students"))
					 
					          
          complexdf.show()
          complexdf.printSchema()
					
	      }
}

-------------
Out put:
--------
+--------------+-----+--------+-------+
|      Students|Years|     org|trainer|
+--------------+-----+--------+-------+
|[ajay, Rajesh]|    4|zeyobron|    sai|
+--------------+-----+--------+-------+

root
 |-- Students: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- Years: long (nullable = true)
 |-- org: string (nullable = true)
 |-- trainer: string (nullable = true)

+--------+-----+--------+-------+
|Students|Years|     org|trainer|
+--------+-----+--------+-------+
|    ajay|    4|zeyobron|    sai|
|  Rajesh|    4|zeyobron|    sai|
+--------+-----+--------+-------+

root
 |-- Students: string (nullable = true)
 |-- Years: long (nullable = true)
 |-- org: string (nullable = true)
 |-- trainer: string (nullable = true)

+-----+--------+-------+--------------+
|Years|     org|trainer|      Students|
+-----+--------+-------+--------------+
|    4|zeyobron|    sai|[ajay, Rajesh]|
+-----+--------+-------+--------------+

root
 |-- Years: long (nullable = true)
 |-- org: string (nullable = true)
 |-- trainer: string (nullable = true)
 |-- Students: array (nullable = true)
 |    |-- element: string (containsNull = true)
 
 
============================================================================================
Task 3(Optional)  ----- Whenever u get some time.. Pick a file from complexjson and solve it









============================================================================================


============================================================================================
SQL Doc Topics

select * from table
select columns from filters
select single filter
Select Multi Column Filter
Select Multi value filter
like Filter
Not filters
null filters
between
replace
min
max
avg
Sum
Count
Case when conditions and Multiple
concat
concat_ws
lower
lpad
ceil
Round
cast
Coalesce
ltrim
rpad
repeat
rtrim
distinct
substring
substring_index
split
trim
upper
subquery
constraint and 
union
unionAll
join
OrderBy
GroupBy
groupby (Min,max,average,sum,count)
having
from_unixtime - unix_timestamp
Window (Rank,DenseRank,row_number,lead,lag,ntile,percent_rank())
Explode
explode outer
Joins  (       )
Indexes
collect_list
collect_set
analytics functions
Exists and Not exists
minus
self join --- Test
limit
Primary key
foreign Key
Decode


Buffer

Self Join
Offset buffer
UDF ---


============================================================================================


====================
31-12-2022
====================
9 Steps --- Project

1) Create an eclipse project
2) Add avro jar and read projectsample.avro
3) Read URL data and convert that to Dataframe

	https://randomuser.me/api/0.8/?results=10

4) Flatten the URL and do not select (COLUMN CAPS)

5) Remove numericals from username column

6) Do left broadcast join (avrodf and numerical removed url df)

7) From dataframe of 6th Step -- Create two dataframes

	val availableCustomer    ==== nationality is not null
	val notavailabelcustomer ==== nationality is  null


===========================
package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.functions.upper
import org.apache.spark.sql.catalyst.expressions.Upper
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window
import scala.io.Source
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions._

object obj {

	def main(args:Array[String]):Unit={


			val conf = new SparkConf().setAppName("ES").setMaster("local[*]")

					val sc = new SparkContext(conf)
					sc.setLogLevel("Error")


					val spark = SparkSession.builder()



					.getOrCreate()

					import spark.implicits._


					println
					println
					println
					println
					println
					println("======================= Step 2 ========Raw data=============================================")
					println
					println
					println
					println
					println




					val data = spark.read.format("avro")
					.load("file:///D://data//projectsample.avro")

					data.show()




					println
					println
					println
					println
					println
					println("======================== Step 3 ========Url data=============================================")
					println
					println
					println
					println
					println


					val html = Source.fromURL("https://randomuser.me/api/0.8/?results=500")
					val s = html.mkString
					//println(s)


					val urldf = spark.read.json(sc.parallelize(List(s)))
					urldf.show()




					println
					println
					println
					println
					println
					println("========================step 4 flatten dataframe=============================================")
					println
					println
					println
					println
					println

					val flatdf = urldf.withColumn("results",explode(col("results"))).select("nationality","seed","version",
							"results.user.username","results.user.cell","results.user.dob","results.user.email",
							"results.user.gender","results.user.location.city","results.user.location.state",
							"results.user.location.street","results.user.location.zip","results.user.md5",
							"results.user.name.first","results.user.name.last","results.user.name.title",
							"results.user.password","results.user.phone","results.user.picture.large","results.user.picture.medium"
							,"results.user.picture.thumbnail","results.user.registered","results.user.salt","results.user.sha1"
							,"results.user.sha256")
					flatdf.show()






					println
					println
					println
					println
					println
					println("========================step 5 removed numericals Dataframe=============================================")
					println
					println
					println
					println
					println

					val rm=flatdf.withColumn("username",regexp_replace(col("username"),  "([0-9])", ""))
					rm.show()







					println
					println
					println
					println
					println
					println("====================== Step 6 =========Joined Dataframe=============================================")
					println
					println
					println
					println
					println
					val joindf = data.join(broadcast(rm),Seq("username"),"left")


					joindf.show()


					println
					println
					println
					println
					println("=================== Step 7 a ============Not available customers=============================================")
					println
					println
					println
					println
					println
					println


					val dfnull = joindf.filter(col("nationality").isNull)


					val dfnotnull=joindf.filter(col("nationality").isNotNull)



					dfnull.show()




					println
					
					
					println
					println
					println
					println("==================  Step 7 b =============available customers=============================================")
					println
					println
					println
					println
					println
					println




					dfnotnull.show()





					println
					println
					println
					println
					println("=============== Step 8 ================Null handled dataframe=============================================")
					println
					println
					println
					println
					println
					println



					val replacenull= dfnull.na.fill("Not Available").na.fill(0)
					replacenull.show()


					println
					println
					println
					println
					println("=============== Step 9 a ================not available customers with current date dataframe=============================================")
					println
					println
					println
					println
					println
					println



					val replacenull_with_current_date=replacenull.withColumn("current_date",current_date)

					replacenull_with_current_date.show()

					println
					println
					println



					println
					println
					println
					println
					println("=============== Step 9 b ================available customers with current date dataframe=============================================")
					println
					println
					println
					println
					println
					println



					val notnull_with_current_date=dfnotnull.withColumn("current_date",current_date)


					notnull_with_current_date.show()




					notnull_with_current_date.write.format("parquet").mode("append").partitionBy("current_date")
					            .save("file:///C:/data/project/availablecustomer")
					
					
					
					replacenull_with_current_date.write.format("parquet").mode("append").partitionBy("current_date")
					            .save("file:///C:/data/project/notavailablecustomer")
					
					
					
					
					
					




	}

}


=============
Out put:
--------
======================= Step 2 ========Raw data=============================================





+---+-------------+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+
| id|     username|amount|             ip|            createdt|value|score|regioncode|status|method|                 key|count|                type|                site|statuscode|
+---+-------------+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+
|  1|beautifulbear| 01743|  68.688.326.58|13/Jun/2012:10:55...|   10|   55|        45| -0500|   GET|           /products|    0|   /product/product4|Mozilla/4.0 (comp...|       100|
|  2|beautifulbear| 75756|  361.631.17.30|01/Nov/2011:10:53...|   10|   53|        01| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|
|  3|beautifulbear| 05901|   11.308.46.48|21/Jun/2012:01:12...|   01|   12|        33| -0500|   GET|               /news|    0|    /partners/resell|Mozilla/5.0 (comp...|       100|
|  4|beautifulbear| 24538|  361.631.17.30|01/Nov/2011:13:48...|   13|   48|        30| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|
|  5|beautifulbear| 08116|  322.76.611.36|30/Jun/2012:07:19...|   07|   19|        30| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|
|  6|beautifulbear| 04675|  361.631.17.30|01/Nov/2011:16:19...|   16|   19|        00| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|
|  7|beautifulbear| 06575| 367.34.686.631|01/Jul/2012:00:02...|   00|   02|        00| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|
|  8|beautifulbear| 27382|   88.633.11.47|04/Nov/2011:13:00...|   13|   00|        30| -0500|   GET|        /feeds/press|    0|                   -|Apple-PubSub/65.12.1|       100|
|  9|beautifulbear| 09934|   11.308.46.48|01/Jul/2012:23:13...|   23|   13|        53| -0500|   GET|           /partners|    0|   /product/product2|Mozilla/5.0 (comp...|       100|
| 10|beautifulbear| 01716|683.615.622.618|05/Nov/2011:22:03...|   22|   03|        01| -0500|   GET|    /partners/resell|    0|    /partners/resell|                null|       100|
| 11|beautifulbear| 55195| 682.62.387.672|02/Jul/2012:10:17...|   10|   17|        48| -0500|   GET|/download/downloa...|    0|/download/downloa...|Mozilla/5.0 (Wind...|       100|
| 12|beautifulbear| 69809|  43.68.686.668|09/Nov/2011:05:25...|   05|   25|        00| -0500|   GET|        /feeds/press|    0|                   -|  Apple-PubSub/65.11|       100|
| 13|beautifulbear| 78800|321.336.372.320|06/Jul/2012:01:41...|   01|   41|        00| -0500|   GET|        /feeds/press|    0|                   -|Mozilla/5.0 (Wind...|       100|
| 14|beautifulbear| 58994|  361.631.17.30|16/Nov/2011:00:50...|   00|   50|        00| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|
| 15|beautifulbear| 94543| 361.638.668.62|13/Jul/2012:23:44...|   23|   44|        30| -0500|   GET|            /ad/save|    0|                   -|Mozilla/5.0 (Twic...|       100|
| 16|beautifulbear| 33695|   325.87.75.36|16/Nov/2011:04:58...|   04|   58|        30| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|
| 17|beautifulbear| 01755|  18.332.18.672|16/Jul/2012:03:20...|   03|   20|        30| -0500|   GET|        /feeds/press|    0|                   -|Apple-PubSub/65.12.1|       100|
| 18|beautifulbear| 01784|   361.321.73.6|17/Nov/2011:13:08...|   13|   08|        30| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|
| 19|beautifulbear| 09671|   364.63.61.83|23/Jul/2012:17:00...|   17|   00|        37| -0500|   GET|   /product/product3|    0|      /docs/doc5.pdf|Mozilla/5.0 (X11;...|       100|
| 20|beautifulbear| 04288|    13.53.52.13|26/Nov/2011:19:16...|   19|   16|        00| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|
+---+-------------+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+
only showing top 20 rows






======================== Step 3 ========Url data=============================================





+-----------+--------------------+------------------+-------+
|nationality|             results|              seed|version|
+-----------+--------------------+------------------+-------+
|         IR|[[[0917-637-3940,...|c28a5188e87e1ccf0a|    0.8|
+-----------+--------------------+------------------+-------+






========================step 4 flatten dataframe=============================================





+-----------+------------------+-------+-----------------+-------------+----------+--------------------+------+---------+-------------------+--------------------+-----+--------------------+--------+-----------+-----+--------+------------+--------------------+--------------------+--------------------+----------+--------+--------------------+--------------------+
|nationality|              seed|version|         username|         cell|       dob|               email|gender|     city|              state|              street|  zip|                 md5|   first|       last|title|password|       phone|               large|              medium|           thumbnail|registered|    salt|                sha1|              sha256|
+-----------+------------------+-------+-----------------+-------------+----------+--------------------+------+---------+-------------------+--------------------+-----+--------------------+--------+-----------+-----+--------+------------+--------------------+--------------------+--------------------+----------+--------+--------------------+--------------------+
|         IR|c28a5188e87e1ccf0a|    0.8|    orangebear992|0917-637-3940| 734871555|. @exam...|female|    |             |   4071  |38319|7d48cb9d335c74f60...|    |     |  mrs| porsche|006-67179486|https://randomuse...|https://randomuse...|https://randomuse...| 924650346|qEW43GjN|2cd77003dd116a1fb...|9eb94b33cf302a99d...|
|         IR|c28a5188e87e1ccf0a|    0.8|     heavyduck873|0960-715-5901| 290024535|.@ex...|  male|    |    |     6628  |71154|79e2d8b769ad63784...||      |   mr|  enters|058-18738366|https://randomuse...|https://randomuse...|https://randomuse...|1086161743|kfaSaAcj|085e2e7d5973a4bd4...|524b7fc3d251b4924...|
|         IR|c28a5188e87e1ccf0a|    0.8|      heavydog149|0927-746-8000|1058012734|.@examp...|  male| |   |        4380 |61793|4b756f855ec02eae3...|   |      |   mr|  errors|089-34823792|https://randomuse...|https://randomuse...|https://randomuse...|1286476539|84xUKJGM|4d082d320cbd4ff7b...|9a3b33667a470fc2f...|
|         IR|c28a5188e87e1ccf0a|    0.8|    yellowduck525|0991-439-3042|1322120648|.@examp...|  male|    |  |     6374  |54352|ec933e8804669f006...|   |      |   mr|semperfi|081-64964544|https://randomuse...|https://randomuse...|https://randomuse...|1373384721|wxzuBJ7k|9af6eaa097fa7a279...|4a2c0d400314bf0bb...|
|         IR|c28a5188e87e1ccf0a|    0.8|      redsnake141|0983-541-3287| 547915965|.@exam...|female|    |      |8427   |51459|0b6c14466f01e622f...|  |      | miss| cheetah|009-79363320|https://randomuse...|https://randomuse...|https://randomuse...|1380177303|Xns7siLh|b454921fb4cd7b3d0...|bc9f5b52ccdd590b5...|
|         IR|c28a5188e87e1ccf0a|    0.8|   goldensnake531|0940-915-2574| 944239893|.@exa...|female|      |              |          5465 |19323|f6c673e481d92743e...|   |    |   ms|eastwood|011-76616695|https://randomuse...|https://randomuse...|https://randomuse...|1167345473|8Wz7SON2|9213ee170134f677d...|c24d968d688407f47...|
|         IR|c28a5188e87e1ccf0a|    0.8|      tinybear445|0957-096-0393| 213095911|.@examp...|  male|     |                |   4365  |17657|3fdd25299fda31513...|   |      |   mr|barcelon|046-81161748|https://randomuse...|https://randomuse...|https://randomuse...| 915820788|8zivkF0P|5277449600f3c8c13...|49a70e0e657a9af9e...|
|         IR|c28a5188e87e1ccf0a|    0.8|  tinyelephant174|0926-918-1100| 982692979|.@exam...|  male|     |              |5632   ...|31284|df7d5470564ced685...|    |    |   mr|beatrice|079-90740609|https://randomuse...|https://randomuse...|https://randomuse...|1178200492|6DuLV87T|20aa8a1c64e00c857...|efc5ec1688bb5a0af...|
|         IR|c28a5188e87e1ccf0a|    0.8|     greenduck907|0910-819-5226| 939413168|.@exam...|  male|    |                |4329   |37664|4413cd1a032cae340...|  |      |   mr|wolfgang|065-69270568|https://randomuse...|https://randomuse...|https://randomuse...|1176376362|7YcQZ3Rw|1f149fd430f10a779...|c4d33ea385a52163e...|
|         IR|c28a5188e87e1ccf0a|    0.8|beautifulgoose433|0993-318-1881| 744030874|.@exam...|female|     |              | 6663  17 |59512|8bc7a1bf29ce0877f...|    |    |   ms|   fresh|080-68606112|https://randomuse...|https://randomuse...|https://randomuse...|1043977538|sLF1y3ol|36d1288d63368e27c...|f65b6e344d3a1974b...|
|         IR|c28a5188e87e1ccf0a|    0.8|       bigfish755|0982-151-6076| 785472053|.@examp...|female|      |              |    7195  |27206|d878f8f0f414c9354...|    |     | miss| lazarus|076-69597970|https://randomuse...|https://randomuse...|https://randomuse...|1196426604|0hYz3TaN|fc6afe53c714743d2...|b57bac9252ffb373f...|
|         IR|c28a5188e87e1ccf0a|    0.8|    greenmouse984|0970-254-8095| 746654845|.@examp...|female|   |   |          2011 |59173|d931938c333e0ba42...|   |      |   ms|  zzzzzz|066-26746121|https://randomuse...|https://randomuse...|https://randomuse...| 956617900|J5TdC8Zq|ca0a10890fb82db6c...|7b12031a291e99c1c...|
|         IR|c28a5188e87e1ccf0a|    0.8|  ticklishswan573|0992-842-0712|1046988745|.@example...|  male| |              |    6968  |33587|629346735c68af2bd...|    |       |   mr|  flyers|066-95589142|https://randomuse...|https://randomuse...|https://randomuse...| 940880362|YP3i2hNd|cefb24c198b8b9a0c...|9229be362e3c4ad29...|
|         IR|c28a5188e87e1ccf0a|    0.8|    yellowwolf663|0936-984-2004| 285292393|. @...|female|    |             |     6854  |87584|1ccfed1b506dfb2cf...|    | | miss|  diaper|076-42056034|https://randomuse...|https://randomuse...|https://randomuse...|1118637692|ztjXs8dB|68dcd248bedf170da...|1cc30a8612736bc45...|
|         IR|c28a5188e87e1ccf0a|    0.8|    redleopard130|0924-911-9520|  18269654|.@examp...|female|      |      |   2322  |88251|044bff81099f7c2fe...|   |      |  mrs|military|067-74855368|https://randomuse...|https://randomuse...|https://randomuse...| 932024038|ILEu36K2|af1e3d45d6e4548c5...|32c28b2de4310c1be...|
|         IR|c28a5188e87e1ccf0a|    0.8|  blackleopard982|0938-593-1590| 925138291|.@examp...|female| |              |    8432  |21432|86e3a9d2a817f82a6...|   |      |   ms|    yang|069-07049516|https://randomuse...|https://randomuse...|https://randomuse...|1040221137|WhiS9gzW|0dd412a179b9c5eee...|8b2c1da2f15c7b004...|
|         IR|c28a5188e87e1ccf0a|    0.8|       tinycat520|0984-587-2680|  83408984|.@exa...|  male|    |              |8368   |44450|63a488d5024849356...| |      |   mr|    this|089-64892407|https://randomuse...|https://randomuse...|https://randomuse...|1115363675|103cTdid|9fc4935c44bf11df7...|e207a3cbe19c95c16...|
|         IR|c28a5188e87e1ccf0a|    0.8| tinybutterfly775|0922-458-9459| 537560972|.@exampl...|female|      |   | 2588  |49075|fa295065a6e06295d...|    |      | miss|    rush|005-01144954|https://randomuse...|https://randomuse...|https://randomuse...| 940869853|C3kC7600|1dbdfbde9b158553f...|9f0b85d9c52b26bc8...|
|         IR|c28a5188e87e1ccf0a|    0.8|     whitefrog405|0973-155-7004|1259032241|.@example.com|female|   |             |      5995  |64436|02f99948df9a7f6a8...|     |       |  mrs|  stinks|011-98158575|https://randomuse...|https://randomuse...|https://randomuse...| 919544238|B9Tmn7lI|c8f7eac08720ee5fb...|e5bc398648f985321...|
|         IR|c28a5188e87e1ccf0a|    0.8|    crazysnake110|0985-858-1822| 996339130|.@exampl...|female||               | 8939   |52917|aaf5414b2b49ccccd...|    |      |  mrs| springs|010-01414930|https://randomuse...|https://randomuse...|https://randomuse...| 952936434|xrCPEpa4|30830949fb8d3d9aa...|d044c9f3277d137f2...|
+-----------+------------------+-------+-----------------+-------------+----------+--------------------+------+---------+-------------------+--------------------+-----+--------------------+--------+-----------+-----+--------+------------+--------------------+--------------------+--------------------+----------+--------+--------------------+--------------------+
only showing top 20 rows






========================step 5 removed numericals Dataframe=============================================





+-----------+------------------+-------+--------------+-------------+----------+--------------------+------+---------+-------------------+--------------------+-----+--------------------+--------+-----------+-----+--------+------------+--------------------+--------------------+--------------------+----------+--------+--------------------+--------------------+
|nationality|              seed|version|      username|         cell|       dob|               email|gender|     city|              state|              street|  zip|                 md5|   first|       last|title|password|       phone|               large|              medium|           thumbnail|registered|    salt|                sha1|              sha256|
+-----------+------------------+-------+--------------+-------------+----------+--------------------+------+---------+-------------------+--------------------+-----+--------------------+--------+-----------+-----+--------+------------+--------------------+--------------------+--------------------+----------+--------+--------------------+--------------------+
|         IR|c28a5188e87e1ccf0a|    0.8|    orangebear|0917-637-3940| 734871555|. @exam...|female|    |             |   4071  |38319|7d48cb9d335c74f60...|    |     |  mrs| porsche|006-67179486|https://randomuse...|https://randomuse...|https://randomuse...| 924650346|qEW43GjN|2cd77003dd116a1fb...|9eb94b33cf302a99d...|
|         IR|c28a5188e87e1ccf0a|    0.8|     heavyduck|0960-715-5901| 290024535|.@ex...|  male|    |    |     6628  |71154|79e2d8b769ad63784...||      |   mr|  enters|058-18738366|https://randomuse...|https://randomuse...|https://randomuse...|1086161743|kfaSaAcj|085e2e7d5973a4bd4...|524b7fc3d251b4924...|
|         IR|c28a5188e87e1ccf0a|    0.8|      heavydog|0927-746-8000|1058012734|.@examp...|  male| |   |        4380 |61793|4b756f855ec02eae3...|   |      |   mr|  errors|089-34823792|https://randomuse...|https://randomuse...|https://randomuse...|1286476539|84xUKJGM|4d082d320cbd4ff7b...|9a3b33667a470fc2f...|
|         IR|c28a5188e87e1ccf0a|    0.8|    yellowduck|0991-439-3042|1322120648|.@examp...|  male|    |  |     6374  |54352|ec933e8804669f006...|   |      |   mr|semperfi|081-64964544|https://randomuse...|https://randomuse...|https://randomuse...|1373384721|wxzuBJ7k|9af6eaa097fa7a279...|4a2c0d400314bf0bb...|
|         IR|c28a5188e87e1ccf0a|    0.8|      redsnake|0983-541-3287| 547915965|.@exam...|female|    |      |8427   |51459|0b6c14466f01e622f...|  |      | miss| cheetah|009-79363320|https://randomuse...|https://randomuse...|https://randomuse...|1380177303|Xns7siLh|b454921fb4cd7b3d0...|bc9f5b52ccdd590b5...|
|         IR|c28a5188e87e1ccf0a|    0.8|   goldensnake|0940-915-2574| 944239893|.@exa...|female|      |              |          5465 |19323|f6c673e481d92743e...|   |    |   ms|eastwood|011-76616695|https://randomuse...|https://randomuse...|https://randomuse...|1167345473|8Wz7SON2|9213ee170134f677d...|c24d968d688407f47...|
|         IR|c28a5188e87e1ccf0a|    0.8|      tinybear|0957-096-0393| 213095911|.@examp...|  male|     |                |   4365  |17657|3fdd25299fda31513...|   |      |   mr|barcelon|046-81161748|https://randomuse...|https://randomuse...|https://randomuse...| 915820788|8zivkF0P|5277449600f3c8c13...|49a70e0e657a9af9e...|
|         IR|c28a5188e87e1ccf0a|    0.8|  tinyelephant|0926-918-1100| 982692979|.@exam...|  male|     |              |5632   ...|31284|df7d5470564ced685...|    |    |   mr|beatrice|079-90740609|https://randomuse...|https://randomuse...|https://randomuse...|1178200492|6DuLV87T|20aa8a1c64e00c857...|efc5ec1688bb5a0af...|
|         IR|c28a5188e87e1ccf0a|    0.8|     greenduck|0910-819-5226| 939413168|.@exam...|  male|    |                |4329   |37664|4413cd1a032cae340...|  |      |   mr|wolfgang|065-69270568|https://randomuse...|https://randomuse...|https://randomuse...|1176376362|7YcQZ3Rw|1f149fd430f10a779...|c4d33ea385a52163e...|
|         IR|c28a5188e87e1ccf0a|    0.8|beautifulgoose|0993-318-1881| 744030874|.@exam...|female|     |              | 6663  17 |59512|8bc7a1bf29ce0877f...|    |    |   ms|   fresh|080-68606112|https://randomuse...|https://randomuse...|https://randomuse...|1043977538|sLF1y3ol|36d1288d63368e27c...|f65b6e344d3a1974b...|
|         IR|c28a5188e87e1ccf0a|    0.8|       bigfish|0982-151-6076| 785472053|.@examp...|female|      |              |    7195  |27206|d878f8f0f414c9354...|    |     | miss| lazarus|076-69597970|https://randomuse...|https://randomuse...|https://randomuse...|1196426604|0hYz3TaN|fc6afe53c714743d2...|b57bac9252ffb373f...|
|         IR|c28a5188e87e1ccf0a|    0.8|    greenmouse|0970-254-8095| 746654845|.@examp...|female|   |   |          2011 |59173|d931938c333e0ba42...|   |      |   ms|  zzzzzz|066-26746121|https://randomuse...|https://randomuse...|https://randomuse...| 956617900|J5TdC8Zq|ca0a10890fb82db6c...|7b12031a291e99c1c...|
|         IR|c28a5188e87e1ccf0a|    0.8|  ticklishswan|0992-842-0712|1046988745|.@example...|  male| |              |    6968  |33587|629346735c68af2bd...|    |       |   mr|  flyers|066-95589142|https://randomuse...|https://randomuse...|https://randomuse...| 940880362|YP3i2hNd|cefb24c198b8b9a0c...|9229be362e3c4ad29...|
|         IR|c28a5188e87e1ccf0a|    0.8|    yellowwolf|0936-984-2004| 285292393|. @...|female|    |             |     6854  |87584|1ccfed1b506dfb2cf...|    | | miss|  diaper|076-42056034|https://randomuse...|https://randomuse...|https://randomuse...|1118637692|ztjXs8dB|68dcd248bedf170da...|1cc30a8612736bc45...|
|         IR|c28a5188e87e1ccf0a|    0.8|    redleopard|0924-911-9520|  18269654|.@examp...|female|      |      |   2322  |88251|044bff81099f7c2fe...|   |      |  mrs|military|067-74855368|https://randomuse...|https://randomuse...|https://randomuse...| 932024038|ILEu36K2|af1e3d45d6e4548c5...|32c28b2de4310c1be...|
|         IR|c28a5188e87e1ccf0a|    0.8|  blackleopard|0938-593-1590| 925138291|.@examp...|female| |              |    8432  |21432|86e3a9d2a817f82a6...|   |      |   ms|    yang|069-07049516|https://randomuse...|https://randomuse...|https://randomuse...|1040221137|WhiS9gzW|0dd412a179b9c5eee...|8b2c1da2f15c7b004...|
|         IR|c28a5188e87e1ccf0a|    0.8|       tinycat|0984-587-2680|  83408984|.@exa...|  male|    |              |8368   |44450|63a488d5024849356...| |      |   mr|    this|089-64892407|https://randomuse...|https://randomuse...|https://randomuse...|1115363675|103cTdid|9fc4935c44bf11df7...|e207a3cbe19c95c16...|
|         IR|c28a5188e87e1ccf0a|    0.8| tinybutterfly|0922-458-9459| 537560972|.@exampl...|female|      |   | 2588  |49075|fa295065a6e06295d...|    |      | miss|    rush|005-01144954|https://randomuse...|https://randomuse...|https://randomuse...| 940869853|C3kC7600|1dbdfbde9b158553f...|9f0b85d9c52b26bc8...|
|         IR|c28a5188e87e1ccf0a|    0.8|     whitefrog|0973-155-7004|1259032241|.@example.com|female|   |             |      5995  |64436|02f99948df9a7f6a8...|     |       |  mrs|  stinks|011-98158575|https://randomuse...|https://randomuse...|https://randomuse...| 919544238|B9Tmn7lI|c8f7eac08720ee5fb...|e5bc398648f985321...|
|         IR|c28a5188e87e1ccf0a|    0.8|    crazysnake|0985-858-1822| 996339130|.@exampl...|female||               | 8939   |52917|aaf5414b2b49ccccd...|    |      |  mrs| springs|010-01414930|https://randomuse...|https://randomuse...|https://randomuse...| 952936434|xrCPEpa4|30830949fb8d3d9aa...|d044c9f3277d137f2...|
+-----------+------------------+-------+--------------+-------------+----------+--------------------+------+---------+-------------------+--------------------+-----+--------------------+--------+-----------+-----+--------+------------+--------------------+--------------------+--------------------+----------+--------+--------------------+--------------------+
only showing top 20 rows






====================== Step 6 =========Joined Dataframe=============================================





+-------------+---+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+-----------+----+-------+----+----+-----+------+----+-----+------+----+----+-----+----+-----+--------+-----+-----+------+---------+----------+----+----+------+
|     username| id|amount|             ip|            createdt|value|score|regioncode|status|method|                 key|count|                type|                site|statuscode|nationality|seed|version|cell| dob|email|gender|city|state|street| zip| md5|first|last|title|password|phone|large|medium|thumbnail|registered|salt|sha1|sha256|
+-------------+---+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+-----------+----+-------+----+----+-----+------+----+-----+------+----+----+-----+----+-----+--------+-----+-----+------+---------+----------+----+----+------+
|beautifulbear|  1| 01743|  68.688.326.58|13/Jun/2012:10:55...|   10|   55|        45| -0500|   GET|           /products|    0|   /product/product4|Mozilla/4.0 (comp...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  2| 75756|  361.631.17.30|01/Nov/2011:10:53...|   10|   53|        01| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  3| 05901|   11.308.46.48|21/Jun/2012:01:12...|   01|   12|        33| -0500|   GET|               /news|    0|    /partners/resell|Mozilla/5.0 (comp...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  4| 24538|  361.631.17.30|01/Nov/2011:13:48...|   13|   48|        30| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  5| 08116|  322.76.611.36|30/Jun/2012:07:19...|   07|   19|        30| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  6| 04675|  361.631.17.30|01/Nov/2011:16:19...|   16|   19|        00| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  7| 06575| 367.34.686.631|01/Jul/2012:00:02...|   00|   02|        00| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  8| 27382|   88.633.11.47|04/Nov/2011:13:00...|   13|   00|        30| -0500|   GET|        /feeds/press|    0|                   -|Apple-PubSub/65.12.1|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  9| 09934|   11.308.46.48|01/Jul/2012:23:13...|   23|   13|        53| -0500|   GET|           /partners|    0|   /product/product2|Mozilla/5.0 (comp...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 10| 01716|683.615.622.618|05/Nov/2011:22:03...|   22|   03|        01| -0500|   GET|    /partners/resell|    0|    /partners/resell|                null|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 11| 55195| 682.62.387.672|02/Jul/2012:10:17...|   10|   17|        48| -0500|   GET|/download/downloa...|    0|/download/downloa...|Mozilla/5.0 (Wind...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 12| 69809|  43.68.686.668|09/Nov/2011:05:25...|   05|   25|        00| -0500|   GET|        /feeds/press|    0|                   -|  Apple-PubSub/65.11|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 13| 78800|321.336.372.320|06/Jul/2012:01:41...|   01|   41|        00| -0500|   GET|        /feeds/press|    0|                   -|Mozilla/5.0 (Wind...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 14| 58994|  361.631.17.30|16/Nov/2011:00:50...|   00|   50|        00| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 15| 94543| 361.638.668.62|13/Jul/2012:23:44...|   23|   44|        30| -0500|   GET|            /ad/save|    0|                   -|Mozilla/5.0 (Twic...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 16| 33695|   325.87.75.36|16/Nov/2011:04:58...|   04|   58|        30| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 17| 01755|  18.332.18.672|16/Jul/2012:03:20...|   03|   20|        30| -0500|   GET|        /feeds/press|    0|                   -|Apple-PubSub/65.12.1|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 18| 01784|   361.321.73.6|17/Nov/2011:13:08...|   13|   08|        30| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 19| 09671|   364.63.61.83|23/Jul/2012:17:00...|   17|   00|        37| -0500|   GET|   /product/product3|    0|      /docs/doc5.pdf|Mozilla/5.0 (X11;...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 20| 04288|    13.53.52.13|26/Nov/2011:19:16...|   19|   16|        00| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
+-------------+---+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+-----------+----+-------+----+----+-----+------+----+-----+------+----+----+-----+----+-----+--------+-----+-----+------+---------+----------+----+----+------+
only showing top 20 rows





=================== Step 7 a ============Not available customers=============================================






+-------------+---+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+-----------+----+-------+----+----+-----+------+----+-----+------+----+----+-----+----+-----+--------+-----+-----+------+---------+----------+----+----+------+
|     username| id|amount|             ip|            createdt|value|score|regioncode|status|method|                 key|count|                type|                site|statuscode|nationality|seed|version|cell| dob|email|gender|city|state|street| zip| md5|first|last|title|password|phone|large|medium|thumbnail|registered|salt|sha1|sha256|
+-------------+---+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+-----------+----+-------+----+----+-----+------+----+-----+------+----+----+-----+----+-----+--------+-----+-----+------+---------+----------+----+----+------+
|beautifulbear|  1| 01743|  68.688.326.58|13/Jun/2012:10:55...|   10|   55|        45| -0500|   GET|           /products|    0|   /product/product4|Mozilla/4.0 (comp...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  2| 75756|  361.631.17.30|01/Nov/2011:10:53...|   10|   53|        01| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  3| 05901|   11.308.46.48|21/Jun/2012:01:12...|   01|   12|        33| -0500|   GET|               /news|    0|    /partners/resell|Mozilla/5.0 (comp...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  4| 24538|  361.631.17.30|01/Nov/2011:13:48...|   13|   48|        30| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  5| 08116|  322.76.611.36|30/Jun/2012:07:19...|   07|   19|        30| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  6| 04675|  361.631.17.30|01/Nov/2011:16:19...|   16|   19|        00| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  7| 06575| 367.34.686.631|01/Jul/2012:00:02...|   00|   02|        00| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  8| 27382|   88.633.11.47|04/Nov/2011:13:00...|   13|   00|        30| -0500|   GET|        /feeds/press|    0|                   -|Apple-PubSub/65.12.1|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  9| 09934|   11.308.46.48|01/Jul/2012:23:13...|   23|   13|        53| -0500|   GET|           /partners|    0|   /product/product2|Mozilla/5.0 (comp...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 10| 01716|683.615.622.618|05/Nov/2011:22:03...|   22|   03|        01| -0500|   GET|    /partners/resell|    0|    /partners/resell|                null|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 11| 55195| 682.62.387.672|02/Jul/2012:10:17...|   10|   17|        48| -0500|   GET|/download/downloa...|    0|/download/downloa...|Mozilla/5.0 (Wind...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 12| 69809|  43.68.686.668|09/Nov/2011:05:25...|   05|   25|        00| -0500|   GET|        /feeds/press|    0|                   -|  Apple-PubSub/65.11|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 13| 78800|321.336.372.320|06/Jul/2012:01:41...|   01|   41|        00| -0500|   GET|        /feeds/press|    0|                   -|Mozilla/5.0 (Wind...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 14| 58994|  361.631.17.30|16/Nov/2011:00:50...|   00|   50|        00| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 15| 94543| 361.638.668.62|13/Jul/2012:23:44...|   23|   44|        30| -0500|   GET|            /ad/save|    0|                   -|Mozilla/5.0 (Twic...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 16| 33695|   325.87.75.36|16/Nov/2011:04:58...|   04|   58|        30| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 17| 01755|  18.332.18.672|16/Jul/2012:03:20...|   03|   20|        30| -0500|   GET|        /feeds/press|    0|                   -|Apple-PubSub/65.12.1|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 18| 01784|   361.321.73.6|17/Nov/2011:13:08...|   13|   08|        30| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 19| 09671|   364.63.61.83|23/Jul/2012:17:00...|   17|   00|        37| -0500|   GET|   /product/product3|    0|      /docs/doc5.pdf|Mozilla/5.0 (X11;...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 20| 04288|    13.53.52.13|26/Nov/2011:19:16...|   19|   16|        00| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
+-------------+---+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+-----------+----+-------+----+----+-----+------+----+-----+------+----+----+-----+----+-----+--------+-----+-----+------+---------+----------+----+----+------+
only showing top 20 rows





==================  Step 7 b =============available customers=============================================






+-------------+----+------+-------------+--------------------+-----+-----+----------+------+------+--------------------+-----+-----------------+--------------------+----------+-----------+------------------+-------+-------------+----------+--------------------+------+------+------------+--------------------+-----+--------------------+-----+------+-----+--------+------------+--------------------+--------------------+--------------------+----------+--------+--------------------+--------------------+
|     username|  id|amount|           ip|            createdt|value|score|regioncode|status|method|                 key|count|             type|                site|statuscode|nationality|              seed|version|         cell|       dob|               email|gender|  city|       state|              street|  zip|                 md5|first|  last|title|password|       phone|               large|              medium|           thumbnail|registered|    salt|                sha1|              sha256|
+-------------+----+------+-------------+--------------------+-----+-----+----------+------+------+--------------------+-----+-----------------+--------------------+----------+-----------+------------------+-------+-------------+----------+--------------------+------+------+------------+--------------------+-----+--------------------+-----+------+-----+--------+------------+--------------------+--------------------+--------------------+----------+--------+--------------------+--------------------+
|beautifulduck|2201| 66419|361.631.17.30|01/Feb/2012:13:27...|   13|   27|        08| -0500|   GET|               /demo|    0|            /demo|Jakarta Commons-H...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0907-592-2776| 735636504|.@examp...|  male| | |          5638 |50181|0a99d29a90d2b80e9...|| |   mr|  cutter|059-79002407|https://randomuse...|https://randomuse...|https://randomuse...|1301995909|8FFVKJA4|d8e5c2aa50628ede9...|9f9ae3cbd44513e70...|
|beautifulduck|2201| 66419|361.631.17.30|01/Feb/2012:13:27...|   13|   27|        08| -0500|   GET|               /demo|    0|            /demo|Jakarta Commons-H...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0974-297-2862| 255223873|.@examp...|  male| |       |8586   |64240|28fb2112dafd8dcf2...| ||   mr| golfpro|094-13140918|https://randomuse...|https://randomuse...|https://randomuse...|1393111806|Z2ypc9tB|dfd63fe3e35680257...|4fd7b8e859ab9db65...|
|beautifulduck|2201| 66419|361.631.17.30|01/Feb/2012:13:27...|   13|   27|        08| -0500|   GET|               /demo|    0|            /demo|Jakarta Commons-H...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0935-906-2550|1244525876|.@exam...|  male||      |     2478  |31839|e37bf901f4f00091b...|||   mr|  686868|064-42326749|https://randomuse...|https://randomuse...|https://randomuse...|1123054755|GWD1kQsc|c830522c905c72181...|07383cb34c38b1780...|
|beautifulduck|2202| 09311| 56.30.617.38|28/Jul/2012:08:39...|   08|   39|        04| -0500|   GET|               /demo|    0|            /demo|Jakarta Commons-H...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0907-592-2776| 735636504|.@examp...|  male| | |          5638 |50181|0a99d29a90d2b80e9...|| |   mr|  cutter|059-79002407|https://randomuse...|https://randomuse...|https://randomuse...|1301995909|8FFVKJA4|d8e5c2aa50628ede9...|9f9ae3cbd44513e70...|
|beautifulduck|2202| 09311| 56.30.617.38|28/Jul/2012:08:39...|   08|   39|        04| -0500|   GET|               /demo|    0|            /demo|Jakarta Commons-H...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0974-297-2862| 255223873|.@examp...|  male| |       |8586   |64240|28fb2112dafd8dcf2...| ||   mr| golfpro|094-13140918|https://randomuse...|https://randomuse...|https://randomuse...|1393111806|Z2ypc9tB|dfd63fe3e35680257...|4fd7b8e859ab9db65...|
|beautifulduck|2202| 09311| 56.30.617.38|28/Jul/2012:08:39...|   08|   39|        04| -0500|   GET|               /demo|    0|            /demo|Jakarta Commons-H...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0935-906-2550|1244525876|.@exam...|  male||      |     2478  |31839|e37bf901f4f00091b...|||   mr|  686868|064-42326749|https://randomuse...|https://randomuse...|https://randomuse...|1123054755|GWD1kQsc|c830522c905c72181...|07383cb34c38b1780...|
|beautifulduck|2203| 99022|682.44.88.325|03/Feb/2012:03:06...|   03|   06|        04| -0500|   GET|   /product/product6|    0|/product/product6|Mozilla/4.0 (comp...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0907-592-2776| 735636504|.@examp...|  male| | |          5638 |50181|0a99d29a90d2b80e9...|| |   mr|  cutter|059-79002407|https://randomuse...|https://randomuse...|https://randomuse...|1301995909|8FFVKJA4|d8e5c2aa50628ede9...|9f9ae3cbd44513e70...|
|beautifulduck|2203| 99022|682.44.88.325|03/Feb/2012:03:06...|   03|   06|        04| -0500|   GET|   /product/product6|    0|/product/product6|Mozilla/4.0 (comp...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0974-297-2862| 255223873|.@examp...|  male| |       |8586   |64240|28fb2112dafd8dcf2...| ||   mr| golfpro|094-13140918|https://randomuse...|https://randomuse...|https://randomuse...|1393111806|Z2ypc9tB|dfd63fe3e35680257...|4fd7b8e859ab9db65...|
|beautifulduck|2203| 99022|682.44.88.325|03/Feb/2012:03:06...|   03|   06|        04| -0500|   GET|   /product/product6|    0|/product/product6|Mozilla/4.0 (comp...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0935-906-2550|1244525876|.@exam...|  male||      |     2478  |31839|e37bf901f4f00091b...|||   mr|  686868|064-42326749|https://randomuse...|https://randomuse...|https://randomuse...|1123054755|GWD1kQsc|c830522c905c72181...|07383cb34c38b1780...|
|beautifulduck|2204| 15 13| 58.633.38.78|31/Jul/2012:03:48...|   03|   48|        09| -0500|   GET|/analyst_perspective|    0|         /ad/save|       Java/1.6.0_04|       100|         IR|c28a5188e87e1ccf0a|    0.8|0907-592-2776| 735636504|.@examp...|  male| | |          5638 |50181|0a99d29a90d2b80e9...|| |   mr|  cutter|059-79002407|https://randomuse...|https://randomuse...|https://randomuse...|1301995909|8FFVKJA4|d8e5c2aa50628ede9...|9f9ae3cbd44513e70...|
|beautifulduck|2204| 15 13| 58.633.38.78|31/Jul/2012:03:48...|   03|   48|        09| -0500|   GET|/analyst_perspective|    0|         /ad/save|       Java/1.6.0_04|       100|         IR|c28a5188e87e1ccf0a|    0.8|0974-297-2862| 255223873|.@examp...|  male| |       |8586   |64240|28fb2112dafd8dcf2...| ||   mr| golfpro|094-13140918|https://randomuse...|https://randomuse...|https://randomuse...|1393111806|Z2ypc9tB|dfd63fe3e35680257...|4fd7b8e859ab9db65...|
|beautifulduck|2204| 15 13| 58.633.38.78|31/Jul/2012:03:48...|   03|   48|        09| -0500|   GET|/analyst_perspective|    0|         /ad/save|       Java/1.6.0_04|       100|         IR|c28a5188e87e1ccf0a|    0.8|0935-906-2550|1244525876|.@exam...|  male||      |     2478  |31839|e37bf901f4f00091b...|||   mr|  686868|064-42326749|https://randomuse...|https://randomuse...|https://randomuse...|1123054755|GWD1kQsc|c830522c905c72181...|07383cb34c38b1780...|
|beautifulduck|2205| 09092|361.631.17.30|05/Feb/2012:21:00...|   21|   00|        00| -0500|   GET|               /demo|    0|            /demo|Jakarta Commons-H...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0907-592-2776| 735636504|.@examp...|  male| | |          5638 |50181|0a99d29a90d2b80e9...|| |   mr|  cutter|059-79002407|https://randomuse...|https://randomuse...|https://randomuse...|1301995909|8FFVKJA4|d8e5c2aa50628ede9...|9f9ae3cbd44513e70...|
|beautifulduck|2205| 09092|361.631.17.30|05/Feb/2012:21:00...|   21|   00|        00| -0500|   GET|               /demo|    0|            /demo|Jakarta Commons-H...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0974-297-2862| 255223873|.@examp...|  male| |       |8586   |64240|28fb2112dafd8dcf2...| ||   mr| golfpro|094-13140918|https://randomuse...|https://randomuse...|https://randomuse...|1393111806|Z2ypc9tB|dfd63fe3e35680257...|4fd7b8e859ab9db65...|
|beautifulduck|2205| 09092|361.631.17.30|05/Feb/2012:21:00...|   21|   00|        00| -0500|   GET|               /demo|    0|            /demo|Jakarta Commons-H...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0935-906-2550|1244525876|.@exam...|  male||      |     2478  |31839|e37bf901f4f00091b...|||   mr|  686868|064-42326749|https://randomuse...|https://randomuse...|https://randomuse...|1123054755|GWD1kQsc|c830522c905c72181...|07383cb34c38b1780...|
|beautifulduck|2206| 06855|14.323.74.653|02/Aug/2012:11:10...|   11|   10|        01| -0500|   GET|               /demo|    0|            /demo|Jakarta Commons-H...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0907-592-2776| 735636504|.@examp...|  male| | |          5638 |50181|0a99d29a90d2b80e9...|| |   mr|  cutter|059-79002407|https://randomuse...|https://randomuse...|https://randomuse...|1301995909|8FFVKJA4|d8e5c2aa50628ede9...|9f9ae3cbd44513e70...|
|beautifulduck|2206| 06855|14.323.74.653|02/Aug/2012:11:10...|   11|   10|        01| -0500|   GET|               /demo|    0|            /demo|Jakarta Commons-H...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0974-297-2862| 255223873|.@examp...|  male| |       |8586   |64240|28fb2112dafd8dcf2...| ||   mr| golfpro|094-13140918|https://randomuse...|https://randomuse...|https://randomuse...|1393111806|Z2ypc9tB|dfd63fe3e35680257...|4fd7b8e859ab9db65...|
|beautifulduck|2206| 06855|14.323.74.653|02/Aug/2012:11:10...|   11|   10|        01| -0500|   GET|               /demo|    0|            /demo|Jakarta Commons-H...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0935-906-2550|1244525876|.@exam...|  male||      |     2478  |31839|e37bf901f4f00091b...|||   mr|  686868|064-42326749|https://randomuse...|https://randomuse...|https://randomuse...|1123054755|GWD1kQsc|c830522c905c72181...|07383cb34c38b1780...|
|beautifulduck|2207| 18056| 10.307.34.33|05/Feb/2012:23:04...|   23|   04|        30| -0500|   GET|           /download|    0|                -|Mozilla/5.0 (X11;...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0907-592-2776| 735636504|.@examp...|  male| | |          5638 |50181|0a99d29a90d2b80e9...|| |   mr|  cutter|059-79002407|https://randomuse...|https://randomuse...|https://randomuse...|1301995909|8FFVKJA4|d8e5c2aa50628ede9...|9f9ae3cbd44513e70...|
|beautifulduck|2207| 18056| 10.307.34.33|05/Feb/2012:23:04...|   23|   04|        30| -0500|   GET|           /download|    0|                -|Mozilla/5.0 (X11;...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0974-297-2862| 255223873|.@examp...|  male| |       |8586   |64240|28fb2112dafd8dcf2...| ||   mr| golfpro|094-13140918|https://randomuse...|https://randomuse...|https://randomuse...|1393111806|Z2ypc9tB|dfd63fe3e35680257...|4fd7b8e859ab9db65...|
+-------------+----+------+-------------+--------------------+-----+-----+----------+------+------+--------------------+-----+-----------------+--------------------+----------+-----------+------------------+-------+-------------+----------+--------------------+------+------+------------+--------------------+-----+--------------------+-----+------+-----+--------+------------+--------------------+--------------------+--------------------+----------+--------+--------------------+--------------------+
only showing top 20 rows





=============== Step 8 ================Null handled dataframe=============================================






+-------------+---+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+-------------+-------------+-------------+-------------+---+-------------+-------------+-------------+-------------+-------------+---+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+----------+-------------+-------------+-------------+
|     username| id|amount|             ip|            createdt|value|score|regioncode|status|method|                 key|count|                type|                site|statuscode|  nationality|         seed|      version|         cell|dob|        email|       gender|         city|        state|       street|zip|          md5|        first|         last|        title|     password|        phone|        large|       medium|    thumbnail|registered|         salt|         sha1|       sha256|
+-------------+---+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+-------------+-------------+-------------+-------------+---+-------------+-------------+-------------+-------------+-------------+---+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+----------+-------------+-------------+-------------+
|beautifulbear|  1| 01743|  68.688.326.58|13/Jun/2012:10:55...|   10|   55|        45| -0500|   GET|           /products|    0|   /product/product4|Mozilla/4.0 (comp...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear|  2| 75756|  361.631.17.30|01/Nov/2011:10:53...|   10|   53|        01| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear|  3| 05901|   11.308.46.48|21/Jun/2012:01:12...|   01|   12|        33| -0500|   GET|               /news|    0|    /partners/resell|Mozilla/5.0 (comp...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear|  4| 24538|  361.631.17.30|01/Nov/2011:13:48...|   13|   48|        30| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear|  5| 08116|  322.76.611.36|30/Jun/2012:07:19...|   07|   19|        30| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear|  6| 04675|  361.631.17.30|01/Nov/2011:16:19...|   16|   19|        00| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear|  7| 06575| 367.34.686.631|01/Jul/2012:00:02...|   00|   02|        00| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear|  8| 27382|   88.633.11.47|04/Nov/2011:13:00...|   13|   00|        30| -0500|   GET|        /feeds/press|    0|                   -|Apple-PubSub/65.12.1|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear|  9| 09934|   11.308.46.48|01/Jul/2012:23:13...|   23|   13|        53| -0500|   GET|           /partners|    0|   /product/product2|Mozilla/5.0 (comp...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear| 10| 01716|683.615.622.618|05/Nov/2011:22:03...|   22|   03|        01| -0500|   GET|    /partners/resell|    0|    /partners/resell|                null|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear| 11| 55195| 682.62.387.672|02/Jul/2012:10:17...|   10|   17|        48| -0500|   GET|/download/downloa...|    0|/download/downloa...|Mozilla/5.0 (Wind...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear| 12| 69809|  43.68.686.668|09/Nov/2011:05:25...|   05|   25|        00| -0500|   GET|        /feeds/press|    0|                   -|  Apple-PubSub/65.11|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear| 13| 78800|321.336.372.320|06/Jul/2012:01:41...|   01|   41|        00| -0500|   GET|        /feeds/press|    0|                   -|Mozilla/5.0 (Wind...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear| 14| 58994|  361.631.17.30|16/Nov/2011:00:50...|   00|   50|        00| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear| 15| 94543| 361.638.668.62|13/Jul/2012:23:44...|   23|   44|        30| -0500|   GET|            /ad/save|    0|                   -|Mozilla/5.0 (Twic...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear| 16| 33695|   325.87.75.36|16/Nov/2011:04:58...|   04|   58|        30| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear| 17| 01755|  18.332.18.672|16/Jul/2012:03:20...|   03|   20|        30| -0500|   GET|        /feeds/press|    0|                   -|Apple-PubSub/65.12.1|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear| 18| 01784|   361.321.73.6|17/Nov/2011:13:08...|   13|   08|        30| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear| 19| 09671|   364.63.61.83|23/Jul/2012:17:00...|   17|   00|        37| -0500|   GET|   /product/product3|    0|      /docs/doc5.pdf|Mozilla/5.0 (X11;...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear| 20| 04288|    13.53.52.13|26/Nov/2011:19:16...|   19|   16|        00| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
+-------------+---+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+-------------+-------------+-------------+-------------+---+-------------+-------------+-------------+-------------+-------------+---+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+----------+-------------+-------------+-------------+
only showing top 20 rows





=============== Step 9 a ================not available customers with current date dataframe=============================================






+-------------+---+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+-------------+-------------+-------------+-------------+---+-------------+-------------+-------------+-------------+-------------+---+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+----------+-------------+-------------+-------------+------------+
|     username| id|amount|             ip|            createdt|value|score|regioncode|status|method|                 key|count|                type|                site|statuscode|  nationality|         seed|      version|         cell|dob|        email|       gender|         city|        state|       street|zip|          md5|        first|         last|        title|     password|        phone|        large|       medium|    thumbnail|registered|         salt|         sha1|       sha256|current_date|
+-------------+---+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+-------------+-------------+-------------+-------------+---+-------------+-------------+-------------+-------------+-------------+---+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+----------+-------------+-------------+-------------+------------+
|beautifulbear|  1| 01743|  68.688.326.58|13/Jun/2012:10:55...|   10|   55|        45| -0500|   GET|           /products|    0|   /product/product4|Mozilla/4.0 (comp...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear|  2| 75756|  361.631.17.30|01/Nov/2011:10:53...|   10|   53|        01| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear|  3| 05901|   11.308.46.48|21/Jun/2012:01:12...|   01|   12|        33| -0500|   GET|               /news|    0|    /partners/resell|Mozilla/5.0 (comp...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear|  4| 24538|  361.631.17.30|01/Nov/2011:13:48...|   13|   48|        30| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear|  5| 08116|  322.76.611.36|30/Jun/2012:07:19...|   07|   19|        30| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear|  6| 04675|  361.631.17.30|01/Nov/2011:16:19...|   16|   19|        00| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear|  7| 06575| 367.34.686.631|01/Jul/2012:00:02...|   00|   02|        00| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear|  8| 27382|   88.633.11.47|04/Nov/2011:13:00...|   13|   00|        30| -0500|   GET|        /feeds/press|    0|                   -|Apple-PubSub/65.12.1|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear|  9| 09934|   11.308.46.48|01/Jul/2012:23:13...|   23|   13|        53| -0500|   GET|           /partners|    0|   /product/product2|Mozilla/5.0 (comp...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear| 10| 01716|683.615.622.618|05/Nov/2011:22:03...|   22|   03|        01| -0500|   GET|    /partners/resell|    0|    /partners/resell|                null|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear| 11| 55195| 682.62.387.672|02/Jul/2012:10:17...|   10|   17|        48| -0500|   GET|/download/downloa...|    0|/download/downloa...|Mozilla/5.0 (Wind...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear| 12| 69809|  43.68.686.668|09/Nov/2011:05:25...|   05|   25|        00| -0500|   GET|        /feeds/press|    0|                   -|  Apple-PubSub/65.11|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear| 13| 78800|321.336.372.320|06/Jul/2012:01:41...|   01|   41|        00| -0500|   GET|        /feeds/press|    0|                   -|Mozilla/5.0 (Wind...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear| 14| 58994|  361.631.17.30|16/Nov/2011:00:50...|   00|   50|        00| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear| 15| 94543| 361.638.668.62|13/Jul/2012:23:44...|   23|   44|        30| -0500|   GET|            /ad/save|    0|                   -|Mozilla/5.0 (Twic...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear| 16| 33695|   325.87.75.36|16/Nov/2011:04:58...|   04|   58|        30| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear| 17| 01755|  18.332.18.672|16/Jul/2012:03:20...|   03|   20|        30| -0500|   GET|        /feeds/press|    0|                   -|Apple-PubSub/65.12.1|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear| 18| 01784|   361.321.73.6|17/Nov/2011:13:08...|   13|   08|        30| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear| 19| 09671|   364.63.61.83|23/Jul/2012:17:00...|   17|   00|        37| -0500|   GET|   /product/product3|    0|      /docs/doc5.pdf|Mozilla/5.0 (X11;...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear| 20| 04288|    13.53.52.13|26/Nov/2011:19:16...|   19|   16|        00| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
+-------------+---+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+-------------+-------------+-------------+-------------+---+-------------+-------------+-------------+-------------+-------------+---+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+----------+-------------+-------------+-------------+------------+
only showing top 20 rows








=============== Step 9 b ================available customers with current date dataframe=============================================






+-------------+----+------+-------------+--------------------+-----+-----+----------+------+------+--------------------+-----+-----------------+--------------------+----------+-----------+------------------+-------+-------------+----------+--------------------+------+------+------------+--------------------+-----+--------------------+-----+------+-----+--------+------------+--------------------+--------------------+--------------------+----------+--------+--------------------+--------------------+------------+
|     username|  id|amount|           ip|            createdt|value|score|regioncode|status|method|                 key|count|             type|                site|statuscode|nationality|              seed|version|         cell|       dob|               email|gender|  city|       state|              street|  zip|                 md5|first|  last|title|password|       phone|               large|              medium|           thumbnail|registered|    salt|                sha1|              sha256|current_date|
+-------------+----+------+-------------+--------------------+-----+-----+----------+------+------+--------------------+-----+-----------------+--------------------+----------+-----------+------------------+-------+-------------+----------+--------------------+------+------+------------+--------------------+-----+--------------------+-----+------+-----+--------+------------+--------------------+--------------------+--------------------+----------+--------+--------------------+--------------------+------------+
|beautifulduck|2201| 66419|361.631.17.30|01/Feb/2012:13:27...|   13|   27|        08| -0500|   GET|               /demo|    0|            /demo|Jakarta Commons-H...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0907-592-2776| 735636504|.@examp...|  male| | |          5638 |50181|0a99d29a90d2b80e9...|| |   mr|  cutter|059-79002407|https://randomuse...|https://randomuse...|https://randomuse...|1301995909|8FFVKJA4|d8e5c2aa50628ede9...|9f9ae3cbd44513e70...|  2023-01-01|
|beautifulduck|2201| 66419|361.631.17.30|01/Feb/2012:13:27...|   13|   27|        08| -0500|   GET|               /demo|    0|            /demo|Jakarta Commons-H...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0974-297-2862| 255223873|.@examp...|  male| |       |8586   |64240|28fb2112dafd8dcf2...| ||   mr| golfpro|094-13140918|https://randomuse...|https://randomuse...|https://randomuse...|1393111806|Z2ypc9tB|dfd63fe3e35680257...|4fd7b8e859ab9db65...|  2023-01-01|
|beautifulduck|2201| 66419|361.631.17.30|01/Feb/2012:13:27...|   13|   27|        08| -0500|   GET|               /demo|    0|            /demo|Jakarta Commons-H...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0935-906-2550|1244525876|.@exam...|  male||      |     2478  |31839|e37bf901f4f00091b...|||   mr|  686868|064-42326749|https://randomuse...|https://randomuse...|https://randomuse...|1123054755|GWD1kQsc|c830522c905c72181...|07383cb34c38b1780...|  2023-01-01|
|beautifulduck|2202| 09311| 56.30.617.38|28/Jul/2012:08:39...|   08|   39|        04| -0500|   GET|               /demo|    0|            /demo|Jakarta Commons-H...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0907-592-2776| 735636504|.@examp...|  male| | |          5638 |50181|0a99d29a90d2b80e9...|| |   mr|  cutter|059-79002407|https://randomuse...|https://randomuse...|https://randomuse...|1301995909|8FFVKJA4|d8e5c2aa50628ede9...|9f9ae3cbd44513e70...|  2023-01-01|
|beautifulduck|2202| 09311| 56.30.617.38|28/Jul/2012:08:39...|   08|   39|        04| -0500|   GET|               /demo|    0|            /demo|Jakarta Commons-H...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0974-297-2862| 255223873|.@examp...|  male| |       |8586   |64240|28fb2112dafd8dcf2...| ||   mr| golfpro|094-13140918|https://randomuse...|https://randomuse...|https://randomuse...|1393111806|Z2ypc9tB|dfd63fe3e35680257...|4fd7b8e859ab9db65...|  2023-01-01|
|beautifulduck|2202| 09311| 56.30.617.38|28/Jul/2012:08:39...|   08|   39|        04| -0500|   GET|               /demo|    0|            /demo|Jakarta Commons-H...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0935-906-2550|1244525876|.@exam...|  male||      |     2478  |31839|e37bf901f4f00091b...|||   mr|  686868|064-42326749|https://randomuse...|https://randomuse...|https://randomuse...|1123054755|GWD1kQsc|c830522c905c72181...|07383cb34c38b1780...|  2023-01-01|
|beautifulduck|2203| 99022|682.44.88.325|03/Feb/2012:03:06...|   03|   06|        04| -0500|   GET|   /product/product6|    0|/product/product6|Mozilla/4.0 (comp...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0907-592-2776| 735636504|.@examp...|  male| | |          5638 |50181|0a99d29a90d2b80e9...|| |   mr|  cutter|059-79002407|https://randomuse...|https://randomuse...|https://randomuse...|1301995909|8FFVKJA4|d8e5c2aa50628ede9...|9f9ae3cbd44513e70...|  2023-01-01|
|beautifulduck|2203| 99022|682.44.88.325|03/Feb/2012:03:06...|   03|   06|        04| -0500|   GET|   /product/product6|    0|/product/product6|Mozilla/4.0 (comp...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0974-297-2862| 255223873|.@examp...|  male| |       |8586   |64240|28fb2112dafd8dcf2...| ||   mr| golfpro|094-13140918|https://randomuse...|https://randomuse...|https://randomuse...|1393111806|Z2ypc9tB|dfd63fe3e35680257...|4fd7b8e859ab9db65...|  2023-01-01|
|beautifulduck|2203| 99022|682.44.88.325|03/Feb/2012:03:06...|   03|   06|        04| -0500|   GET|   /product/product6|    0|/product/product6|Mozilla/4.0 (comp...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0935-906-2550|1244525876|.@exam...|  male||      |     2478  |31839|e37bf901f4f00091b...|||   mr|  686868|064-42326749|https://randomuse...|https://randomuse...|https://randomuse...|1123054755|GWD1kQsc|c830522c905c72181...|07383cb34c38b1780...|  2023-01-01|
|beautifulduck|2204| 15 13| 58.633.38.78|31/Jul/2012:03:48...|   03|   48|        09| -0500|   GET|/analyst_perspective|    0|         /ad/save|       Java/1.6.0_04|       100|         IR|c28a5188e87e1ccf0a|    0.8|0907-592-2776| 735636504|.@examp...|  male| | |          5638 |50181|0a99d29a90d2b80e9...|| |   mr|  cutter|059-79002407|https://randomuse...|https://randomuse...|https://randomuse...|1301995909|8FFVKJA4|d8e5c2aa50628ede9...|9f9ae3cbd44513e70...|  2023-01-01|
|beautifulduck|2204| 15 13| 58.633.38.78|31/Jul/2012:03:48...|   03|   48|        09| -0500|   GET|/analyst_perspective|    0|         /ad/save|       Java/1.6.0_04|       100|         IR|c28a5188e87e1ccf0a|    0.8|0974-297-2862| 255223873|.@examp...|  male| |       |8586   |64240|28fb2112dafd8dcf2...| ||   mr| golfpro|094-13140918|https://randomuse...|https://randomuse...|https://randomuse...|1393111806|Z2ypc9tB|dfd63fe3e35680257...|4fd7b8e859ab9db65...|  2023-01-01|
|beautifulduck|2204| 15 13| 58.633.38.78|31/Jul/2012:03:48...|   03|   48|        09| -0500|   GET|/analyst_perspective|    0|         /ad/save|       Java/1.6.0_04|       100|         IR|c28a5188e87e1ccf0a|    0.8|0935-906-2550|1244525876|.@exam...|  male||      |     2478  |31839|e37bf901f4f00091b...|||   mr|  686868|064-42326749|https://randomuse...|https://randomuse...|https://randomuse...|1123054755|GWD1kQsc|c830522c905c72181...|07383cb34c38b1780...|  2023-01-01|
|beautifulduck|2205| 09092|361.631.17.30|05/Feb/2012:21:00...|   21|   00|        00| -0500|   GET|               /demo|    0|            /demo|Jakarta Commons-H...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0907-592-2776| 735636504|.@examp...|  male| | |          5638 |50181|0a99d29a90d2b80e9...|| |   mr|  cutter|059-79002407|https://randomuse...|https://randomuse...|https://randomuse...|1301995909|8FFVKJA4|d8e5c2aa50628ede9...|9f9ae3cbd44513e70...|  2023-01-01|
|beautifulduck|2205| 09092|361.631.17.30|05/Feb/2012:21:00...|   21|   00|        00| -0500|   GET|               /demo|    0|            /demo|Jakarta Commons-H...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0974-297-2862| 255223873|.@examp...|  male| |       |8586   |64240|28fb2112dafd8dcf2...| ||   mr| golfpro|094-13140918|https://randomuse...|https://randomuse...|https://randomuse...|1393111806|Z2ypc9tB|dfd63fe3e35680257...|4fd7b8e859ab9db65...|  2023-01-01|
|beautifulduck|2205| 09092|361.631.17.30|05/Feb/2012:21:00...|   21|   00|        00| -0500|   GET|               /demo|    0|            /demo|Jakarta Commons-H...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0935-906-2550|1244525876|.@exam...|  male||      |     2478  |31839|e37bf901f4f00091b...|||   mr|  686868|064-42326749|https://randomuse...|https://randomuse...|https://randomuse...|1123054755|GWD1kQsc|c830522c905c72181...|07383cb34c38b1780...|  2023-01-01|
|beautifulduck|2206| 06855|14.323.74.653|02/Aug/2012:11:10...|   11|   10|        01| -0500|   GET|               /demo|    0|            /demo|Jakarta Commons-H...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0907-592-2776| 735636504|.@examp...|  male| | |          5638 |50181|0a99d29a90d2b80e9...|| |   mr|  cutter|059-79002407|https://randomuse...|https://randomuse...|https://randomuse...|1301995909|8FFVKJA4|d8e5c2aa50628ede9...|9f9ae3cbd44513e70...|  2023-01-01|
|beautifulduck|2206| 06855|14.323.74.653|02/Aug/2012:11:10...|   11|   10|        01| -0500|   GET|               /demo|    0|            /demo|Jakarta Commons-H...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0974-297-2862| 255223873|.@examp...|  male| |       |8586   |64240|28fb2112dafd8dcf2...| ||   mr| golfpro|094-13140918|https://randomuse...|https://randomuse...|https://randomuse...|1393111806|Z2ypc9tB|dfd63fe3e35680257...|4fd7b8e859ab9db65...|  2023-01-01|
|beautifulduck|2206| 06855|14.323.74.653|02/Aug/2012:11:10...|   11|   10|        01| -0500|   GET|               /demo|    0|            /demo|Jakarta Commons-H...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0935-906-2550|1244525876|.@exam...|  male||      |     2478  |31839|e37bf901f4f00091b...|||   mr|  686868|064-42326749|https://randomuse...|https://randomuse...|https://randomuse...|1123054755|GWD1kQsc|c830522c905c72181...|07383cb34c38b1780...|  2023-01-01|
|beautifulduck|2207| 18056| 10.307.34.33|05/Feb/2012:23:04...|   23|   04|        30| -0500|   GET|           /download|    0|                -|Mozilla/5.0 (X11;...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0907-592-2776| 735636504|.@examp...|  male| | |          5638 |50181|0a99d29a90d2b80e9...|| |   mr|  cutter|059-79002407|https://randomuse...|https://randomuse...|https://randomuse...|1301995909|8FFVKJA4|d8e5c2aa50628ede9...|9f9ae3cbd44513e70...|  2023-01-01|
|beautifulduck|2207| 18056| 10.307.34.33|05/Feb/2012:23:04...|   23|   04|        30| -0500|   GET|           /download|    0|                -|Mozilla/5.0 (X11;...|       100|         IR|c28a5188e87e1ccf0a|    0.8|0974-297-2862| 255223873|.@examp...|  male| |       |8586   |64240|28fb2112dafd8dcf2...| ||   mr| golfpro|094-13140918|https://randomuse...|https://randomuse...|https://randomuse...|1393111806|Z2ypc9tB|dfd63fe3e35680257...|4fd7b8e859ab9db65...|  2023-01-01|
+-------------+----+------+-------------+--------------------+-----+-----+----------+------+------+--------------------+-----+-----------------+--------------------+----------+-----------+------------------+-------+-------------+----------+--------------------+------+------+------------+--------------------+-----+--------------------+-----+------+-----+--------+------------+--------------------+--------------------+--------------------+----------+--------+--------------------+--------------------+------------+
only showing top 20 rows



=========================================================================
Project Code:
-------------

package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.functions.upper
import org.apache.spark.sql.catalyst.expressions.Upper
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window
import scala.io.Source
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions._

object obj {


	def main(args:Array[String]):Unit={


			val conf = new SparkConf().setAppName("ES").setMaster("local[*]")

					val sc = new SparkContext(conf)
					sc.setLogLevel("Error")


					val spark = SparkSession.builder()



					.getOrCreate()

					import spark.implicits._


					println
					println
					println
					println
					println
					println("======================= Step 2 ========Raw data=============================================")
					println
					println
					println
					println
					println




					val data = spark.read.format("com.databricks.spark.avro")
					.load("file:///D://data//projectsample.avro")

					data.show()




					println
					println
					println
					println
					println
					println("======================== Step 3 ========Url data=============================================")
					println
					println
					println
					println
					println


					val html = Source.fromURL("https://randomuser.me/api/0.8/?results=100")
					val s = html.mkString
					//println(s)


					val urldf = spark.read.json(sc.parallelize(List(s)))
					urldf.show()




					println
					println
					println
					println
					println
					println("========================step 4 flatten dataframe=============================================")
					println
					println
					println
					println
					println

					val flatdf = urldf.withColumn("results",explode(col("results"))).select("nationality","seed","version",
							"results.user.username","results.user.cell","results.user.dob","results.user.email",
							"results.user.gender","results.user.location.city","results.user.location.state",
							"results.user.location.street","results.user.location.zip","results.user.md5",
							"results.user.name.first","results.user.name.last","results.user.name.title",
							"results.user.password","results.user.phone","results.user.picture.large","results.user.picture.medium"
							,"results.user.picture.thumbnail","results.user.registered","results.user.salt","results.user.sha1"
							,"results.user.sha256")
					flatdf.show()






					println
					println
					println
					println
					println
					println("========================step 5 removed numericals Dataframe=============================================")
					println
					println
					println
					println
					println

					val rm=flatdf.withColumn("username",regexp_replace(col("username"),  "([0-9])", ""))
					rm.show()







					println
					println
					println
					println
					println
					println("====================== Step 6 =========Joined Dataframe=============================================")
					println
					println
					println
					println
					println
					val joindf = data.join(broadcast(rm),Seq("username"),"left")


					joindf.show()


					println
					println
					println
					println
					println("=================== Step 7 a ============Not available customers=============================================")
					println
					println
					println
					println
					println
					println


					val dfnull = joindf.filter(col("nationality").isNull)


					val dfnotnull=joindf.filter(col("nationality").isNotNull)






					println
					println
					println
					println
					println("==================  Step 7 b =============available customers=============================================")
					println
					println
					println
					println
					println
					println






	}


}

============
Out put:
--------
======================= Step 2 ========Raw data=============================================





+---+-------------+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+
| id|     username|amount|             ip|            createdt|value|score|regioncode|status|method|                 key|count|                type|                site|statuscode|
+---+-------------+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+
|  1|beautifulbear| 01743|  68.688.326.58|13/Jun/2012:10:55...|   10|   55|        45| -0500|   GET|           /products|    0|   /product/product4|Mozilla/4.0 (comp...|       100|
|  2|beautifulbear| 75756|  361.631.17.30|01/Nov/2011:10:53...|   10|   53|        01| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|
|  3|beautifulbear| 05901|   11.308.46.48|21/Jun/2012:01:12...|   01|   12|        33| -0500|   GET|               /news|    0|    /partners/resell|Mozilla/5.0 (comp...|       100|
|  4|beautifulbear| 24538|  361.631.17.30|01/Nov/2011:13:48...|   13|   48|        30| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|
|  5|beautifulbear| 08116|  322.76.611.36|30/Jun/2012:07:19...|   07|   19|        30| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|
|  6|beautifulbear| 04675|  361.631.17.30|01/Nov/2011:16:19...|   16|   19|        00| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|
|  7|beautifulbear| 06575| 367.34.686.631|01/Jul/2012:00:02...|   00|   02|        00| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|
|  8|beautifulbear| 27382|   88.633.11.47|04/Nov/2011:13:00...|   13|   00|        30| -0500|   GET|        /feeds/press|    0|                   -|Apple-PubSub/65.12.1|       100|
|  9|beautifulbear| 09934|   11.308.46.48|01/Jul/2012:23:13...|   23|   13|        53| -0500|   GET|           /partners|    0|   /product/product2|Mozilla/5.0 (comp...|       100|
| 10|beautifulbear| 01716|683.615.622.618|05/Nov/2011:22:03...|   22|   03|        01| -0500|   GET|    /partners/resell|    0|    /partners/resell|                null|       100|
| 11|beautifulbear| 55195| 682.62.387.672|02/Jul/2012:10:17...|   10|   17|        48| -0500|   GET|/download/downloa...|    0|/download/downloa...|Mozilla/5.0 (Wind...|       100|
| 12|beautifulbear| 69809|  43.68.686.668|09/Nov/2011:05:25...|   05|   25|        00| -0500|   GET|        /feeds/press|    0|                   -|  Apple-PubSub/65.11|       100|
| 13|beautifulbear| 78800|321.336.372.320|06/Jul/2012:01:41...|   01|   41|        00| -0500|   GET|        /feeds/press|    0|                   -|Mozilla/5.0 (Wind...|       100|
| 14|beautifulbear| 58994|  361.631.17.30|16/Nov/2011:00:50...|   00|   50|        00| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|
| 15|beautifulbear| 94543| 361.638.668.62|13/Jul/2012:23:44...|   23|   44|        30| -0500|   GET|            /ad/save|    0|                   -|Mozilla/5.0 (Twic...|       100|
| 16|beautifulbear| 33695|   325.87.75.36|16/Nov/2011:04:58...|   04|   58|        30| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|
| 17|beautifulbear| 01755|  18.332.18.672|16/Jul/2012:03:20...|   03|   20|        30| -0500|   GET|        /feeds/press|    0|                   -|Apple-PubSub/65.12.1|       100|
| 18|beautifulbear| 01784|   361.321.73.6|17/Nov/2011:13:08...|   13|   08|        30| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|
| 19|beautifulbear| 09671|   364.63.61.83|23/Jul/2012:17:00...|   17|   00|        37| -0500|   GET|   /product/product3|    0|      /docs/doc5.pdf|Mozilla/5.0 (X11;...|       100|
| 20|beautifulbear| 04288|    13.53.52.13|26/Nov/2011:19:16...|   19|   16|        00| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|
+---+-------------+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+
only showing top 20 rows






======================== Step 3 ========Url data=============================================





+-----------+--------------------+------------------+-------+
|nationality|             results|              seed|version|
+-----------+--------------------+------------------+-------+
|         NZ|[[[(575)-734-4069...|67a0d5b2cce1b2630c|    0.8|
+-----------+--------------------+------------------+-------+






========================step 4 flatten dataframe=============================================





+-----------+------------------+-------+------------------+--------------+----------+--------------------+------+----------------+-----------------+--------------------+-----+--------------------+---------+-------+-----+---------+--------------+--------------------+--------------------+--------------------+----------+--------+--------------------+--------------------+
|nationality|              seed|version|          username|          cell|       dob|               email|gender|            city|            state|              street|  zip|                 md5|    first|   last|title| password|         phone|               large|              medium|           thumbnail|registered|    salt|                sha1|              sha256|
+-----------+------------------+-------+------------------+--------------+----------+--------------------+------+----------------+-----------------+--------------------+-----+--------------------+---------+-------+-----+---------+--------------+--------------------+--------------------+--------------------+----------+--------+--------------------+--------------------+
|         NZ|67a0d5b2cce1b2630c|    0.8|   blackgorilla404|(575)-734-4069| 140750583|ellie.clarke@exam...|female|       masterton|      marlborough| 5574 fendalton road|85939|0fc9d2c30b1e71b0d...|    ellie| clarke|   ms|    allan|(491)-202-9225|https://randomuse...|https://randomuse...|https://randomuse...|1143161312|eudKEy4T|771d774ca8330aae3...|b0929fa66cf8501a2...|
|         NZ|67a0d5b2cce1b2630c|    0.8|      blackduck861|(141)-751-6826| 434546394|tyler.turner@exam...|  male|      lower hutt|         taranaki|4344 sherborne st...|38693|00578de8ab88f2b9f...|    tyler| turner|   mr|  roberta|(601)-140-2603|https://randomuse...|https://randomuse...|https://randomuse...|1250337824|d7kJ0OoZ|f1c8aaf0c2cc414e1...|fa98940eaa4830437...|
|         NZ|67a0d5b2cce1b2630c|    0.8|  goldenostrich556|(943)-616-6037|1244410079|mackenzie.harris@...|female|        tauranga|            otago|7437 maraekakaho ...|39480|de35ea363274ec217...|mackenzie| harris|  mrs| pathfind|(841)-484-3483|https://randomuse...|https://randomuse...|https://randomuse...|1175358622|sEu4N95d|10bf2298358986b05...|46a5d45bd1a7ec1f0...|
|         NZ|67a0d5b2cce1b2630c|    0.8|  silverpeacock622|(307)-089-3146| 432968310|ivy.smith@example...|female|      lower hutt|        northland|5665 princess street|58060|109fdca1fdb9f89ee...|      ivy|  smith| miss|    titan|(987)-147-4845|https://randomuse...|https://randomuse...|https://randomuse...|1014225485|P4vPdsAk|be055502f1ccf22eb...|c7200e712298f7cec...|
|         NZ|67a0d5b2cce1b2630c|    0.8|       redtiger440|(639)-652-8519| 905238245|jackson.walker@ex...|  male|        gisborne|       wellington|  6422 edmonton road|32978|a39e19f5b3e046d12...|  jackson| walker|   mr|   greens|(610)-263-1027|https://randomuse...|https://randomuse...|https://randomuse...|1423786250|NmE8DjSm|5822556f37b5380e7...|6e9e0c697c1e46da8...|
|         NZ|67a0d5b2cce1b2630c|    0.8|   beautifuldog585|(809)-384-2657| 200217692|scarlett.jones@ex...|female|        hamilton|       canterbury|7833 south wester...|81149|38d1ea807a3728945...| scarlett|  jones|   ms|     1952|(128)-681-9528|https://randomuse...|https://randomuse...|https://randomuse...|1014987993|sOjxotld|1aa1162da5850fd16...|163d9fc884736aef4...|
|         NZ|67a0d5b2cce1b2630c|    0.8|     yellowduck495|(289)-836-6419|1064605949|dylan.white@examp...|  male|    christchurch|manawatu-wanganui|   4845 chatham road|79449|6793655f52306265e...|    dylan|  white|   mr| cygnusx1|(170)-488-2886|https://randomuse...|https://randomuse...|https://randomuse...|1175669524|bezAtsfi|64ed9cf0d9bfb7d1d...|32e10119bb427eb92...|
|         NZ|67a0d5b2cce1b2630c|    0.8|     goldenswan515|(479)-746-6637| 801729386|tyler.kumar@examp...|  male|          napier|manawatu-wanganui|    8341 taieri road|65062|59e6497f49c08d14f...|    tyler|  kumar|   mr|    fmale|(080)-535-1387|https://randomuse...|https://randomuse...|https://randomuse...|1299445629|cdWOCdzm|e7af222dfbafdf992...|ecf3c5949b43f3dfc...|
|         NZ|67a0d5b2cce1b2630c|    0.8|     bigostrich223|(260)-794-8920|1409374812|hunter.jones@exam...|  male|       whangarei|           tasman|1741 western hill...|98012|81c77ff66661e8cdc...|   hunter|  jones|   mr|    butch|(989)-661-2323|https://randomuse...|https://randomuse...|https://randomuse...|1361694923|p9NiRchp|307d8c2f41cb71a41...|1130031757e33ecd4...|
|         NZ|67a0d5b2cce1b2630c|    0.8|         redcat462|(537)-883-9797| 876506296|hugo.lewis@exampl...|  male|    invercargill|            otago|   5051 papanui road|89486|9ea9cb1051f21e840...|     hugo|  lewis|   mr|  spanker|(414)-077-6696|https://randomuse...|https://randomuse...|https://randomuse...|1302037561|67NqHPI3|24d8f1d2e73319002...|576fb0bbabcda4cff...|
|         NZ|67a0d5b2cce1b2630c|    0.8| organicpeacock232|(324)-275-2275| 244678702|chloe.roberts@exa...|female|       whangarei|         taranaki|  4363 fraser street|17397|97ff0f2be537b268b...|    chloe|roberts| miss|   kodiak|(700)-770-5125|https://randomuse...|https://randomuse...|https://randomuse...|1264331025|uq5H7kc5|13baf9af38a85d643...|52e5fd287509f742e...|
|         NZ|67a0d5b2cce1b2630c|    0.8|      heavybear340|(103)-435-9554| 651199027|joseph.kumar@exam...|  male|      upper hutt|           nelson|8972 montreal street|69034|6b4ee8afadad9ced2...|   joseph|  kumar|   mr|   gunner|(513)-237-2428|https://randomuse...|https://randomuse...|https://randomuse...|1260556789|LszA5gCa|6d3dfb11915861a60...|c9fec3a76ac28c5b9...|
|         NZ|67a0d5b2cce1b2630c|    0.8|    tinyladybug661|(623)-463-4940| 962020819|madison.wood@exam...|female|palmerston north|        southland|  1378 walton street|72660|1c12fc10f29ae6971...|  madison|   wood|   ms|    blink|(807)-816-9610|https://randomuse...|https://randomuse...|https://randomuse...|1307881381|7oo7WGCQ|7e816d8f480201585...|5ddd5605c6ba7cdbd...|
|         NZ|67a0d5b2cce1b2630c|    0.8|beautifulrabbit476|(300)-219-9878|1206746947|maia.wood@example...|female|       whangarei|         gisborne|  3176 marine parade|68608|1025e6ddee39de623...|     maia|   wood|   ms|  glasgow|(212)-156-8122|https://randomuse...|https://randomuse...|https://randomuse...|1143886534|kasVQOxj|bf96445dcba4effb7...|cb55db9a97019214a...|
|         NZ|67a0d5b2cce1b2630c|    0.8|  silverleopard506|(817)-488-2109| 881901818|edward.johnson@ex...|  male|        tauranga|          waikato|1184 main street ...|39162|3f6d5a8fe77e82815...|   edward|johnson|   mr| zzzzzzzz|(780)-840-7948|https://randomuse...|https://randomuse...|https://randomuse...|1016587804|BGJlhyEU|2aae3d484fdca6401...|54867e97102233bf3...|
|         NZ|67a0d5b2cce1b2630c|    0.8|    goldentiger522|(686)-142-2269| 677226781|dylan.edwards@exa...|  male|palmerston north|      marlborough|2100 andersons ba...|28810|4471e2d9618b7d737...|    dylan|edwards|   mr|   jerome|(235)-734-4962|https://randomuse...|https://randomuse...|https://randomuse...|1251756355|XiCltdzq|cb0d2e2caf1e64802...|baba487115b8ebc7d...|
|         NZ|67a0d5b2cce1b2630c|    0.8|       crazycat975|(088)-602-0681|1169108381|emma.walker@examp...|female|           taupo|    bay of plenty|     8925 opawa road|89117|fb6d7013a2ad80d46...|     emma| walker|  mrs|   rubble|(282)-495-4878|https://randomuse...|https://randomuse...|https://randomuse...|1076433972|iIRszvWN|fa220a80d95564b4f...|cc11a523bdcfc01ef...|
|         NZ|67a0d5b2cce1b2630c|    0.8|      greenswan143|(855)-092-6425| 259975499|oliver.taylor@exa...|  male|      upper hutt|       canterbury|    5576 wairau road|53933|72342c37001da20ff...|   oliver| taylor|   mr|   combat|(645)-744-8102|https://randomuse...|https://randomuse...|https://randomuse...|1429829246|uktFtg5Q|d5f43e08d09a29492...|bfdf49dcd571160ba...|
|         NZ|67a0d5b2cce1b2630c|    0.8|    silvermouse316|(916)-041-5973| 854550510|max.taylor@exampl...|  male|         dunedin|       west coast|7520 cavendish drive|42615|402b7cbaeffc09476...|      max| taylor|   mr|   eagle1|(143)-216-7525|https://randomuse...|https://randomuse...|https://randomuse...|1312280197|DWfIO2pA|a45d2c9a546567bab...|f73f240957934c0b5...|
|         NZ|67a0d5b2cce1b2630c|    0.8|    silvertiger233|(411)-777-1459| 224533700|james.davies@exam...|  male|        hastings|        southland|   5665 penrose road|30303|7d1fbeb156f753889...|    james| davies|   mr|wolverine|(428)-914-3505|https://randomuse...|https://randomuse...|https://randomuse...|1163581362|h1F7goSb|763e427ca59e11330...|7d59347b8ca4474ce...|
+-----------+------------------+-------+------------------+--------------+----------+--------------------+------+----------------+-----------------+--------------------+-----+--------------------+---------+-------+-----+---------+--------------+--------------------+--------------------+--------------------+----------+--------+--------------------+--------------------+
only showing top 20 rows






========================step 5 removed numericals Dataframe=============================================





+-----------+------------------+-------+---------------+--------------+----------+--------------------+------+----------------+-----------------+--------------------+-----+--------------------+---------+-------+-----+---------+--------------+--------------------+--------------------+--------------------+----------+--------+--------------------+--------------------+
|nationality|              seed|version|       username|          cell|       dob|               email|gender|            city|            state|              street|  zip|                 md5|    first|   last|title| password|         phone|               large|              medium|           thumbnail|registered|    salt|                sha1|              sha256|
+-----------+------------------+-------+---------------+--------------+----------+--------------------+------+----------------+-----------------+--------------------+-----+--------------------+---------+-------+-----+---------+--------------+--------------------+--------------------+--------------------+----------+--------+--------------------+--------------------+
|         NZ|67a0d5b2cce1b2630c|    0.8|   blackgorilla|(575)-734-4069| 140750583|ellie.clarke@exam...|female|       masterton|      marlborough| 5574 fendalton road|85939|0fc9d2c30b1e71b0d...|    ellie| clarke|   ms|    allan|(491)-202-9225|https://randomuse...|https://randomuse...|https://randomuse...|1143161312|eudKEy4T|771d774ca8330aae3...|b0929fa66cf8501a2...|
|         NZ|67a0d5b2cce1b2630c|    0.8|      blackduck|(141)-751-6826| 434546394|tyler.turner@exam...|  male|      lower hutt|         taranaki|4344 sherborne st...|38693|00578de8ab88f2b9f...|    tyler| turner|   mr|  roberta|(601)-140-2603|https://randomuse...|https://randomuse...|https://randomuse...|1250337824|d7kJ0OoZ|f1c8aaf0c2cc414e1...|fa98940eaa4830437...|
|         NZ|67a0d5b2cce1b2630c|    0.8|  goldenostrich|(943)-616-6037|1244410079|mackenzie.harris@...|female|        tauranga|            otago|7437 maraekakaho ...|39480|de35ea363274ec217...|mackenzie| harris|  mrs| pathfind|(841)-484-3483|https://randomuse...|https://randomuse...|https://randomuse...|1175358622|sEu4N95d|10bf2298358986b05...|46a5d45bd1a7ec1f0...|
|         NZ|67a0d5b2cce1b2630c|    0.8|  silverpeacock|(307)-089-3146| 432968310|ivy.smith@example...|female|      lower hutt|        northland|5665 princess street|58060|109fdca1fdb9f89ee...|      ivy|  smith| miss|    titan|(987)-147-4845|https://randomuse...|https://randomuse...|https://randomuse...|1014225485|P4vPdsAk|be055502f1ccf22eb...|c7200e712298f7cec...|
|         NZ|67a0d5b2cce1b2630c|    0.8|       redtiger|(639)-652-8519| 905238245|jackson.walker@ex...|  male|        gisborne|       wellington|  6422 edmonton road|32978|a39e19f5b3e046d12...|  jackson| walker|   mr|   greens|(610)-263-1027|https://randomuse...|https://randomuse...|https://randomuse...|1423786250|NmE8DjSm|5822556f37b5380e7...|6e9e0c697c1e46da8...|
|         NZ|67a0d5b2cce1b2630c|    0.8|   beautifuldog|(809)-384-2657| 200217692|scarlett.jones@ex...|female|        hamilton|       canterbury|7833 south wester...|81149|38d1ea807a3728945...| scarlett|  jones|   ms|     1952|(128)-681-9528|https://randomuse...|https://randomuse...|https://randomuse...|1014987993|sOjxotld|1aa1162da5850fd16...|163d9fc884736aef4...|
|         NZ|67a0d5b2cce1b2630c|    0.8|     yellowduck|(289)-836-6419|1064605949|dylan.white@examp...|  male|    christchurch|manawatu-wanganui|   4845 chatham road|79449|6793655f52306265e...|    dylan|  white|   mr| cygnusx1|(170)-488-2886|https://randomuse...|https://randomuse...|https://randomuse...|1175669524|bezAtsfi|64ed9cf0d9bfb7d1d...|32e10119bb427eb92...|
|         NZ|67a0d5b2cce1b2630c|    0.8|     goldenswan|(479)-746-6637| 801729386|tyler.kumar@examp...|  male|          napier|manawatu-wanganui|    8341 taieri road|65062|59e6497f49c08d14f...|    tyler|  kumar|   mr|    fmale|(080)-535-1387|https://randomuse...|https://randomuse...|https://randomuse...|1299445629|cdWOCdzm|e7af222dfbafdf992...|ecf3c5949b43f3dfc...|
|         NZ|67a0d5b2cce1b2630c|    0.8|     bigostrich|(260)-794-8920|1409374812|hunter.jones@exam...|  male|       whangarei|           tasman|1741 western hill...|98012|81c77ff66661e8cdc...|   hunter|  jones|   mr|    butch|(989)-661-2323|https://randomuse...|https://randomuse...|https://randomuse...|1361694923|p9NiRchp|307d8c2f41cb71a41...|1130031757e33ecd4...|
|         NZ|67a0d5b2cce1b2630c|    0.8|         redcat|(537)-883-9797| 876506296|hugo.lewis@exampl...|  male|    invercargill|            otago|   5051 papanui road|89486|9ea9cb1051f21e840...|     hugo|  lewis|   mr|  spanker|(414)-077-6696|https://randomuse...|https://randomuse...|https://randomuse...|1302037561|67NqHPI3|24d8f1d2e73319002...|576fb0bbabcda4cff...|
|         NZ|67a0d5b2cce1b2630c|    0.8| organicpeacock|(324)-275-2275| 244678702|chloe.roberts@exa...|female|       whangarei|         taranaki|  4363 fraser street|17397|97ff0f2be537b268b...|    chloe|roberts| miss|   kodiak|(700)-770-5125|https://randomuse...|https://randomuse...|https://randomuse...|1264331025|uq5H7kc5|13baf9af38a85d643...|52e5fd287509f742e...|
|         NZ|67a0d5b2cce1b2630c|    0.8|      heavybear|(103)-435-9554| 651199027|joseph.kumar@exam...|  male|      upper hutt|           nelson|8972 montreal street|69034|6b4ee8afadad9ced2...|   joseph|  kumar|   mr|   gunner|(513)-237-2428|https://randomuse...|https://randomuse...|https://randomuse...|1260556789|LszA5gCa|6d3dfb11915861a60...|c9fec3a76ac28c5b9...|
|         NZ|67a0d5b2cce1b2630c|    0.8|    tinyladybug|(623)-463-4940| 962020819|madison.wood@exam...|female|palmerston north|        southland|  1378 walton street|72660|1c12fc10f29ae6971...|  madison|   wood|   ms|    blink|(807)-816-9610|https://randomuse...|https://randomuse...|https://randomuse...|1307881381|7oo7WGCQ|7e816d8f480201585...|5ddd5605c6ba7cdbd...|
|         NZ|67a0d5b2cce1b2630c|    0.8|beautifulrabbit|(300)-219-9878|1206746947|maia.wood@example...|female|       whangarei|         gisborne|  3176 marine parade|68608|1025e6ddee39de623...|     maia|   wood|   ms|  glasgow|(212)-156-8122|https://randomuse...|https://randomuse...|https://randomuse...|1143886534|kasVQOxj|bf96445dcba4effb7...|cb55db9a97019214a...|
|         NZ|67a0d5b2cce1b2630c|    0.8|  silverleopard|(817)-488-2109| 881901818|edward.johnson@ex...|  male|        tauranga|          waikato|1184 main street ...|39162|3f6d5a8fe77e82815...|   edward|johnson|   mr| zzzzzzzz|(780)-840-7948|https://randomuse...|https://randomuse...|https://randomuse...|1016587804|BGJlhyEU|2aae3d484fdca6401...|54867e97102233bf3...|
|         NZ|67a0d5b2cce1b2630c|    0.8|    goldentiger|(686)-142-2269| 677226781|dylan.edwards@exa...|  male|palmerston north|      marlborough|2100 andersons ba...|28810|4471e2d9618b7d737...|    dylan|edwards|   mr|   jerome|(235)-734-4962|https://randomuse...|https://randomuse...|https://randomuse...|1251756355|XiCltdzq|cb0d2e2caf1e64802...|baba487115b8ebc7d...|
|         NZ|67a0d5b2cce1b2630c|    0.8|       crazycat|(088)-602-0681|1169108381|emma.walker@examp...|female|           taupo|    bay of plenty|     8925 opawa road|89117|fb6d7013a2ad80d46...|     emma| walker|  mrs|   rubble|(282)-495-4878|https://randomuse...|https://randomuse...|https://randomuse...|1076433972|iIRszvWN|fa220a80d95564b4f...|cc11a523bdcfc01ef...|
|         NZ|67a0d5b2cce1b2630c|    0.8|      greenswan|(855)-092-6425| 259975499|oliver.taylor@exa...|  male|      upper hutt|       canterbury|    5576 wairau road|53933|72342c37001da20ff...|   oliver| taylor|   mr|   combat|(645)-744-8102|https://randomuse...|https://randomuse...|https://randomuse...|1429829246|uktFtg5Q|d5f43e08d09a29492...|bfdf49dcd571160ba...|
|         NZ|67a0d5b2cce1b2630c|    0.8|    silvermouse|(916)-041-5973| 854550510|max.taylor@exampl...|  male|         dunedin|       west coast|7520 cavendish drive|42615|402b7cbaeffc09476...|      max| taylor|   mr|   eagle1|(143)-216-7525|https://randomuse...|https://randomuse...|https://randomuse...|1312280197|DWfIO2pA|a45d2c9a546567bab...|f73f240957934c0b5...|
|         NZ|67a0d5b2cce1b2630c|    0.8|    silvertiger|(411)-777-1459| 224533700|james.davies@exam...|  male|        hastings|        southland|   5665 penrose road|30303|7d1fbeb156f753889...|    james| davies|   mr|wolverine|(428)-914-3505|https://randomuse...|https://randomuse...|https://randomuse...|1163581362|h1F7goSb|763e427ca59e11330...|7d59347b8ca4474ce...|
+-----------+------------------+-------+---------------+--------------+----------+--------------------+------+----------------+-----------------+--------------------+-----+--------------------+---------+-------+-----+---------+--------------+--------------------+--------------------+--------------------+----------+--------+--------------------+--------------------+
only showing top 20 rows






====================== Step 6 =========Joined Dataframe=============================================





+-------------+---+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+-----------+----+-------+----+----+-----+------+----+-----+------+----+----+-----+----+-----+--------+-----+-----+------+---------+----------+----+----+------+
|     username| id|amount|             ip|            createdt|value|score|regioncode|status|method|                 key|count|                type|                site|statuscode|nationality|seed|version|cell| dob|email|gender|city|state|street| zip| md5|first|last|title|password|phone|large|medium|thumbnail|registered|salt|sha1|sha256|
+-------------+---+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+-----------+----+-------+----+----+-----+------+----+-----+------+----+----+-----+----+-----+--------+-----+-----+------+---------+----------+----+----+------+
|beautifulbear|  1| 01743|  68.688.326.58|13/Jun/2012:10:55...|   10|   55|        45| -0500|   GET|           /products|    0|   /product/product4|Mozilla/4.0 (comp...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  2| 75756|  361.631.17.30|01/Nov/2011:10:53...|   10|   53|        01| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  3| 05901|   11.308.46.48|21/Jun/2012:01:12...|   01|   12|        33| -0500|   GET|               /news|    0|    /partners/resell|Mozilla/5.0 (comp...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  4| 24538|  361.631.17.30|01/Nov/2011:13:48...|   13|   48|        30| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  5| 08116|  322.76.611.36|30/Jun/2012:07:19...|   07|   19|        30| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  6| 04675|  361.631.17.30|01/Nov/2011:16:19...|   16|   19|        00| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  7| 06575| 367.34.686.631|01/Jul/2012:00:02...|   00|   02|        00| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  8| 27382|   88.633.11.47|04/Nov/2011:13:00...|   13|   00|        30| -0500|   GET|        /feeds/press|    0|                   -|Apple-PubSub/65.12.1|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  9| 09934|   11.308.46.48|01/Jul/2012:23:13...|   23|   13|        53| -0500|   GET|           /partners|    0|   /product/product2|Mozilla/5.0 (comp...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 10| 01716|683.615.622.618|05/Nov/2011:22:03...|   22|   03|        01| -0500|   GET|    /partners/resell|    0|    /partners/resell|                null|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 11| 55195| 682.62.387.672|02/Jul/2012:10:17...|   10|   17|        48| -0500|   GET|/download/downloa...|    0|/download/downloa...|Mozilla/5.0 (Wind...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 12| 69809|  43.68.686.668|09/Nov/2011:05:25...|   05|   25|        00| -0500|   GET|        /feeds/press|    0|                   -|  Apple-PubSub/65.11|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 13| 78800|321.336.372.320|06/Jul/2012:01:41...|   01|   41|        00| -0500|   GET|        /feeds/press|    0|                   -|Mozilla/5.0 (Wind...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 14| 58994|  361.631.17.30|16/Nov/2011:00:50...|   00|   50|        00| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 15| 94543| 361.638.668.62|13/Jul/2012:23:44...|   23|   44|        30| -0500|   GET|            /ad/save|    0|                   -|Mozilla/5.0 (Twic...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 16| 33695|   325.87.75.36|16/Nov/2011:04:58...|   04|   58|        30| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 17| 01755|  18.332.18.672|16/Jul/2012:03:20...|   03|   20|        30| -0500|   GET|        /feeds/press|    0|                   -|Apple-PubSub/65.12.1|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 18| 01784|   361.321.73.6|17/Nov/2011:13:08...|   13|   08|        30| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 19| 09671|   364.63.61.83|23/Jul/2012:17:00...|   17|   00|        37| -0500|   GET|   /product/product3|    0|      /docs/doc5.pdf|Mozilla/5.0 (X11;...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 20| 04288|    13.53.52.13|26/Nov/2011:19:16...|   19|   16|        00| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
+-------------+---+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+-----------+----+-------+----+----+-----+------+----+-----+------+----+----+-----+----+-----+--------+-----+-----+------+---------+----------+----+----+------+
only showing top 20 rows





=================== Step 7 a ============Not available customers=============================================










==================  Step 7 b =============available customers=============================================






=========================================================================

==========================================
package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.functions.upper
import org.apache.spark.sql.catalyst.expressions.Upper
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window
import scala.io.Source
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions._

object obj {

	def main(args:Array[String]):Unit={
			val conf = new SparkConf().setAppName("Revision").setMaster("local[*]")
					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")

					val spark = SparkSession.builder().enableHiveSupport()
					.config("spark.sql.warehouse.dir", "file:///D:/hivewarehou/")
					.config("spark.sql.catalogImplementation","hive").getOrCreate()
					import spark.implicits._


					val df = spark.read.format("json")
					.option("multiline","true")
					.load("file:///D:/data/jc6.json")

					df.show()
					df.printSchema()

					println
					println
					println



					val flattendf = df
					.withColumn("Students", expr("explode(Students)"))

					flattendf.show()
					flattendf.printSchema()
					
					
					println
					println
					println
					
					
					
					
				val finalflatten = flattendf.select(
				    
				                          "Students.user.*",
				                          "Years",
				                          "org",
				                          "trainer"
				                          
		
				                      )
					
				  finalflatten.show()
				  finalflatten.printSchema()
				  
				  
				  
				  
				  
      val complexdf = finalflatten.groupBy("Years","org","trainer")
                      
                      .agg(
                            
                             collect_list(
                                 
                                   struct(
                                        
                                        struct(
                                              col("age"),
                                              col("name")
                                          
                                        
                                              ).as("user") 
                                   
                                          )
                              
                                 ).as("Students") 
                          
                            )
				  
				complexdf.show()
				complexdf.printSchema()
	
	}



}

===========
Out put:
--------
+--------------------+-----+--------+-------+
|            Students|Years|     org|trainer|
+--------------------+-----+--------+-------+
|[[[40, ravi]], [[...|    4|zeyobron|    sai|
+--------------------+-----+--------+-------+

root
 |-- Students: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- user: struct (nullable = true)
 |    |    |    |-- age: long (nullable = true)
 |    |    |    |-- name: string (nullable = true)
 |-- Years: long (nullable = true)
 |-- org: string (nullable = true)
 |-- trainer: string (nullable = true)




+------------+-----+--------+-------+
|    Students|Years|     org|trainer|
+------------+-----+--------+-------+
|[[40, ravi]]|    4|zeyobron|    sai|
|[[20, vasu]]|    4|zeyobron|    sai|
+------------+-----+--------+-------+

root
 |-- Students: struct (nullable = true)
 |    |-- user: struct (nullable = true)
 |    |    |-- age: long (nullable = true)
 |    |    |-- name: string (nullable = true)
 |-- Years: long (nullable = true)
 |-- org: string (nullable = true)
 |-- trainer: string (nullable = true)




+---+----+-----+--------+-------+
|age|name|Years|     org|trainer|
+---+----+-----+--------+-------+
| 40|ravi|    4|zeyobron|    sai|
| 20|vasu|    4|zeyobron|    sai|
+---+----+-----+--------+-------+

root
 |-- age: long (nullable = true)
 |-- name: string (nullable = true)
 |-- Years: long (nullable = true)
 |-- org: string (nullable = true)
 |-- trainer: string (nullable = true)

+-----+--------+-------+--------------------+
|Years|     org|trainer|            Students|
+-----+--------+-------+--------------------+
|    4|zeyobron|    sai|[[[40, ravi]], [[...|
+-----+--------+-------+--------------------+

root
 |-- Years: long (nullable = true)
 |-- org: string (nullable = true)
 |-- trainer: string (nullable = true)
 |-- Students: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- user: struct (nullable = false)
 |    |    |    |-- age: long (nullable = true)
 |    |    |    |-- name: string (nullable = true)

==========================================


====================================================
Project Execution 1 10 Steps
====================================================

package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.functions.upper
import org.apache.spark.sql.catalyst.expressions.Upper
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window
import scala.io.Source
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions._

object obj {

	def main(args:Array[String]):Unit={


			val conf = new SparkConf().setAppName("ES").setMaster("local[*]")

					val sc = new SparkContext(conf)
					sc.setLogLevel("Error")


					val spark = SparkSession.builder()



					.getOrCreate()

					import spark.implicits._


					println
					println
					println
					println
					println
					println("======================= Step 2 ========Raw data=============================================")
					println
					println
					println
					println
					println




					val data = spark.read.format("avro")
					.load("file:///D://data//projectsample.avro")

					data.show()




					println
					println
					println
					println
					println
					println("======================== Step 3 ========Url data=============================================")
					println
					println
					println
					println
					println


					val html = Source.fromURL("https://randomuser.me/api/0.8/?results=500")
					val s = html.mkString
					//println(s)


					val urldf = spark.read.json(sc.parallelize(List(s)))
					urldf.show()




					println
					println
					println
					println
					println
					println("========================step 4 flatten dataframe=============================================")
					println
					println
					println
					println
					println

					val flatdf = urldf.withColumn("results",explode(col("results"))).select("nationality","seed","version",
							"results.user.username","results.user.cell","results.user.dob","results.user.email",
							"results.user.gender","results.user.location.city","results.user.location.state",
							"results.user.location.street","results.user.location.zip","results.user.md5",
							"results.user.name.first","results.user.name.last","results.user.name.title",
							"results.user.password","results.user.phone","results.user.picture.large","results.user.picture.medium"
							,"results.user.picture.thumbnail","results.user.registered","results.user.salt","results.user.sha1"
							,"results.user.sha256")
					flatdf.show()






					println
					println
					println
					println
					println
					println("========================step 5 removed numericals Dataframe=============================================")
					println
					println
					println
					println
					println

					val rm=flatdf.withColumn("username",regexp_replace(col("username"),  "([0-9])", ""))
					rm.show()







					println
					println
					println
					println
					println
					println("====================== Step 6 =========Joined Dataframe=============================================")
					println
					println
					println
					println
					println
					val joindf = data.join(broadcast(rm),Seq("username"),"left")


					joindf.show()


					println
					println
					println
					println
					println("=================== Step 7 a ============Not available customers=============================================")
					println
					println
					println
					println
					println
					println


					val dfnull = joindf.filter(col("nationality").isNull)


					val dfnotnull=joindf.filter(col("nationality").isNotNull)



					dfnull.show()




					println
					
					
					println
					println
					println
					println("==================  Step 7 b =============available customers=============================================")
					println
					println
					println
					println
					println
					println




					dfnotnull.show()





					println
					println
					println
					println
					println("=============== Step 8 ================Null handled dataframe=============================================")
					println
					println
					println
					println
					println
					println



					val replacenull= dfnull.na.fill("Not Available").na.fill(0)
					replacenull.show()


					println
					println
					println
					println
					println("=============== Step 9 a ================not available customers with current date dataframe=============================================")
					println
					println
					println
					println
					println
					println



					val replacenull_with_current_date=replacenull.withColumn("current_date",current_date)

					replacenull_with_current_date.show()

					println
					println
					println



					println
					println
					println
					println
					println("=============== Step 9 b ================available customers with current date dataframe=============================================")
					println
					println
					println
					println
					println
					println



					val notnull_with_current_date=dfnotnull.withColumn("current_date",current_date)


					notnull_with_current_date.show()




					notnull_with_current_date.write.format("parquet").mode("append").partitionBy("current_date")
					            .save("file:///D:/data/project/availablecustomer")
					
					
					
					replacenull_with_current_date.write.format("parquet").mode("append").partitionBy("current_date")
					            .save("file:///D:/data/project/notavailablecustomer")
					
					
	    }



	}
	
===================
Out put:
--------
======================= Step 2 ========Raw data=============================================





+---+-------------+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+
| id|     username|amount|             ip|            createdt|value|score|regioncode|status|method|                 key|count|                type|                site|statuscode|
+---+-------------+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+
|  1|beautifulbear| 01743|  68.688.326.58|13/Jun/2012:10:55...|   10|   55|        45| -0500|   GET|           /products|    0|   /product/product4|Mozilla/4.0 (comp...|       100|
|  2|beautifulbear| 75756|  361.631.17.30|01/Nov/2011:10:53...|   10|   53|        01| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|
|  3|beautifulbear| 05901|   11.308.46.48|21/Jun/2012:01:12...|   01|   12|        33| -0500|   GET|               /news|    0|    /partners/resell|Mozilla/5.0 (comp...|       100|
|  4|beautifulbear| 24538|  361.631.17.30|01/Nov/2011:13:48...|   13|   48|        30| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|
|  5|beautifulbear| 08116|  322.76.611.36|30/Jun/2012:07:19...|   07|   19|        30| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|
|  6|beautifulbear| 04675|  361.631.17.30|01/Nov/2011:16:19...|   16|   19|        00| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|
|  7|beautifulbear| 06575| 367.34.686.631|01/Jul/2012:00:02...|   00|   02|        00| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|
|  8|beautifulbear| 27382|   88.633.11.47|04/Nov/2011:13:00...|   13|   00|        30| -0500|   GET|        /feeds/press|    0|                   -|Apple-PubSub/65.12.1|       100|
|  9|beautifulbear| 09934|   11.308.46.48|01/Jul/2012:23:13...|   23|   13|        53| -0500|   GET|           /partners|    0|   /product/product2|Mozilla/5.0 (comp...|       100|
| 10|beautifulbear| 01716|683.615.622.618|05/Nov/2011:22:03...|   22|   03|        01| -0500|   GET|    /partners/resell|    0|    /partners/resell|                null|       100|
| 11|beautifulbear| 55195| 682.62.387.672|02/Jul/2012:10:17...|   10|   17|        48| -0500|   GET|/download/downloa...|    0|/download/downloa...|Mozilla/5.0 (Wind...|       100|
| 12|beautifulbear| 69809|  43.68.686.668|09/Nov/2011:05:25...|   05|   25|        00| -0500|   GET|        /feeds/press|    0|                   -|  Apple-PubSub/65.11|       100|
| 13|beautifulbear| 78800|321.336.372.320|06/Jul/2012:01:41...|   01|   41|        00| -0500|   GET|        /feeds/press|    0|                   -|Mozilla/5.0 (Wind...|       100|
| 14|beautifulbear| 58994|  361.631.17.30|16/Nov/2011:00:50...|   00|   50|        00| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|
| 15|beautifulbear| 94543| 361.638.668.62|13/Jul/2012:23:44...|   23|   44|        30| -0500|   GET|            /ad/save|    0|                   -|Mozilla/5.0 (Twic...|       100|
| 16|beautifulbear| 33695|   325.87.75.36|16/Nov/2011:04:58...|   04|   58|        30| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|
| 17|beautifulbear| 01755|  18.332.18.672|16/Jul/2012:03:20...|   03|   20|        30| -0500|   GET|        /feeds/press|    0|                   -|Apple-PubSub/65.12.1|       100|
| 18|beautifulbear| 01784|   361.321.73.6|17/Nov/2011:13:08...|   13|   08|        30| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|
| 19|beautifulbear| 09671|   364.63.61.83|23/Jul/2012:17:00...|   17|   00|        37| -0500|   GET|   /product/product3|    0|      /docs/doc5.pdf|Mozilla/5.0 (X11;...|       100|
| 20|beautifulbear| 04288|    13.53.52.13|26/Nov/2011:19:16...|   19|   16|        00| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|
+---+-------------+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+
only showing top 20 rows






======================== Step 3 ========Url data=============================================





+-----------+--------------------+------------------+-------+
|nationality|             results|              seed|version|
+-----------+--------------------+------------------+-------+
|         US|[[[794-27-7030, (...|c147a1dc8e2ced1c0d|    0.8|
+-----------+--------------------+------------------+-------+






========================step 4 flatten dataframe=============================================





+-----------+------------------+-------+------------------+--------------+----------+--------------------+------+----------------+--------------+--------------------+-----+--------------------+----------+---------+-----+--------+--------------+--------------------+--------------------+--------------------+----------+--------+--------------------+--------------------+
|nationality|              seed|version|          username|          cell|       dob|               email|gender|            city|         state|              street|  zip|                 md5|     first|     last|title|password|         phone|               large|              medium|           thumbnail|registered|    salt|                sha1|              sha256|
+-----------+------------------+-------+------------------+--------------+----------+--------------------+------+----------------+--------------+--------------------+-----+--------------------+----------+---------+-----+--------+--------------+--------------------+--------------------+--------------------+----------+--------+--------------------+--------------------+
|         US|c147a1dc8e2ced1c0d|    0.8|     silverbear545|(701)-381-5922| 705345419|danielle.diaz@exa...|female|       las vegas| massachusetts|    8421 fairview st|43036|4c08f23b2022a15ba...|  danielle|     diaz| miss|broncos1|(719)-555-9771|https://randomuse...|https://randomuse...|https://randomuse...|1224512053|WgAoFb1Z|9234d614052be2c05...|38a8dcdbe0a9fb33a...|
|         US|c147a1dc8e2ced1c0d|    0.8|    brownrabbit627|(194)-103-4269| 295990876|louella.williams@...|female|       lafayette|  north dakota|   4242 hamilton ave|74474|cfebc022daf9b4676...|   louella| williams| miss|    vbnm|(476)-061-3150|https://randomuse...|https://randomuse...|https://randomuse...| 927568221|3bJIHh6M|b8f85980ccca4604d...|be808e291c944a1e6...|
|         US|c147a1dc8e2ced1c0d|    0.8|        lazydog788|(747)-349-6707|1189322573|ben.dixon@example...|  male|      the colony|north carolina|       3562 hogan st|12829|ef47a0bdd6a60cf27...|       ben|    dixon|   mr|toriamos|(605)-844-2801|https://randomuse...|https://randomuse...|https://randomuse...|1249711992|G9fX0RQZ|d003fc029061ce0d9...|c6fdefd75a2936cac...|
|         US|c147a1dc8e2ced1c0d|    0.8|     goldenduck451|(406)-868-7732|1336870293|clifford.garza@ex...|  male|       princeton|      colorado|  9960 n stelling rd|22365|d0d0346002fe0cfd4...|  clifford|    garza|   mr|  trance|(982)-650-5835|https://randomuse...|https://randomuse...|https://randomuse...|1201247499|m6RIARw4|fbc786b6c71c2869d...|e474337c1447f770c...|
|         US|c147a1dc8e2ced1c0d|    0.8|       blackcat144|(344)-101-8824| 876171382|vera.marshall@exa...|female|         jackson| new hampshire|       6666 karen dr|90425|1aeb796aa875f83b7...|      vera| marshall|   ms|    1001|(623)-490-7935|https://randomuse...|https://randomuse...|https://randomuse...|1243922732|1tCrEUay|43cc213a19b41d409...|91abe633b02473bd4...|
|         US|c147a1dc8e2ced1c0d|    0.8|    silvergoose801|(709)-839-5291|1353481204|janice.kuhn@examp...|female|          newark|       georgia|      3007 poplar dr|95967|d8a8e095fb2ab3f6f...|    janice|     kuhn|  mrs|   skirt|(837)-819-4131|https://randomuse...|https://randomuse...|https://randomuse...|1126461036|kTgq2xpC|0ccbcd25f8b554834...|d28510775ed25c4f3...|
|         US|c147a1dc8e2ced1c0d|    0.8|        bluedog398|(347)-222-5515| 771136554|gwendolyn.fields@...|female|       ann arbor|          iowa|      4074 smokey ln|78127|ef2b253b3e244679e...| gwendolyn|   fields|  mrs| slugger|(760)-080-6140|https://randomuse...|https://randomuse...|https://randomuse...|1399049446|RyTlTtpG|ef6e06367ee5106fe...|381f1da9f47b7a5a6...|
|         US|c147a1dc8e2ced1c0d|    0.8|      crazyswan543|(128)-446-6539|1023970331|peyton.torres@exa...|female|west valley city|  south dakota|   8807 photinia ave|60877|91a318ea8f4aac870...|    peyton|   torres|  mrs| squeeze|(452)-625-7836|https://randomuse...|https://randomuse...|https://randomuse...|1040314800|c3MUVjV8|8ede6fe6772368fd4...|35b335a29241f2b34...|
|         US|c147a1dc8e2ced1c0d|    0.8|     smallgoose379|(925)-105-5820| 350686778|wesley.duncan@exa...|  male|           omaha|          utah|9963 timber wolf ...|28884|75f99ab7adc564245...|    wesley|   duncan|   mr|   games|(612)-404-8364|https://randomuse...|https://randomuse...|https://randomuse...|1000733606|hH3Qhw95|28358523bbbe231d8...|57f561b8832a60b0c...|
|         US|c147a1dc8e2ced1c0d|    0.8|     whitepanda633|(720)-312-7311| 810540592|tommy.jordan@exam...|  male|      costa mesa|    new jersey|      3320 sunset st|74430|78e519978a74305c6...|     tommy|   jordan|   mr|  komodo|(689)-577-9349|https://randomuse...|https://randomuse...|https://randomuse...|1360504034|Z7zblNub|0ed3c67ba12c98378...|91a8749e4990f11d9...|
|         US|c147a1dc8e2ced1c0d|    0.8|     goldenfrog554|(853)-419-5042| 447450456|franklin.young@ex...|  male|      south bend|      oklahoma|    3168 paddock way|44736|40f03fe7ad9d699ad...|  franklin|    young|   mr| stryker|(757)-938-8724|https://randomuse...|https://randomuse...|https://randomuse...|1245193120|iNZThJHR|80e00c59053a3ee32...|d74e9b62c97a20e99...|
|         US|c147a1dc8e2ced1c0d|    0.8|ticklishleopard123|(152)-590-8318|1071191776|jacqueline.garza@...|female|     victorville|   mississippi|     6398 camden ave|18643|2b3066ae40df0e353...|jacqueline|    garza|  mrs| merlin1|(638)-670-5371|https://randomuse...|https://randomuse...|https://randomuse...| 951939219|G5yu6mXv|600a624f78d6a17c6...|1e70ceeb836541160...|
|         US|c147a1dc8e2ced1c0d|    0.8|   organictiger650|(245)-397-2073| 164048285|christian.lawrenc...|  male|        savannah|      new york|     3318 miller ave|47656|e2e2ca13c399f0e51...| christian| lawrence|   mr|  hun999|(976)-345-7468|https://randomuse...|https://randomuse...|https://randomuse...|1050987002|eoR45FDt|658e1e3eee7efd3f4...|d4bd3f611cf0c2d03...|
|         US|c147a1dc8e2ced1c0d|    0.8|  greenelephant580|(849)-059-9185| 758749000|ruby.mcdonalid@ex...|female|      providence|  south dakota|       7174 daisy dr|40757|8f453a8b12fc31372...|      ruby|mcdonalid| miss|   leigh|(629)-614-3426|https://randomuse...|https://randomuse...|https://randomuse...| 994033421|mBlzY05H|c46da236939b80b51...|c6888b9b27d28c734...|
|         US|c147a1dc8e2ced1c0d|    0.8|    silvergoose973|(703)-572-1606| 237245714|marcus.kelley@exa...|  male|     sioux falls|         maine|      8351 cherry st|43059|8f1f431f801573dae...|    marcus|   kelley|   mr| nittany|(261)-791-5607|https://randomuse...|https://randomuse...|https://randomuse...| 946017953|sP4jLGHx|921643834dbb7a23a...|905d8a6211d14ae7b...|
|         US|c147a1dc8e2ced1c0d|    0.8|      heavylion658|(517)-083-4818| 577563277|rene.hill@example...|  male|          athens|       georgia|1890 stevens cree...|24557|a5f2f227121b09c0e...|      rene|     hill|   mr|  poland|(323)-752-6268|https://randomuse...|https://randomuse...|https://randomuse...|1207884506|t6LyBEba|988f4b9599b4d8965...|6ddaea8cb1ef2ece3...|
|         US|c147a1dc8e2ced1c0d|    0.8|   crazyladybug829|(648)-647-1218| 613896878|rachel.fuller@exa...|female|     simi valley|        nevada|     2697 e pecan st|93264|9af94229c66a112c1...|    rachel|   fuller|  mrs| mankind|(744)-922-2440|https://randomuse...|https://randomuse...|https://randomuse...|1414917829|J71qS0kX|2711310450c1f7805...|d89aa9466faf466c8...|
|         US|c147a1dc8e2ced1c0d|    0.8|        lazydog687|(552)-296-4973|1047153248|lisa.torres@examp...|female|           miami|          ohio|4504 timber wolf ...|95720|33b99173831e3c3ad...|      lisa|   torres|   ms|  elaine|(463)-926-8083|https://randomuse...|https://randomuse...|https://randomuse...|1291714701|0P6WsY3w|38f97b50a86a885f4...|15da6e5b4cb66ce29...|
|         US|c147a1dc8e2ced1c0d|    0.8|     lazyrabbit964|(469)-514-1900|1363719188|lisa.sims@example...|female|           akron|    new jersey|5757 lone wolf trail|31253|dce713c99adc3cfc6...|      lisa|     sims|  mrs| trucker|(254)-046-3907|https://randomuse...|https://randomuse...|https://randomuse...|1266952000|33cZ3hXI|5b045983477d96539...|c53a6da96b7697992...|
|         US|c147a1dc8e2ced1c0d|    0.8|      lazypanda488|(851)-340-3246| 773246709|terrence.jordan@e...|  male|       san diego|      illinois|   6395 homestead rd|88716|ff4743fd18c48ade4...|  terrence|   jordan|   mr|satan666|(139)-839-0874|https://randomuse...|https://randomuse...|https://randomuse...|1199147913|wksqsIsW|a0573661d209ad2c3...|7e3dd6125ab399838...|
+-----------+------------------+-------+------------------+--------------+----------+--------------------+------+----------------+--------------+--------------------+-----+--------------------+----------+---------+-----+--------+--------------+--------------------+--------------------+--------------------+----------+--------+--------------------+--------------------+
only showing top 20 rows






========================step 5 removed numericals Dataframe=============================================





+-----------+------------------+-------+---------------+--------------+----------+--------------------+------+----------------+--------------+--------------------+-----+--------------------+----------+---------+-----+--------+--------------+--------------------+--------------------+--------------------+----------+--------+--------------------+--------------------+
|nationality|              seed|version|       username|          cell|       dob|               email|gender|            city|         state|              street|  zip|                 md5|     first|     last|title|password|         phone|               large|              medium|           thumbnail|registered|    salt|                sha1|              sha256|
+-----------+------------------+-------+---------------+--------------+----------+--------------------+------+----------------+--------------+--------------------+-----+--------------------+----------+---------+-----+--------+--------------+--------------------+--------------------+--------------------+----------+--------+--------------------+--------------------+
|         US|c147a1dc8e2ced1c0d|    0.8|     silverbear|(701)-381-5922| 705345419|danielle.diaz@exa...|female|       las vegas| massachusetts|    8421 fairview st|43036|4c08f23b2022a15ba...|  danielle|     diaz| miss|broncos1|(719)-555-9771|https://randomuse...|https://randomuse...|https://randomuse...|1224512053|WgAoFb1Z|9234d614052be2c05...|38a8dcdbe0a9fb33a...|
|         US|c147a1dc8e2ced1c0d|    0.8|    brownrabbit|(194)-103-4269| 295990876|louella.williams@...|female|       lafayette|  north dakota|   4242 hamilton ave|74474|cfebc022daf9b4676...|   louella| williams| miss|    vbnm|(476)-061-3150|https://randomuse...|https://randomuse...|https://randomuse...| 927568221|3bJIHh6M|b8f85980ccca4604d...|be808e291c944a1e6...|
|         US|c147a1dc8e2ced1c0d|    0.8|        lazydog|(747)-349-6707|1189322573|ben.dixon@example...|  male|      the colony|north carolina|       3562 hogan st|12829|ef47a0bdd6a60cf27...|       ben|    dixon|   mr|toriamos|(605)-844-2801|https://randomuse...|https://randomuse...|https://randomuse...|1249711992|G9fX0RQZ|d003fc029061ce0d9...|c6fdefd75a2936cac...|
|         US|c147a1dc8e2ced1c0d|    0.8|     goldenduck|(406)-868-7732|1336870293|clifford.garza@ex...|  male|       princeton|      colorado|  9960 n stelling rd|22365|d0d0346002fe0cfd4...|  clifford|    garza|   mr|  trance|(982)-650-5835|https://randomuse...|https://randomuse...|https://randomuse...|1201247499|m6RIARw4|fbc786b6c71c2869d...|e474337c1447f770c...|
|         US|c147a1dc8e2ced1c0d|    0.8|       blackcat|(344)-101-8824| 876171382|vera.marshall@exa...|female|         jackson| new hampshire|       6666 karen dr|90425|1aeb796aa875f83b7...|      vera| marshall|   ms|    1001|(623)-490-7935|https://randomuse...|https://randomuse...|https://randomuse...|1243922732|1tCrEUay|43cc213a19b41d409...|91abe633b02473bd4...|
|         US|c147a1dc8e2ced1c0d|    0.8|    silvergoose|(709)-839-5291|1353481204|janice.kuhn@examp...|female|          newark|       georgia|      3007 poplar dr|95967|d8a8e095fb2ab3f6f...|    janice|     kuhn|  mrs|   skirt|(837)-819-4131|https://randomuse...|https://randomuse...|https://randomuse...|1126461036|kTgq2xpC|0ccbcd25f8b554834...|d28510775ed25c4f3...|
|         US|c147a1dc8e2ced1c0d|    0.8|        bluedog|(347)-222-5515| 771136554|gwendolyn.fields@...|female|       ann arbor|          iowa|      4074 smokey ln|78127|ef2b253b3e244679e...| gwendolyn|   fields|  mrs| slugger|(760)-080-6140|https://randomuse...|https://randomuse...|https://randomuse...|1399049446|RyTlTtpG|ef6e06367ee5106fe...|381f1da9f47b7a5a6...|
|         US|c147a1dc8e2ced1c0d|    0.8|      crazyswan|(128)-446-6539|1023970331|peyton.torres@exa...|female|west valley city|  south dakota|   8807 photinia ave|60877|91a318ea8f4aac870...|    peyton|   torres|  mrs| squeeze|(452)-625-7836|https://randomuse...|https://randomuse...|https://randomuse...|1040314800|c3MUVjV8|8ede6fe6772368fd4...|35b335a29241f2b34...|
|         US|c147a1dc8e2ced1c0d|    0.8|     smallgoose|(925)-105-5820| 350686778|wesley.duncan@exa...|  male|           omaha|          utah|9963 timber wolf ...|28884|75f99ab7adc564245...|    wesley|   duncan|   mr|   games|(612)-404-8364|https://randomuse...|https://randomuse...|https://randomuse...|1000733606|hH3Qhw95|28358523bbbe231d8...|57f561b8832a60b0c...|
|         US|c147a1dc8e2ced1c0d|    0.8|     whitepanda|(720)-312-7311| 810540592|tommy.jordan@exam...|  male|      costa mesa|    new jersey|      3320 sunset st|74430|78e519978a74305c6...|     tommy|   jordan|   mr|  komodo|(689)-577-9349|https://randomuse...|https://randomuse...|https://randomuse...|1360504034|Z7zblNub|0ed3c67ba12c98378...|91a8749e4990f11d9...|
|         US|c147a1dc8e2ced1c0d|    0.8|     goldenfrog|(853)-419-5042| 447450456|franklin.young@ex...|  male|      south bend|      oklahoma|    3168 paddock way|44736|40f03fe7ad9d699ad...|  franklin|    young|   mr| stryker|(757)-938-8724|https://randomuse...|https://randomuse...|https://randomuse...|1245193120|iNZThJHR|80e00c59053a3ee32...|d74e9b62c97a20e99...|
|         US|c147a1dc8e2ced1c0d|    0.8|ticklishleopard|(152)-590-8318|1071191776|jacqueline.garza@...|female|     victorville|   mississippi|     6398 camden ave|18643|2b3066ae40df0e353...|jacqueline|    garza|  mrs| merlin1|(638)-670-5371|https://randomuse...|https://randomuse...|https://randomuse...| 951939219|G5yu6mXv|600a624f78d6a17c6...|1e70ceeb836541160...|
|         US|c147a1dc8e2ced1c0d|    0.8|   organictiger|(245)-397-2073| 164048285|christian.lawrenc...|  male|        savannah|      new york|     3318 miller ave|47656|e2e2ca13c399f0e51...| christian| lawrence|   mr|  hun999|(976)-345-7468|https://randomuse...|https://randomuse...|https://randomuse...|1050987002|eoR45FDt|658e1e3eee7efd3f4...|d4bd3f611cf0c2d03...|
|         US|c147a1dc8e2ced1c0d|    0.8|  greenelephant|(849)-059-9185| 758749000|ruby.mcdonalid@ex...|female|      providence|  south dakota|       7174 daisy dr|40757|8f453a8b12fc31372...|      ruby|mcdonalid| miss|   leigh|(629)-614-3426|https://randomuse...|https://randomuse...|https://randomuse...| 994033421|mBlzY05H|c46da236939b80b51...|c6888b9b27d28c734...|
|         US|c147a1dc8e2ced1c0d|    0.8|    silvergoose|(703)-572-1606| 237245714|marcus.kelley@exa...|  male|     sioux falls|         maine|      8351 cherry st|43059|8f1f431f801573dae...|    marcus|   kelley|   mr| nittany|(261)-791-5607|https://randomuse...|https://randomuse...|https://randomuse...| 946017953|sP4jLGHx|921643834dbb7a23a...|905d8a6211d14ae7b...|
|         US|c147a1dc8e2ced1c0d|    0.8|      heavylion|(517)-083-4818| 577563277|rene.hill@example...|  male|          athens|       georgia|1890 stevens cree...|24557|a5f2f227121b09c0e...|      rene|     hill|   mr|  poland|(323)-752-6268|https://randomuse...|https://randomuse...|https://randomuse...|1207884506|t6LyBEba|988f4b9599b4d8965...|6ddaea8cb1ef2ece3...|
|         US|c147a1dc8e2ced1c0d|    0.8|   crazyladybug|(648)-647-1218| 613896878|rachel.fuller@exa...|female|     simi valley|        nevada|     2697 e pecan st|93264|9af94229c66a112c1...|    rachel|   fuller|  mrs| mankind|(744)-922-2440|https://randomuse...|https://randomuse...|https://randomuse...|1414917829|J71qS0kX|2711310450c1f7805...|d89aa9466faf466c8...|
|         US|c147a1dc8e2ced1c0d|    0.8|        lazydog|(552)-296-4973|1047153248|lisa.torres@examp...|female|           miami|          ohio|4504 timber wolf ...|95720|33b99173831e3c3ad...|      lisa|   torres|   ms|  elaine|(463)-926-8083|https://randomuse...|https://randomuse...|https://randomuse...|1291714701|0P6WsY3w|38f97b50a86a885f4...|15da6e5b4cb66ce29...|
|         US|c147a1dc8e2ced1c0d|    0.8|     lazyrabbit|(469)-514-1900|1363719188|lisa.sims@example...|female|           akron|    new jersey|5757 lone wolf trail|31253|dce713c99adc3cfc6...|      lisa|     sims|  mrs| trucker|(254)-046-3907|https://randomuse...|https://randomuse...|https://randomuse...|1266952000|33cZ3hXI|5b045983477d96539...|c53a6da96b7697992...|
|         US|c147a1dc8e2ced1c0d|    0.8|      lazypanda|(851)-340-3246| 773246709|terrence.jordan@e...|  male|       san diego|      illinois|   6395 homestead rd|88716|ff4743fd18c48ade4...|  terrence|   jordan|   mr|satan666|(139)-839-0874|https://randomuse...|https://randomuse...|https://randomuse...|1199147913|wksqsIsW|a0573661d209ad2c3...|7e3dd6125ab399838...|
+-----------+------------------+-------+---------------+--------------+----------+--------------------+------+----------------+--------------+--------------------+-----+--------------------+----------+---------+-----+--------+--------------+--------------------+--------------------+--------------------+----------+--------+--------------------+--------------------+
only showing top 20 rows






====================== Step 6 =========Joined Dataframe=============================================





+-------------+---+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+-----------+----+-------+----+----+-----+------+----+-----+------+----+----+-----+----+-----+--------+-----+-----+------+---------+----------+----+----+------+
|     username| id|amount|             ip|            createdt|value|score|regioncode|status|method|                 key|count|                type|                site|statuscode|nationality|seed|version|cell| dob|email|gender|city|state|street| zip| md5|first|last|title|password|phone|large|medium|thumbnail|registered|salt|sha1|sha256|
+-------------+---+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+-----------+----+-------+----+----+-----+------+----+-----+------+----+----+-----+----+-----+--------+-----+-----+------+---------+----------+----+----+------+
|beautifulbear|  1| 01743|  68.688.326.58|13/Jun/2012:10:55...|   10|   55|        45| -0500|   GET|           /products|    0|   /product/product4|Mozilla/4.0 (comp...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  2| 75756|  361.631.17.30|01/Nov/2011:10:53...|   10|   53|        01| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  3| 05901|   11.308.46.48|21/Jun/2012:01:12...|   01|   12|        33| -0500|   GET|               /news|    0|    /partners/resell|Mozilla/5.0 (comp...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  4| 24538|  361.631.17.30|01/Nov/2011:13:48...|   13|   48|        30| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  5| 08116|  322.76.611.36|30/Jun/2012:07:19...|   07|   19|        30| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  6| 04675|  361.631.17.30|01/Nov/2011:16:19...|   16|   19|        00| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  7| 06575| 367.34.686.631|01/Jul/2012:00:02...|   00|   02|        00| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  8| 27382|   88.633.11.47|04/Nov/2011:13:00...|   13|   00|        30| -0500|   GET|        /feeds/press|    0|                   -|Apple-PubSub/65.12.1|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  9| 09934|   11.308.46.48|01/Jul/2012:23:13...|   23|   13|        53| -0500|   GET|           /partners|    0|   /product/product2|Mozilla/5.0 (comp...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 10| 01716|683.615.622.618|05/Nov/2011:22:03...|   22|   03|        01| -0500|   GET|    /partners/resell|    0|    /partners/resell|                null|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 11| 55195| 682.62.387.672|02/Jul/2012:10:17...|   10|   17|        48| -0500|   GET|/download/downloa...|    0|/download/downloa...|Mozilla/5.0 (Wind...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 12| 69809|  43.68.686.668|09/Nov/2011:05:25...|   05|   25|        00| -0500|   GET|        /feeds/press|    0|                   -|  Apple-PubSub/65.11|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 13| 78800|321.336.372.320|06/Jul/2012:01:41...|   01|   41|        00| -0500|   GET|        /feeds/press|    0|                   -|Mozilla/5.0 (Wind...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 14| 58994|  361.631.17.30|16/Nov/2011:00:50...|   00|   50|        00| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 15| 94543| 361.638.668.62|13/Jul/2012:23:44...|   23|   44|        30| -0500|   GET|            /ad/save|    0|                   -|Mozilla/5.0 (Twic...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 16| 33695|   325.87.75.36|16/Nov/2011:04:58...|   04|   58|        30| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 17| 01755|  18.332.18.672|16/Jul/2012:03:20...|   03|   20|        30| -0500|   GET|        /feeds/press|    0|                   -|Apple-PubSub/65.12.1|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 18| 01784|   361.321.73.6|17/Nov/2011:13:08...|   13|   08|        30| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 19| 09671|   364.63.61.83|23/Jul/2012:17:00...|   17|   00|        37| -0500|   GET|   /product/product3|    0|      /docs/doc5.pdf|Mozilla/5.0 (X11;...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 20| 04288|    13.53.52.13|26/Nov/2011:19:16...|   19|   16|        00| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
+-------------+---+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+-----------+----+-------+----+----+-----+------+----+-----+------+----+----+-----+----+-----+--------+-----+-----+------+---------+----------+----+----+------+
only showing top 20 rows





=================== Step 7 a ============Not available customers=============================================






+-------------+---+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+-----------+----+-------+----+----+-----+------+----+-----+------+----+----+-----+----+-----+--------+-----+-----+------+---------+----------+----+----+------+
|     username| id|amount|             ip|            createdt|value|score|regioncode|status|method|                 key|count|                type|                site|statuscode|nationality|seed|version|cell| dob|email|gender|city|state|street| zip| md5|first|last|title|password|phone|large|medium|thumbnail|registered|salt|sha1|sha256|
+-------------+---+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+-----------+----+-------+----+----+-----+------+----+-----+------+----+----+-----+----+-----+--------+-----+-----+------+---------+----------+----+----+------+
|beautifulbear|  1| 01743|  68.688.326.58|13/Jun/2012:10:55...|   10|   55|        45| -0500|   GET|           /products|    0|   /product/product4|Mozilla/4.0 (comp...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  2| 75756|  361.631.17.30|01/Nov/2011:10:53...|   10|   53|        01| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  3| 05901|   11.308.46.48|21/Jun/2012:01:12...|   01|   12|        33| -0500|   GET|               /news|    0|    /partners/resell|Mozilla/5.0 (comp...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  4| 24538|  361.631.17.30|01/Nov/2011:13:48...|   13|   48|        30| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  5| 08116|  322.76.611.36|30/Jun/2012:07:19...|   07|   19|        30| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  6| 04675|  361.631.17.30|01/Nov/2011:16:19...|   16|   19|        00| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  7| 06575| 367.34.686.631|01/Jul/2012:00:02...|   00|   02|        00| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  8| 27382|   88.633.11.47|04/Nov/2011:13:00...|   13|   00|        30| -0500|   GET|        /feeds/press|    0|                   -|Apple-PubSub/65.12.1|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear|  9| 09934|   11.308.46.48|01/Jul/2012:23:13...|   23|   13|        53| -0500|   GET|           /partners|    0|   /product/product2|Mozilla/5.0 (comp...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 10| 01716|683.615.622.618|05/Nov/2011:22:03...|   22|   03|        01| -0500|   GET|    /partners/resell|    0|    /partners/resell|                null|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 11| 55195| 682.62.387.672|02/Jul/2012:10:17...|   10|   17|        48| -0500|   GET|/download/downloa...|    0|/download/downloa...|Mozilla/5.0 (Wind...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 12| 69809|  43.68.686.668|09/Nov/2011:05:25...|   05|   25|        00| -0500|   GET|        /feeds/press|    0|                   -|  Apple-PubSub/65.11|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 13| 78800|321.336.372.320|06/Jul/2012:01:41...|   01|   41|        00| -0500|   GET|        /feeds/press|    0|                   -|Mozilla/5.0 (Wind...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 14| 58994|  361.631.17.30|16/Nov/2011:00:50...|   00|   50|        00| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 15| 94543| 361.638.668.62|13/Jul/2012:23:44...|   23|   44|        30| -0500|   GET|            /ad/save|    0|                   -|Mozilla/5.0 (Twic...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 16| 33695|   325.87.75.36|16/Nov/2011:04:58...|   04|   58|        30| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 17| 01755|  18.332.18.672|16/Jul/2012:03:20...|   03|   20|        30| -0500|   GET|        /feeds/press|    0|                   -|Apple-PubSub/65.12.1|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 18| 01784|   361.321.73.6|17/Nov/2011:13:08...|   13|   08|        30| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 19| 09671|   364.63.61.83|23/Jul/2012:17:00...|   17|   00|        37| -0500|   GET|   /product/product3|    0|      /docs/doc5.pdf|Mozilla/5.0 (X11;...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
|beautifulbear| 20| 04288|    13.53.52.13|26/Nov/2011:19:16...|   19|   16|        00| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|       null|null|   null|null|null| null|  null|null| null|  null|null|null| null|null| null|    null| null| null|  null|     null|      null|null|null|  null|
+-------------+---+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+-----------+----+-------+----+----+-----+------+----+-----+------+----+----+-----+----+-----+--------+-----+-----+------+---------+----------+----+----+------+
only showing top 20 rows





==================  Step 7 b =============available customers=============================================






+-------------+----+------+---------------+--------------------+-----+-----+----------+------+------+-------------------+-----+-----------------+--------------------+----------+-----------+------------------+-------+--------------+---------+--------------------+------+----------+-------+-------------+-----+--------------------+-------+------+-----+--------+--------------+--------------------+--------------------+--------------------+----------+--------+--------------------+--------------------+
|     username|  id|amount|             ip|            createdt|value|score|regioncode|status|method|                key|count|             type|                site|statuscode|nationality|              seed|version|          cell|      dob|               email|gender|      city|  state|       street|  zip|                 md5|  first|  last|title|password|         phone|               large|              medium|           thumbnail|registered|    salt|                sha1|              sha256|
+-------------+----+------+---------------+--------------------+-----+-----+----------+------+------+-------------------+-----+-----------------+--------------------+----------+-----------+------------------+-------+--------------+---------+--------------------+------+----------+-------+-------------+-----+--------------------+-------+------+-----+--------+--------------+--------------------+--------------------+--------------------+----------+--------+--------------------+--------------------+
|beautifulbird|1103| 07257|   325.87.75.36|29/Oct/2011:22:53...|   22|   53|        30| -0500|   GET|              /demo|    0|                -|Jakarta Commons-H...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|
|beautifulbird|1104| 06917|  361.631.17.30|20/Feb/2012:12:02...|   12|   02|        00| -0500|   GET|              /demo|    0|            /demo|Jakarta Commons-H...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|
|beautifulbird|1105| 06647|  325.87.75.336|01/Nov/2011:03:20...|   03|   20|        30| -0500|   GET|              /demo|    0|            /demo|Jakarta Commons-H...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|
|beautifulbird|1106| 13043|  638.45.10.626|22/Feb/2012:12:29...|   12|   29|        54| -0500|   GET|     /events/event5|    0|/product/product2|Mozilla/5.0 (Wind...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|
|beautifulbird|1107| 20972|683.615.622.618|02/Nov/2011:22:46...|   22|   46|        30| -0500|   GET|   /partners/resell|    0| /partners/resell|                null|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|
|beautifulbird|1108| 22362|   325.87.75.36|23/Feb/2012:08:53...|   08|   53|        31| -0500|   GET|              /demo|    0|            /demo|Jakarta Commons-H...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|
|beautifulbird|1109| 01751|  325.87.75.336|07/Nov/2011:03:20...|   03|   20|        30| -0500|   GET|              /demo|    0|            /demo|Jakarta Commons-H...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|
|beautifulbird|1110| 08193| 48.605.338.305|28/Feb/2012:08:39...|   08|   39|        05| -0500|   GET|          /download|    0|        /download|Mozilla/4.0 (comp...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|
|beautifulbird|1111| 09401|   361.321.73.6|07/Nov/2011:04:08...|   04|   08|        00| -0500|   GET|              /demo|    0|            /demo|Jakarta Commons-H...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|
|beautifulbird|1112| 54436| 640.638.335.14|05/Mar/2012:10:17...|   10|   17|        35| -0500|   GET|/news/releases/june|    0|   /docs/doc2.pdf|ia_archiver (+htt...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|
|beautifulbird|1113| 79263|  361.631.17.30|10/Nov/2011:23:54...|   23|   54|        01| -0500|   GET|              /demo|    0|            /demo|Jakarta Commons-H...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|
|beautifulbird|1114| 89767|363.387.621.687|05/Mar/2012:16:14...|   16|   14|        55| -0500|   GET|  /product/product6|    0|/product/product6|Mozilla/5.0 (Wind...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|
|beautifulbird|1115| 79079|320.371.380.628|13/Nov/2011:09:45...|   09|   45|        00| -0500|   GET|  /product/product3|    0|                -|PostRank/2.0 (pos...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|
|beautifulbird|1116| 76155|   608.647.1.38|06/Mar/2012:04:32...|   04|   32|        07| -0500|   GET|              /team|    0|         /contact|Mozilla/4.0 (comp...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|
|beautifulbird|1117| 09369|   361.321.73.6|17/Nov/2011:18:41...|   18|   41|        30| -0500|   GET|              /demo|    0|            /demo|Jakarta Commons-H...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|
|beautifulbird|1118| 06620|    322.74.80.4|08/Mar/2012:02:52...|   02|   52|        37| -0500|   GET|         /highlight|    0|       /highlight|Mozilla/5.0 (X11;...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|
|beautifulbird|1119| 05000|   88.678.03.16|18/Nov/2011:13:15...|   13|   15|        55| -0500|   GET|  /product/product1|    0|/product/product1|Mozilla/5.0 (Wind...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|
|beautifulbird|1120| 08169|  52.334.68.658|08/Mar/2012:11:31...|   11|   31|        50| -0500|   GET|          /download|    0|        /download|Mozilla/4.0 (comp...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|
|beautifulbird|1121| 09315|  326.82.32.367|18/Nov/2011:14:17...|   14|   17|        43| -0500|   GET|  /product/product1|    0|        /download|Mozilla/5.0 (Wind...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|
|beautifulbird|1122| 04873|  14.323.74.653|13/Mar/2012:03:32...|   03|   32|        00| -0500|   GET|              /demo|    0|                -|Jakarta Commons-H...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|
+-------------+----+------+---------------+--------------------+-----+-----+----------+------+------+-------------------+-----+-----------------+--------------------+----------+-----------+------------------+-------+--------------+---------+--------------------+------+----------+-------+-------------+-----+--------------------+-------+------+-----+--------+--------------+--------------------+--------------------+--------------------+----------+--------+--------------------+--------------------+
only showing top 20 rows





=============== Step 8 ================Null handled dataframe=============================================






+-------------+---+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+-------------+-------------+-------------+-------------+---+-------------+-------------+-------------+-------------+-------------+---+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+----------+-------------+-------------+-------------+
|     username| id|amount|             ip|            createdt|value|score|regioncode|status|method|                 key|count|                type|                site|statuscode|  nationality|         seed|      version|         cell|dob|        email|       gender|         city|        state|       street|zip|          md5|        first|         last|        title|     password|        phone|        large|       medium|    thumbnail|registered|         salt|         sha1|       sha256|
+-------------+---+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+-------------+-------------+-------------+-------------+---+-------------+-------------+-------------+-------------+-------------+---+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+----------+-------------+-------------+-------------+
|beautifulbear|  1| 01743|  68.688.326.58|13/Jun/2012:10:55...|   10|   55|        45| -0500|   GET|           /products|    0|   /product/product4|Mozilla/4.0 (comp...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear|  2| 75756|  361.631.17.30|01/Nov/2011:10:53...|   10|   53|        01| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear|  3| 05901|   11.308.46.48|21/Jun/2012:01:12...|   01|   12|        33| -0500|   GET|               /news|    0|    /partners/resell|Mozilla/5.0 (comp...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear|  4| 24538|  361.631.17.30|01/Nov/2011:13:48...|   13|   48|        30| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear|  5| 08116|  322.76.611.36|30/Jun/2012:07:19...|   07|   19|        30| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear|  6| 04675|  361.631.17.30|01/Nov/2011:16:19...|   16|   19|        00| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear|  7| 06575| 367.34.686.631|01/Jul/2012:00:02...|   00|   02|        00| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear|  8| 27382|   88.633.11.47|04/Nov/2011:13:00...|   13|   00|        30| -0500|   GET|        /feeds/press|    0|                   -|Apple-PubSub/65.12.1|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear|  9| 09934|   11.308.46.48|01/Jul/2012:23:13...|   23|   13|        53| -0500|   GET|           /partners|    0|   /product/product2|Mozilla/5.0 (comp...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear| 10| 01716|683.615.622.618|05/Nov/2011:22:03...|   22|   03|        01| -0500|   GET|    /partners/resell|    0|    /partners/resell|                null|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear| 11| 55195| 682.62.387.672|02/Jul/2012:10:17...|   10|   17|        48| -0500|   GET|/download/downloa...|    0|/download/downloa...|Mozilla/5.0 (Wind...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear| 12| 69809|  43.68.686.668|09/Nov/2011:05:25...|   05|   25|        00| -0500|   GET|        /feeds/press|    0|                   -|  Apple-PubSub/65.11|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear| 13| 78800|321.336.372.320|06/Jul/2012:01:41...|   01|   41|        00| -0500|   GET|        /feeds/press|    0|                   -|Mozilla/5.0 (Wind...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear| 14| 58994|  361.631.17.30|16/Nov/2011:00:50...|   00|   50|        00| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear| 15| 94543| 361.638.668.62|13/Jul/2012:23:44...|   23|   44|        30| -0500|   GET|            /ad/save|    0|                   -|Mozilla/5.0 (Twic...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear| 16| 33695|   325.87.75.36|16/Nov/2011:04:58...|   04|   58|        30| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear| 17| 01755|  18.332.18.672|16/Jul/2012:03:20...|   03|   20|        30| -0500|   GET|        /feeds/press|    0|                   -|Apple-PubSub/65.12.1|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear| 18| 01784|   361.321.73.6|17/Nov/2011:13:08...|   13|   08|        30| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear| 19| 09671|   364.63.61.83|23/Jul/2012:17:00...|   17|   00|        37| -0500|   GET|   /product/product3|    0|      /docs/doc5.pdf|Mozilla/5.0 (X11;...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
|beautifulbear| 20| 04288|    13.53.52.13|26/Nov/2011:19:16...|   19|   16|        00| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|
+-------------+---+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+-------------+-------------+-------------+-------------+---+-------------+-------------+-------------+-------------+-------------+---+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+----------+-------------+-------------+-------------+
only showing top 20 rows





=============== Step 9 a ================not available customers with current date dataframe=============================================






+-------------+---+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+-------------+-------------+-------------+-------------+---+-------------+-------------+-------------+-------------+-------------+---+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+----------+-------------+-------------+-------------+------------+
|     username| id|amount|             ip|            createdt|value|score|regioncode|status|method|                 key|count|                type|                site|statuscode|  nationality|         seed|      version|         cell|dob|        email|       gender|         city|        state|       street|zip|          md5|        first|         last|        title|     password|        phone|        large|       medium|    thumbnail|registered|         salt|         sha1|       sha256|current_date|
+-------------+---+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+-------------+-------------+-------------+-------------+---+-------------+-------------+-------------+-------------+-------------+---+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+----------+-------------+-------------+-------------+------------+
|beautifulbear|  1| 01743|  68.688.326.58|13/Jun/2012:10:55...|   10|   55|        45| -0500|   GET|           /products|    0|   /product/product4|Mozilla/4.0 (comp...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear|  2| 75756|  361.631.17.30|01/Nov/2011:10:53...|   10|   53|        01| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear|  3| 05901|   11.308.46.48|21/Jun/2012:01:12...|   01|   12|        33| -0500|   GET|               /news|    0|    /partners/resell|Mozilla/5.0 (comp...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear|  4| 24538|  361.631.17.30|01/Nov/2011:13:48...|   13|   48|        30| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear|  5| 08116|  322.76.611.36|30/Jun/2012:07:19...|   07|   19|        30| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear|  6| 04675|  361.631.17.30|01/Nov/2011:16:19...|   16|   19|        00| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear|  7| 06575| 367.34.686.631|01/Jul/2012:00:02...|   00|   02|        00| -0500|   GET|               /demo|    0|                   -|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear|  8| 27382|   88.633.11.47|04/Nov/2011:13:00...|   13|   00|        30| -0500|   GET|        /feeds/press|    0|                   -|Apple-PubSub/65.12.1|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear|  9| 09934|   11.308.46.48|01/Jul/2012:23:13...|   23|   13|        53| -0500|   GET|           /partners|    0|   /product/product2|Mozilla/5.0 (comp...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear| 10| 01716|683.615.622.618|05/Nov/2011:22:03...|   22|   03|        01| -0500|   GET|    /partners/resell|    0|    /partners/resell|                null|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear| 11| 55195| 682.62.387.672|02/Jul/2012:10:17...|   10|   17|        48| -0500|   GET|/download/downloa...|    0|/download/downloa...|Mozilla/5.0 (Wind...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear| 12| 69809|  43.68.686.668|09/Nov/2011:05:25...|   05|   25|        00| -0500|   GET|        /feeds/press|    0|                   -|  Apple-PubSub/65.11|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear| 13| 78800|321.336.372.320|06/Jul/2012:01:41...|   01|   41|        00| -0500|   GET|        /feeds/press|    0|                   -|Mozilla/5.0 (Wind...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear| 14| 58994|  361.631.17.30|16/Nov/2011:00:50...|   00|   50|        00| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear| 15| 94543| 361.638.668.62|13/Jul/2012:23:44...|   23|   44|        30| -0500|   GET|            /ad/save|    0|                   -|Mozilla/5.0 (Twic...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear| 16| 33695|   325.87.75.36|16/Nov/2011:04:58...|   04|   58|        30| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear| 17| 01755|  18.332.18.672|16/Jul/2012:03:20...|   03|   20|        30| -0500|   GET|        /feeds/press|    0|                   -|Apple-PubSub/65.12.1|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear| 18| 01784|   361.321.73.6|17/Nov/2011:13:08...|   13|   08|        30| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear| 19| 09671|   364.63.61.83|23/Jul/2012:17:00...|   17|   00|        37| -0500|   GET|   /product/product3|    0|      /docs/doc5.pdf|Mozilla/5.0 (X11;...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
|beautifulbear| 20| 04288|    13.53.52.13|26/Nov/2011:19:16...|   19|   16|        00| -0500|   GET|               /demo|    0|               /demo|Jakarta Commons-H...|       100|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|  0|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|Not Available|         0|Not Available|Not Available|Not Available|  2023-01-01|
+-------------+---+------+---------------+--------------------+-----+-----+----------+------+------+--------------------+-----+--------------------+--------------------+----------+-------------+-------------+-------------+-------------+---+-------------+-------------+-------------+-------------+-------------+---+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+----------+-------------+-------------+-------------+------------+
only showing top 20 rows








=============== Step 9 b ================available customers with current date dataframe=============================================






+-------------+----+------+---------------+--------------------+-----+-----+----------+------+------+-------------------+-----+-----------------+--------------------+----------+-----------+------------------+-------+--------------+---------+--------------------+------+----------+-------+-------------+-----+--------------------+-------+------+-----+--------+--------------+--------------------+--------------------+--------------------+----------+--------+--------------------+--------------------+------------+
|     username|  id|amount|             ip|            createdt|value|score|regioncode|status|method|                key|count|             type|                site|statuscode|nationality|              seed|version|          cell|      dob|               email|gender|      city|  state|       street|  zip|                 md5|  first|  last|title|password|         phone|               large|              medium|           thumbnail|registered|    salt|                sha1|              sha256|current_date|
+-------------+----+------+---------------+--------------------+-----+-----+----------+------+------+-------------------+-----+-----------------+--------------------+----------+-----------+------------------+-------+--------------+---------+--------------------+------+----------+-------+-------------+-----+--------------------+-------+------+-----+--------+--------------+--------------------+--------------------+--------------------+----------+--------+--------------------+--------------------+------------+
|beautifulbird|1103| 07257|   325.87.75.36|29/Oct/2011:22:53...|   22|   53|        30| -0500|   GET|              /demo|    0|                -|Jakarta Commons-H...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|  2023-01-01|
|beautifulbird|1104| 06917|  361.631.17.30|20/Feb/2012:12:02...|   12|   02|        00| -0500|   GET|              /demo|    0|            /demo|Jakarta Commons-H...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|  2023-01-01|
|beautifulbird|1105| 06647|  325.87.75.336|01/Nov/2011:03:20...|   03|   20|        30| -0500|   GET|              /demo|    0|            /demo|Jakarta Commons-H...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|  2023-01-01|
|beautifulbird|1106| 13043|  638.45.10.626|22/Feb/2012:12:29...|   12|   29|        54| -0500|   GET|     /events/event5|    0|/product/product2|Mozilla/5.0 (Wind...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|  2023-01-01|
|beautifulbird|1107| 20972|683.615.622.618|02/Nov/2011:22:46...|   22|   46|        30| -0500|   GET|   /partners/resell|    0| /partners/resell|                null|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|  2023-01-01|
|beautifulbird|1108| 22362|   325.87.75.36|23/Feb/2012:08:53...|   08|   53|        31| -0500|   GET|              /demo|    0|            /demo|Jakarta Commons-H...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|  2023-01-01|
|beautifulbird|1109| 01751|  325.87.75.336|07/Nov/2011:03:20...|   03|   20|        30| -0500|   GET|              /demo|    0|            /demo|Jakarta Commons-H...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|  2023-01-01|
|beautifulbird|1110| 08193| 48.605.338.305|28/Feb/2012:08:39...|   08|   39|        05| -0500|   GET|          /download|    0|        /download|Mozilla/4.0 (comp...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|  2023-01-01|
|beautifulbird|1111| 09401|   361.321.73.6|07/Nov/2011:04:08...|   04|   08|        00| -0500|   GET|              /demo|    0|            /demo|Jakarta Commons-H...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|  2023-01-01|
|beautifulbird|1112| 54436| 640.638.335.14|05/Mar/2012:10:17...|   10|   17|        35| -0500|   GET|/news/releases/june|    0|   /docs/doc2.pdf|ia_archiver (+htt...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|  2023-01-01|
|beautifulbird|1113| 79263|  361.631.17.30|10/Nov/2011:23:54...|   23|   54|        01| -0500|   GET|              /demo|    0|            /demo|Jakarta Commons-H...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|  2023-01-01|
|beautifulbird|1114| 89767|363.387.621.687|05/Mar/2012:16:14...|   16|   14|        55| -0500|   GET|  /product/product6|    0|/product/product6|Mozilla/5.0 (Wind...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|  2023-01-01|
|beautifulbird|1115| 79079|320.371.380.628|13/Nov/2011:09:45...|   09|   45|        00| -0500|   GET|  /product/product3|    0|                -|PostRank/2.0 (pos...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|  2023-01-01|
|beautifulbird|1116| 76155|   608.647.1.38|06/Mar/2012:04:32...|   04|   32|        07| -0500|   GET|              /team|    0|         /contact|Mozilla/4.0 (comp...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|  2023-01-01|
|beautifulbird|1117| 09369|   361.321.73.6|17/Nov/2011:18:41...|   18|   41|        30| -0500|   GET|              /demo|    0|            /demo|Jakarta Commons-H...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|  2023-01-01|
|beautifulbird|1118| 06620|    322.74.80.4|08/Mar/2012:02:52...|   02|   52|        37| -0500|   GET|         /highlight|    0|       /highlight|Mozilla/5.0 (X11;...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|  2023-01-01|
|beautifulbird|1119| 05000|   88.678.03.16|18/Nov/2011:13:15...|   13|   15|        55| -0500|   GET|  /product/product1|    0|/product/product1|Mozilla/5.0 (Wind...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|  2023-01-01|
|beautifulbird|1120| 08169|  52.334.68.658|08/Mar/2012:11:31...|   11|   31|        50| -0500|   GET|          /download|    0|        /download|Mozilla/4.0 (comp...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|  2023-01-01|
|beautifulbird|1121| 09315|  326.82.32.367|18/Nov/2011:14:17...|   14|   17|        43| -0500|   GET|  /product/product1|    0|        /download|Mozilla/5.0 (Wind...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|  2023-01-01|
|beautifulbird|1122| 04873|  14.323.74.653|13/Mar/2012:03:32...|   03|   32|        00| -0500|   GET|              /demo|    0|                -|Jakarta Commons-H...|       100|         US|c147a1dc8e2ced1c0d|    0.8|(772)-838-1943|818595248|colleen.fuller@ex...|female|richardson|alabama|6881 adams st|79978|433c24594b664f067...|colleen|fuller|   ms| slacker|(695)-621-0239|https://randomuse...|https://randomuse...|https://randomuse...|1257757045|v3RaFTpC|7e3e976d9815d7379...|d0802499c67d5005f...|  2023-01-01|
+-------------+----+------+---------------+--------------------+-----+-----+----------+------+------+-------------------+-----+-----------------+--------------------+----------+-----------+------------------+-------+--------------+---------+--------------------+------+----------+-------+-------------+-----+--------------------+-------+------+-----+--------+--------------+--------------------+--------------------+--------------------+----------+--------+--------------------+--------------------+------------+
only showing top 20 rows

====================================================================================
	
=====================
SQL SCENARIOS:
--------------
package pack 
import org.apache.spark.SparkContext // rdd
import org.apache.spark.sql.SparkSession // dataframe
import org.apache.spark.SparkConf 
import org.apache.spark.sql._ 
import org.apache.spark.sql.types._ 
import org.apache.spark.sql.types.IntegerType 
import org.apache.spark.sql.functions.upper
import org.apache.spark.sql.catalyst.expressions.Upper
import org.apache.spark.sql.functions._ 
import org.apache.spark.sql.expressions.Window 
import scala.io.Source 
object obj { 
      def main(args:Array[String]):Unit={ 
     val conf = new SparkConf().setAppName("Revision").setMaster("local[*]") 
     
     val sc = new SparkContext(conf) 
         sc.setLogLevel("ERROR") 
     
     val spark = SparkSession.builder().getOrCreate() 
     
     import spark.implicits._
     
     val df = spark.read.option("header","true").csv("file:///D:/data/sql/df.csv") 
     df.show() 
     
     val df1 = spark.read.option("header","true").csv("file:///D:/data/sql/df1.csv") 
     
     df1.show()  
      
     val cust = spark.read.option("header","true").csv("file:///D:/data/sql/cust.csv") 

     cust.show() 
 
     val prod = spark.read.option("header","true").csv("file:///D:/data/sql/prod.csv") 

     prod.show() 
 
     df.createOrReplaceTempView("df") 

     df1.createOrReplaceTempView("df1") 

     cust.createOrReplaceTempView("cust") 

     prod.createOrReplaceTempView("prod") 

     sc.setLogLevel("ERROR") 

     spark.sql("select * from df order by id").show()  
      
     
     spark.sql("select * from df1 order by id").show() 
     
     //Validate data 
     
     spark.sql("select * from df ").show() 
     
     //Select two columns
     
     spark.sql("select id,tdate from df order by id").show() 
     
     //Select column with category filter

     spark.sql("select id,tdate,category from df where category='Exercise' order by id").show() 
     
     //Multi Column filter

     spark.sql("select id,tdate,category,spendby from df where category='Exercise' and spendby='cash' ").show() 
     
     //Multi Value Filter 

     spark.sql("select * from df where category in ('Exercise','Gymnastics')").show() 
     
     //Like Filter

     println("Like Filter") 

     spark.sql("select * from df where product like ('%Gymnastics%')").show() 
     
     //Not Filters

     println("Not Filters") 

     spark.sql("select * from df where category != 'Exercise'").show() 
     
     //Not In Filters

     println("Not In Filters") 

     spark.sql("select * from df where category not in ('Exercise','Gymnastics')").show() 

     //Null Filters

     println("Null Filters") 

     spark.sql("select * from df where product is null").show() 
     
     //Max Function

     println("Max Function") 

     spark.sql("select max(id) from df ").show() 
     
     //Min Funtion

     println("Min Function") 

     spark.sql("select min(id) from df ").show() 
     
     //Count 

     println("Count") 

     spark.sql("select count(1) from df ").show() 

     //Condition statement 

     println("Condition statement") 

     spark.sql("select *,case when spendby='cash' then 1 else 0 end as status from df").show() 
     
     //Concat data

     println("Concat data") 

     spark.sql("select concat(id,'-',category) as concat from df ").show() 
     
     //Concat_ws data

     println("Concat_ws data") 

     spark.sql("select concat_ws('-',id,category,product) as concat from df ").show() 
     
     //Lower Case data

     println("Lower Case data") 

     spark.sql("select category,lower(category) as lower from df ").show() 
     
     //Ceil data

     println("Ceil data") 

     spark.sql("select amount,ceil(amount) from df").show() 

     //Round the data

     println("Round the data") 

     spark.sql("select amount,round(amount) from df").show() 
     
     //Replace Nulls

     println("Replace Nulls") 

     spark.sql("select coalesce(product,'NA') from df").show() 
     
     //Trim the space

     println("Trim the space") 

     spark.sql("select trim(product) from df").show() 
     
     //Distinct the columns

     println("Distinct the columns") 

     spark.sql("select distinct category,spendby from df").show() 
     
     //Substring with Trim

     println("Substring with Trim") 

     spark.sql("select substring(trim(product),1,10) from df").show() 
     
     //Substring/Split operation

     println("Substring/Split operation") 

     spark.sql("select SUBSTRING_INDEX(category,' ',1) as spl from df").show() 
     
     //Union all 

     println("Union all") 

     spark.sql("select * from df union all select * from df1").show() 
     
     //Union

     println("Union") 

     spark.sql("select * from df union select * from df1 order by id").show() 
     
     //Aggregate Sum

     println("Aggregate Sum") 

     spark.sql("select category, sum(amount) as total from df group by category").show() 
     
     //Aggregate sum with two columns

     println("Aggregate sum with two column") 

     spark.sql("select category,spendby,sum(amount) as total from df group by category,spendby").show() 

     //Aggregate Count

     println("Aggregate Count") 

     spark.sql("select category,spendby,sum(amount) As total,count(amount) as cnt from df group by category,spendby").show() 

     //Aggregate Max

     println("Aggregate Max") 

     spark.sql("select category, max(amount) as max from df group by category").show() 
     
     //Aggregate with Order ascending

     println("Aggregate with Order ascending") 

     spark.sql("select category, max(amount) as max from df group by category order by category").show() 
     
     //Aggregate with Order Descending

     println("Aggregate with Order Descending") 

     spark.sql("select category, max(amount) as max from df group by category order by category desc").show() 

     //Window Row Number

     println("Window Row Number") 

     spark.sql("SELECT category,amount, row_number() OVER ( partition by category order by amount desc ) AS row_number FROM df").show() 
     
     //Window Dense_rank Number

     println("Window Dense_rank Number") 

     spark.sql("SELECT category,amount, dense_rank() OVER ( partition by category order by amount desc ) AS dense_rank FROM df").show() 
     
     //Window rank Number

     println("Window rank Number") 

     spark.sql("SELECT category,amount, rank() OVER ( partition by category order by amount desc ) AS rank FROM df").show() 
     
     //Window Lead function

     println("Window Lead function") 

     spark.sql("SELECT category,amount, lead(amount) OVER ( partition by category order by amount desc ) AS lead FROM df").show() 
     
     //Window lag function

     println("Window lag function") 

     spark.sql("SELECT category,amount, lag(amount) OVER ( partition by category order by amount desc ) AS lag FROM df").show() 
     
     //Having function

     println("Having function") 

     spark.sql("select category,count(category) as cnt from df group by category having count(category)>1").show() 

     //Inner Join

     println("Inner Join") 

     spark.sql("select a.id,a.name,b.product from cust a join prod b on a.id=b.id").show() 
     
     //Left Join

     println("Left Join") 

     spark.sql("select a.id,a.name,b.product from cust a left join prod b on a.id=b.id").show() 
     
     //Right Join

     println("Right Join") 

     spark.sql("select a.id,a.name,b.product from cust a right join prod b on a.id=b.id").show() 
     
     //Full Join

     println("Full Join") 

     spark.sql("select a.id,a.name,b.product from cust a full join prod b on a.id=b.id").show() 
     
     //left anti Join

     println("left anti Join") 

     spark.sql("select a.id,a.name from cust a LEFT ANTI JOIN prod b on a.id=b.id").show() 
     
     //left semi Join

     println("left semi Join") 

     spark.sql("select a.id,a.name from cust a LEFT SEMI JOIN prod b on a.id=b.id").show() 
     
     //Date format

     println("Date format") 

     spark.sql("select id,tdate,from_unixtime(unix_timestamp(tdate,'MM-dd-yyyy'),'yyyy-MM-dd') as con_date from df").show() 
     
     //Sub query

     println("Sub query") 

     spark.sql("""

     select sum(amount) as total , con_date from(select 

     id,tdate,from_unixtime(unix_timestamp(tdate,'MM-dd-yyyy'),'yyyy-MM-dd') as 
     
     con_date,amount,category,product,spendby from df) 
 
     group by con_date
     
     """).show() 
     
     //collect_list

     println("collect_list") 

     spark.sql("select category,collect_list(spendby) as col_spend from df group by category").show() 
     
     //Explode

     println("Explode") 

     spark.sql("select category,explode(col_spend) as ex_spend from (select category,collect_set(spendby) as col_spend from df group by category)").show() 
     
     //explode_outer

     println("explode_outer") 

     spark.sql("select category,explode_outer(col_spend) as ex_spend from (select category,collect_set(spendby) as col_spend from df group by category)").show() 
     

     
      }
    
}

=========================================================================

===================
Out put:
--------
+---+----------+------+-------------+--------------+-------+
| id|     tdate|amount|     category|       product|spendby|
+---+----------+------+-------------+--------------+-------+
|  0|06-26-2011| 300.4|     Exercise| GymnasticsPro|   cash|
|  1|05-26-2011| 200.0|Exercise Band| Weightlifting| credit|
|  2|06-01-2011| 300.4|     Exercise|Gymnastics Pro|   cash|
|  3|06-05-2011| 100.0|   Gymnastics|         Rings| credit|
|  4|12-17-2011| 300.0|  Team Sports|         Field|   cash|
|  5|02-14-2011| 200.0|   Gymnastics|          null|   cash|
|  6|06-05-2011| 100.0|     Exercise|         Rings| credit|
|  7|12-17-2011| 300.0|  Team Sports|         Field|   cash|
|  8|02-14-2011| 200.0|   Gymnastics|          null|   cash|
+---+----------+------+-------------+--------------+-------+

+---+----------+------+-----------+-------+-------+
| id|     tdate|amount|   category|product|spendby|
+---+----------+------+-----------+-------+-------+
|  4|12-17-2011| 300.0|Team Sports|  Field|   cash|
|  5|02-14-2011| 200.0| Gymnastics|   null|   cash|
|  6|02-14-2011| 200.0|     Winter|   null|   cash|
|  7|02-14-2011| 200.0|     Winter|   null|   cash|
+---+----------+------+-----------+-------+-------+

+---+----+
| id|name|
+---+----+
|  1| raj|
|  2|ravi|
|  3| sai|
|  5|rani|
+---+----+

+---+-------+
| id|product|
+---+-------+
|  1|    raj|
|  3| mobile|
|  7| laptop|
+---+-------+

+---+----------+------+-------------+--------------+-------+
| id|     tdate|amount|     category|       product|spendby|
+---+----------+------+-------------+--------------+-------+
|  0|06-26-2011| 300.4|     Exercise| GymnasticsPro|   cash|
|  1|05-26-2011| 200.0|Exercise Band| Weightlifting| credit|
|  2|06-01-2011| 300.4|     Exercise|Gymnastics Pro|   cash|
|  3|06-05-2011| 100.0|   Gymnastics|         Rings| credit|
|  4|12-17-2011| 300.0|  Team Sports|         Field|   cash|
|  5|02-14-2011| 200.0|   Gymnastics|          null|   cash|
|  6|06-05-2011| 100.0|     Exercise|         Rings| credit|
|  7|12-17-2011| 300.0|  Team Sports|         Field|   cash|
|  8|02-14-2011| 200.0|   Gymnastics|          null|   cash|
+---+----------+------+-------------+--------------+-------+

+---+----------+------+-----------+-------+-------+
| id|     tdate|amount|   category|product|spendby|
+---+----------+------+-----------+-------+-------+
|  4|12-17-2011| 300.0|Team Sports|  Field|   cash|
|  5|02-14-2011| 200.0| Gymnastics|   null|   cash|
|  6|02-14-2011| 200.0|     Winter|   null|   cash|
|  7|02-14-2011| 200.0|     Winter|   null|   cash|
+---+----------+------+-----------+-------+-------+

+---+----------+------+-------------+--------------+-------+
| id|     tdate|amount|     category|       product|spendby|
+---+----------+------+-------------+--------------+-------+
|  0|06-26-2011| 300.4|     Exercise| GymnasticsPro|   cash|
|  1|05-26-2011| 200.0|Exercise Band| Weightlifting| credit|
|  2|06-01-2011| 300.4|     Exercise|Gymnastics Pro|   cash|
|  3|06-05-2011| 100.0|   Gymnastics|         Rings| credit|
|  4|12-17-2011| 300.0|  Team Sports|         Field|   cash|
|  5|02-14-2011| 200.0|   Gymnastics|          null|   cash|
|  6|06-05-2011| 100.0|     Exercise|         Rings| credit|
|  7|12-17-2011| 300.0|  Team Sports|         Field|   cash|
|  8|02-14-2011| 200.0|   Gymnastics|          null|   cash|
+---+----------+------+-------------+--------------+-------+

+---+----------+
| id|     tdate|
+---+----------+
|  0|06-26-2011|
|  1|05-26-2011|
|  2|06-01-2011|
|  3|06-05-2011|
|  4|12-17-2011|
|  5|02-14-2011|
|  6|06-05-2011|
|  7|12-17-2011|
|  8|02-14-2011|
+---+----------+

+---+----------+--------+
| id|     tdate|category|
+---+----------+--------+
|  0|06-26-2011|Exercise|
|  2|06-01-2011|Exercise|
|  6|06-05-2011|Exercise|
+---+----------+--------+

+---+----------+--------+-------+
| id|     tdate|category|spendby|
+---+----------+--------+-------+
|  0|06-26-2011|Exercise|   cash|
|  2|06-01-2011|Exercise|   cash|
+---+----------+--------+-------+

+---+----------+------+----------+--------------+-------+
| id|     tdate|amount|  category|       product|spendby|
+---+----------+------+----------+--------------+-------+
|  0|06-26-2011| 300.4|  Exercise| GymnasticsPro|   cash|
|  2|06-01-2011| 300.4|  Exercise|Gymnastics Pro|   cash|
|  3|06-05-2011| 100.0|Gymnastics|         Rings| credit|
|  5|02-14-2011| 200.0|Gymnastics|          null|   cash|
|  6|06-05-2011| 100.0|  Exercise|         Rings| credit|
|  8|02-14-2011| 200.0|Gymnastics|          null|   cash|
+---+----------+------+----------+--------------+-------+

Like Filter
+---+----------+------+--------+--------------+-------+
| id|     tdate|amount|category|       product|spendby|
+---+----------+------+--------+--------------+-------+
|  0|06-26-2011| 300.4|Exercise| GymnasticsPro|   cash|
|  2|06-01-2011| 300.4|Exercise|Gymnastics Pro|   cash|
+---+----------+------+--------+--------------+-------+

Not Filters
+---+----------+------+-------------+-------------+-------+
| id|     tdate|amount|     category|      product|spendby|
+---+----------+------+-------------+-------------+-------+
|  1|05-26-2011| 200.0|Exercise Band|Weightlifting| credit|
|  3|06-05-2011| 100.0|   Gymnastics|        Rings| credit|
|  4|12-17-2011| 300.0|  Team Sports|        Field|   cash|
|  5|02-14-2011| 200.0|   Gymnastics|         null|   cash|
|  7|12-17-2011| 300.0|  Team Sports|        Field|   cash|
|  8|02-14-2011| 200.0|   Gymnastics|         null|   cash|
+---+----------+------+-------------+-------------+-------+

Not In Filters
+---+----------+------+-------------+-------------+-------+
| id|     tdate|amount|     category|      product|spendby|
+---+----------+------+-------------+-------------+-------+
|  1|05-26-2011| 200.0|Exercise Band|Weightlifting| credit|
|  4|12-17-2011| 300.0|  Team Sports|        Field|   cash|
|  7|12-17-2011| 300.0|  Team Sports|        Field|   cash|
+---+----------+------+-------------+-------------+-------+

Null Filters
+---+----------+------+----------+-------+-------+
| id|     tdate|amount|  category|product|spendby|
+---+----------+------+----------+-------+-------+
|  5|02-14-2011| 200.0|Gymnastics|   null|   cash|
|  8|02-14-2011| 200.0|Gymnastics|   null|   cash|
+---+----------+------+----------+-------+-------+

Max Function
+-------+
|max(id)|
+-------+
|      8|
+-------+

Min Function
+-------+
|min(id)|
+-------+
|      0|
+-------+

Count
+--------+
|count(1)|
+--------+
|       9|
+--------+

Condition statement
+---+----------+------+-------------+--------------+-------+------+
| id|     tdate|amount|     category|       product|spendby|status|
+---+----------+------+-------------+--------------+-------+------+
|  0|06-26-2011| 300.4|     Exercise| GymnasticsPro|   cash|     1|
|  1|05-26-2011| 200.0|Exercise Band| Weightlifting| credit|     0|
|  2|06-01-2011| 300.4|     Exercise|Gymnastics Pro|   cash|     1|
|  3|06-05-2011| 100.0|   Gymnastics|         Rings| credit|     0|
|  4|12-17-2011| 300.0|  Team Sports|         Field|   cash|     1|
|  5|02-14-2011| 200.0|   Gymnastics|          null|   cash|     1|
|  6|06-05-2011| 100.0|     Exercise|         Rings| credit|     0|
|  7|12-17-2011| 300.0|  Team Sports|         Field|   cash|     1|
|  8|02-14-2011| 200.0|   Gymnastics|          null|   cash|     1|
+---+----------+------+-------------+--------------+-------+------+

Concat data
+---------------+
|         concat|
+---------------+
|     0-Exercise|
|1-Exercise Band|
|     2-Exercise|
|   3-Gymnastics|
|  4-Team Sports|
|   5-Gymnastics|
|     6-Exercise|
|  7-Team Sports|
|   8-Gymnastics|
+---------------+

Concat_ws data
+--------------------+
|              concat|
+--------------------+
|0-Exercise-Gymnas...|
|1-Exercise Band-W...|
|2-Exercise-Gymnas...|
|  3-Gymnastics-Rings|
| 4-Team Sports-Field|
|        5-Gymnastics|
|    6-Exercise-Rings|
| 7-Team Sports-Field|
|        8-Gymnastics|
+--------------------+

Lower Case data
+-------------+-------------+
|     category|        lower|
+-------------+-------------+
|     Exercise|     exercise|
|Exercise Band|exercise band|
|     Exercise|     exercise|
|   Gymnastics|   gymnastics|
|  Team Sports|  team sports|
|   Gymnastics|   gymnastics|
|     Exercise|     exercise|
|  Team Sports|  team sports|
|   Gymnastics|   gymnastics|
+-------------+-------------+

Ceil data
+------+----------------------------+
|amount|CEIL(CAST(amount AS DOUBLE))|
+------+----------------------------+
| 300.4|                         301|
| 200.0|                         200|
| 300.4|                         301|
| 100.0|                         100|
| 300.0|                         300|
| 200.0|                         200|
| 100.0|                         100|
| 300.0|                         300|
| 200.0|                         200|
+------+----------------------------+

Round the data
+------+--------------------------------+
|amount|round(CAST(amount AS DOUBLE), 0)|
+------+--------------------------------+
| 300.4|                           300.0|
| 200.0|                           200.0|
| 300.4|                           300.0|
| 100.0|                           100.0|
| 300.0|                           300.0|
| 200.0|                           200.0|
| 100.0|                           100.0|
| 300.0|                           300.0|
| 200.0|                           200.0|
+------+--------------------------------+

Replace Nulls
+---------------------+
|coalesce(product, NA)|
+---------------------+
|        GymnasticsPro|
|        Weightlifting|
|       Gymnastics Pro|
|                Rings|
|                Field|
|                   NA|
|                Rings|
|                Field|
|                   NA|
+---------------------+

Trim the space
+--------------+
| trim(product)|
+--------------+
| GymnasticsPro|
| Weightlifting|
|Gymnastics Pro|
|         Rings|
|         Field|
|          null|
|         Rings|
|         Field|
|          null|
+--------------+

Distinct the columns
+-------------+-------+
|     category|spendby|
+-------------+-------+
|   Gymnastics|   cash|
|     Exercise|   cash|
|     Exercise| credit|
|  Team Sports|   cash|
|Exercise Band| credit|
|   Gymnastics| credit|
+-------------+-------+

Substring with Trim
+-------------------------------+
|substring(trim(product), 1, 10)|
+-------------------------------+
|                     Gymnastics|
|                     Weightlift|
|                     Gymnastics|
|                          Rings|
|                          Field|
|                           null|
|                          Rings|
|                          Field|
|                           null|
+-------------------------------+

Substring/Split operation
+----------+
|       spl|
+----------+
|  Exercise|
|  Exercise|
|  Exercise|
|Gymnastics|
|      Team|
|Gymnastics|
|  Exercise|
|      Team|
|Gymnastics|
+----------+

Union all
+---+----------+------+-------------+--------------+-------+
| id|     tdate|amount|     category|       product|spendby|
+---+----------+------+-------------+--------------+-------+
|  0|06-26-2011| 300.4|     Exercise| GymnasticsPro|   cash|
|  1|05-26-2011| 200.0|Exercise Band| Weightlifting| credit|
|  2|06-01-2011| 300.4|     Exercise|Gymnastics Pro|   cash|
|  3|06-05-2011| 100.0|   Gymnastics|         Rings| credit|
|  4|12-17-2011| 300.0|  Team Sports|         Field|   cash|
|  5|02-14-2011| 200.0|   Gymnastics|          null|   cash|
|  6|06-05-2011| 100.0|     Exercise|         Rings| credit|
|  7|12-17-2011| 300.0|  Team Sports|         Field|   cash|
|  8|02-14-2011| 200.0|   Gymnastics|          null|   cash|
|  4|12-17-2011| 300.0|  Team Sports|         Field|   cash|
|  5|02-14-2011| 200.0|   Gymnastics|          null|   cash|
|  6|02-14-2011| 200.0|       Winter|          null|   cash|
|  7|02-14-2011| 200.0|       Winter|          null|   cash|
+---+----------+------+-------------+--------------+-------+

Union
+---+----------+------+-------------+--------------+-------+
| id|     tdate|amount|     category|       product|spendby|
+---+----------+------+-------------+--------------+-------+
|  0|06-26-2011| 300.4|     Exercise| GymnasticsPro|   cash|
|  1|05-26-2011| 200.0|Exercise Band| Weightlifting| credit|
|  2|06-01-2011| 300.4|     Exercise|Gymnastics Pro|   cash|
|  3|06-05-2011| 100.0|   Gymnastics|         Rings| credit|
|  4|12-17-2011| 300.0|  Team Sports|         Field|   cash|
|  5|02-14-2011| 200.0|   Gymnastics|          null|   cash|
|  6|06-05-2011| 100.0|     Exercise|         Rings| credit|
|  6|02-14-2011| 200.0|       Winter|          null|   cash|
|  7|12-17-2011| 300.0|  Team Sports|         Field|   cash|
|  7|02-14-2011| 200.0|       Winter|          null|   cash|
|  8|02-14-2011| 200.0|   Gymnastics|          null|   cash|
+---+----------+------+-------------+--------------+-------+

Aggregate Sum
+-------------+-----+
|     category|total|
+-------------+-----+
|   Gymnastics|500.0|
|  Team Sports|600.0|
|     Exercise|700.8|
|Exercise Band|200.0|
+-------------+-----+

Aggregate sum with two column
+-------------+-------+-----+
|     category|spendby|total|
+-------------+-------+-----+
|   Gymnastics|   cash|400.0|
|     Exercise|   cash|600.8|
|     Exercise| credit|100.0|
|  Team Sports|   cash|600.0|
|Exercise Band| credit|200.0|
|   Gymnastics| credit|100.0|
+-------------+-------+-----+

Aggregate Count
+-------------+-------+-----+---+
|     category|spendby|total|cnt|
+-------------+-------+-----+---+
|   Gymnastics|   cash|400.0|  2|
|     Exercise|   cash|600.8|  2|
|     Exercise| credit|100.0|  1|
|  Team Sports|   cash|600.0|  2|
|Exercise Band| credit|200.0|  1|
|   Gymnastics| credit|100.0|  1|
+-------------+-------+-----+---+

Aggregate Max
+-------------+-----+
|     category|  max|
+-------------+-----+
|   Gymnastics|200.0|
|  Team Sports|300.0|
|     Exercise|300.4|
|Exercise Band|200.0|
+-------------+-----+

Aggregate with Order ascending
+-------------+-----+
|     category|  max|
+-------------+-----+
|     Exercise|300.4|
|Exercise Band|200.0|
|   Gymnastics|200.0|
|  Team Sports|300.0|
+-------------+-----+

Aggregate with Order Descending
+-------------+-----+
|     category|  max|
+-------------+-----+
|  Team Sports|300.0|
|   Gymnastics|200.0|
|Exercise Band|200.0|
|     Exercise|300.4|
+-------------+-----+

Window Row Number
+-------------+------+----------+
|     category|amount|row_number|
+-------------+------+----------+
|   Gymnastics| 200.0|         1|
|   Gymnastics| 200.0|         2|
|   Gymnastics| 100.0|         3|
|  Team Sports| 300.0|         1|
|  Team Sports| 300.0|         2|
|     Exercise| 300.4|         1|
|     Exercise| 300.4|         2|
|     Exercise| 100.0|         3|
|Exercise Band| 200.0|         1|
+-------------+------+----------+

Window Dense_rank Number
+-------------+------+----------+
|     category|amount|dense_rank|
+-------------+------+----------+
|   Gymnastics| 200.0|         1|
|   Gymnastics| 200.0|         1|
|   Gymnastics| 100.0|         2|
|  Team Sports| 300.0|         1|
|  Team Sports| 300.0|         1|
|     Exercise| 300.4|         1|
|     Exercise| 300.4|         1|
|     Exercise| 100.0|         2|
|Exercise Band| 200.0|         1|
+-------------+------+----------+

Window rank Number
+-------------+------+----+
|     category|amount|rank|
+-------------+------+----+
|   Gymnastics| 200.0|   1|
|   Gymnastics| 200.0|   1|
|   Gymnastics| 100.0|   3|
|  Team Sports| 300.0|   1|
|  Team Sports| 300.0|   1|
|     Exercise| 300.4|   1|
|     Exercise| 300.4|   1|
|     Exercise| 100.0|   3|
|Exercise Band| 200.0|   1|
+-------------+------+----+

Window Lead function
+-------------+------+-----+
|     category|amount| lead|
+-------------+------+-----+
|   Gymnastics| 200.0|200.0|
|   Gymnastics| 200.0|100.0|
|   Gymnastics| 100.0| null|
|  Team Sports| 300.0|300.0|
|  Team Sports| 300.0| null|
|     Exercise| 300.4|300.4|
|     Exercise| 300.4|100.0|
|     Exercise| 100.0| null|
|Exercise Band| 200.0| null|
+-------------+------+-----+

Window lag function
+-------------+------+-----+
|     category|amount|  lag|
+-------------+------+-----+
|   Gymnastics| 200.0| null|
|   Gymnastics| 200.0|200.0|
|   Gymnastics| 100.0|200.0|
|  Team Sports| 300.0| null|
|  Team Sports| 300.0|300.0|
|     Exercise| 300.4| null|
|     Exercise| 300.4|300.4|
|     Exercise| 100.0|300.4|
|Exercise Band| 200.0| null|
+-------------+------+-----+

Having function
+-----------+---+
|   category|cnt|
+-----------+---+
| Gymnastics|  3|
|Team Sports|  2|
|   Exercise|  3|
+-----------+---+

Inner Join
+---+----+-------+
| id|name|product|
+---+----+-------+
|  1| raj|    raj|
|  3| sai| mobile|
+---+----+-------+

Left Join
+---+----+-------+
| id|name|product|
+---+----+-------+
|  1| raj|    raj|
|  2|ravi|   null|
|  3| sai| mobile|
|  5|rani|   null|
+---+----+-------+

Right Join
+----+----+-------+
|  id|name|product|
+----+----+-------+
|   1| raj|    raj|
|   3| sai| mobile|
|null|null| laptop|
+----+----+-------+

Full Join
+----+----+-------+
|  id|name|product|
+----+----+-------+
|null|null| laptop|
|   3| sai| mobile|
|   5|rani|   null|
|   1| raj|    raj|
|   2|ravi|   null|
+----+----+-------+

left anti Join
+---+----+
| id|name|
+---+----+
|  2|ravi|
|  5|rani|
+---+----+

left semi Join
+---+----+
| id|name|
+---+----+
|  1| raj|
|  3| sai|
+---+----+

Date format
+---+----------+----------+
| id|     tdate|  con_date|
+---+----------+----------+
|  0|06-26-2011|2011-06-26|
|  1|05-26-2011|2011-05-26|
|  2|06-01-2011|2011-06-01|
|  3|06-05-2011|2011-06-05|
|  4|12-17-2011|2011-12-17|
|  5|02-14-2011|2011-02-14|
|  6|06-05-2011|2011-06-05|
|  7|12-17-2011|2011-12-17|
|  8|02-14-2011|2011-02-14|
+---+----------+----------+

Sub query
+-----+----------+
|total|  con_date|
+-----+----------+
|300.4|2011-06-26|
|200.0|2011-05-26|
|300.4|2011-06-01|
|600.0|2011-12-17|
|200.0|2011-06-05|
|400.0|2011-02-14|
+-----+----------+

collect_list
+-------------+--------------------+
|     category|           col_spend|
+-------------+--------------------+
|   Gymnastics|[credit, cash, cash]|
|  Team Sports|        [cash, cash]|
|     Exercise|[cash, cash, credit]|
|Exercise Band|            [credit]|
+-------------+--------------------+

Explode
+-------------+--------+
|     category|ex_spend|
+-------------+--------+
|   Gymnastics|    cash|
|   Gymnastics|  credit|
|  Team Sports|    cash|
|     Exercise|    cash|
|     Exercise|  credit|
|Exercise Band|  credit|
+-------------+--------+

explode_outer
+-------------+--------+
|     category|ex_spend|
+-------------+--------+
|   Gymnastics|    cash|
|   Gymnastics|  credit|
|  Team Sports|    cash|
|     Exercise|    cash|
|     Exercise|  credit|
|Exercise Band|  credit|
+-------------+--------+

==================================================================================


=================
07-01-2023
----------

Array INSIDE Array with Select

package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.functions.upper
import org.apache.spark.sql.catalyst.expressions.Upper
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window
import scala.io.Source
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions._

object obj {

	def main(args:Array[String]):Unit={


			val conf = new SparkConf().setAppName("ES").setMaster("local[*]")

					val sc = new SparkContext(conf)
					sc.setLogLevel("Error")


					val spark = SparkSession.builder()



					.getOrCreate()

					import spark.implicits._


					val df = spark.read.format("json")
					.option("multiline","true")
					.load("file:///C:/data/complexjson/cars.json")

					df.show()	
					df.printSchema()
					
		
					
					
					val flatten = df.withColumn("cars", expr("explode(cars)"))
					
					                .selectExpr(
					                    
					                      "age",
					                      "cars.models",
					                      "cars.name as carName",
					                      "name"
					                
					                
					                
					                      )
					
					flatten.show()
					flatten.printSchema()
					
					
					
					
					
					val finalflatten = flatten.withColumn("models", expr("explode(models)"))
					
					finalflatten.show()
					finalflatten.printSchema()
					
					
					
					
					
					


	}

}

-------------

=============
package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.functions.upper
import org.apache.spark.sql.catalyst.expressions.Upper
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window
import scala.io.Source
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions._

object obj {

	def main(args:Array[String]):Unit={


			val conf = new SparkConf().setAppName("ES").setMaster("local[*]")

					val sc = new SparkContext(conf)
					sc.setLogLevel("Error")


					val spark = SparkSession.builder()



					.getOrCreate()

					import spark.implicits._


					val df = spark.read.format("json")
					.option("multiline","true")
					.load("file:///D:/data/complexjson/cars.json")

					df.show()	
					df.printSchema()
					
		
					
					
					val flatten = df.withColumn("cars", expr("explode(cars)"))
					
					                .selectExpr(
					                    
					                      "age",
					                      "cars.models",
					                      "cars.name as carName",
					                      "name"
					                
					                
					                
					                      )
					
					flatten.show()
					flatten.printSchema()
					
					
					
					
					
					val finalflatten = flatten.withColumn("models", expr("explode(models)"))
					
					finalflatten.show()
					finalflatten.printSchema()
					


	}

}

=============
Out put:
--------
+---+--------------------+----+
|age|                cars|name|
+---+--------------------+----+
| 30|[[[Fiesta, Focus,...|John|
+---+--------------------+----+

root
 |-- age: long (nullable = true)
 |-- cars: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- models: array (nullable = true)
 |    |    |    |-- element: string (containsNull = true)
 |    |    |-- name: string (nullable = true)
 |-- name: string (nullable = true)

+---+--------------------+-------+----+
|age|              models|carName|name|
+---+--------------------+-------+----+
| 30|[Fiesta, Focus, M...|   Ford|John|
| 30|       [320, X3, X5]|    BMW|John|
| 30|        [500, Panda]|   Fiat|John|
+---+--------------------+-------+----+

root
 |-- age: long (nullable = true)
 |-- models: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- carName: string (nullable = true)
 |-- name: string (nullable = true)

+---+-------+-------+----+
|age| models|carName|name|
+---+-------+-------+----+
| 30| Fiesta|   Ford|John|
| 30|  Focus|   Ford|John|
| 30|Mustang|   Ford|John|
| 30|    320|    BMW|John|
| 30|     X3|    BMW|John|
| 30|     X5|    BMW|John|
| 30|    500|   Fiat|John|
| 30|  Panda|   Fiat|John|
+---+-------+-------+----+

root
 |-- age: long (nullable = true)
 |-- models: string (nullable = true)
 |-- carName: string (nullable = true)
 |-- name: string (nullable = true)


=======================================
Array Inside Array

package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.functions.upper
import org.apache.spark.sql.catalyst.expressions.Upper
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window
import scala.io.Source
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions._

object obj {

	def main(args:Array[String]):Unit={


			val conf = new SparkConf().setAppName("ES").setMaster("local[*]")

					val sc = new SparkContext(conf)
					sc.setLogLevel("Error")


					val spark = SparkSession.builder()



					.getOrCreate()

					import spark.implicits._


					val df = spark.read.format("json")
					.option("multiline","true")
					.load("file:///C:/data/complexjson/cars.json")

					df.show()	
					df.printSchema()
					
		
					
					
					val flatten = df.withColumn("cars", expr("explode(cars)"))
					                .withColumn("models",expr("explode(cars.models)"))
					                .withColumn("carname",expr("cars.name"))
					                .drop("cars")
					                
					
					flatten.show()
					flatten.printSchema()
					
					
					
					
					
/*					val finalflatten = flatten.withColumn("models", expr("explode(models)"))
					
					finalflatten.show()
					finalflatten.printSchema()
					*/
					
					
					
					
					


	}

}

=======================
My code:
--------
package pack

import org.apache.spark.SparkContext  // rdd
import org.apache.spark.sql.SparkSession  // dataframe
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.functions.upper
import org.apache.spark.sql.catalyst.expressions.Upper
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window
import scala.io.Source
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions._

object obj {

	def main(args:Array[String]):Unit={


			val conf = new SparkConf().setAppName("ES").setMaster("local[*]")

					val sc = new SparkContext(conf)
					sc.setLogLevel("Error")


					val spark = SparkSession.builder()



					.getOrCreate()

					import spark.implicits._


					val df = spark.read.format("json")
					.option("multiline","true")
					.load("file:///D:/data/complexjson/cars.json")

					df.show()	
					df.printSchema()
					
		
					
					
					val flatten = df.withColumn("cars", expr("explode(cars)"))
					                .withColumn("models",expr("explode(cars.models)"))
					                .withColumn("carname",expr("cars.name"))
					                .drop("cars")
					                
					
					flatten.show()
					flatten.printSchema()
					
					
					
					
					
/*					val finalflatten = flatten.withColumn("models", expr("explode(models)"))
					
					finalflatten.show()
					finalflatten.printSchema()
					*/
					
					
		


	}

}

==============
Out put:
--------
+---+--------------------+----+
|age|                cars|name|
+---+--------------------+----+
| 30|[[[Fiesta, Focus,...|John|
+---+--------------------+----+

root
 |-- age: long (nullable = true)
 |-- cars: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- models: array (nullable = true)
 |    |    |    |-- element: string (containsNull = true)
 |    |    |-- name: string (nullable = true)
 |-- name: string (nullable = true)

+---+----+-------+-------+
|age|name| models|carname|
+---+----+-------+-------+
| 30|John| Fiesta|   Ford|
| 30|John|  Focus|   Ford|
| 30|John|Mustang|   Ford|
| 30|John|    320|    BMW|
| 30|John|     X3|    BMW|
| 30|John|     X5|    BMW|
| 30|John|    500|   Fiat|
| 30|John|  Panda|   Fiat|
+---+----+-------+-------+

root
 |-- age: long (nullable = true)
 |-- name: string (nullable = true)
 |-- models: string (nullable = true)
 |-- carname: string (nullable = true)
============================================================

=======================
cd
rm -rf awscli-bundle.zip
rm -rf awscli-bundle
curl https://s3.amazonaws.com/aws-cli/awscli-bundle-1.16.188.zip -o awscli-bundle.zip
unzip awscli-bundle.zip
./awscli-bundle/install -i /home/<LABUSER>/aws -b /home/<LABUSER>/bin/aws
aws=/home/<LABUSER>/bin/aws
cd
-------------------------------------
cd
rm -rf awscli-bundle.zip
rm -rf awscli-bundle
curl https://s3.amazonaws.com/aws-cli/awscli-bundle-1.16.188.zip -o awscli-bundle.zip
unzip awscli-bundle.zip
./awscli-bundle/install -i /home/itv004068/aws -b /home/itv004068/bin/aws
aws=/home/itv004068/bin/aws
cd
-------------------------------------
===========================
aws configure

AKIAS3T7N3CTG7IBJCNI
iiVM7+/TEa+RfXxQzpuBsEVgAObP6Dgy850QpFT3
ap-south-1
json

Then Type---->

aws s3 ls
===========================
aws s3 mb s3://<NAME>zeyobuck

aws s3 ls s3://

aws s3 rb s3://<NAME>zeyobuck
===========================


===========================
[itv004068@g01 ~]$ cd
[itv004068@g01 ~]$ rm -rf awscli-bundle.zip
[itv004068@g01 ~]$ rm -rf awscli-bundle
[itv004068@g01 ~]$ curl https://s3.amazonaws.com/aws-cli/awscli-bundle-1.16.188.zip -o awscli-bundle.zip
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 11.6M  100 11.6M    0     0  5986k      0  0:00:01  0:00:01 --:--:-- 5986k
[itv004068@g01 ~]$ unzip awscli-bundle.zip
Archive:  awscli-bundle.zip
  inflating: awscli-bundle/install   
  inflating: awscli-bundle/packages/botocore-1.12.178.tar.gz  
  inflating: awscli-bundle/packages/argparse-1.2.1.tar.gz  
  inflating: awscli-bundle/packages/rsa-3.4.2.tar.gz  
  inflating: awscli-bundle/packages/ordereddict-1.1.tar.gz  
  inflating: awscli-bundle/packages/simplejson-3.3.0.tar.gz  
  inflating: awscli-bundle/packages/urllib3-1.25.3.tar.gz  
  inflating: awscli-bundle/packages/python-dateutil-2.6.1.tar.gz  
  inflating: awscli-bundle/packages/s3transfer-0.2.1.tar.gz  
  inflating: awscli-bundle/packages/six-1.12.0.tar.gz  
  inflating: awscli-bundle/packages/python-dateutil-2.8.0.tar.gz  
  inflating: awscli-bundle/packages/virtualenv-15.1.0.tar.gz  
  inflating: awscli-bundle/packages/jmespath-0.9.4.tar.gz  
  inflating: awscli-bundle/packages/urllib3-1.22.tar.gz  
  inflating: awscli-bundle/packages/colorama-0.3.9.tar.gz  
  inflating: awscli-bundle/packages/awscli-1.16.188.tar.gz  
  inflating: awscli-bundle/packages/PyYAML-3.13.tar.gz  
  inflating: awscli-bundle/packages/pyasn1-0.4.5.tar.gz  
  inflating: awscli-bundle/packages/docutils-0.14.tar.gz  
  inflating: awscli-bundle/packages/PyYAML-5.1.tar.gz  
  inflating: awscli-bundle/packages/futures-3.2.0.tar.gz  
  inflating: awscli-bundle/packages/setup/setuptools_scm-1.15.7.tar.gz  
[itv004068@g01 ~]$ ./awscli-bundle/install -i /home/itv004068/aws -b /home/itv004068/bin/aws
Running cmd: /bin/python virtualenv.py --no-download --python /bin/python /home/itv004068/aws
Running cmd: /home/itv004068/aws/bin/pip install --no-cache-dir --no-index --find-links file:///home/itv004068/awscli-bundle/packages/setup setuptools_scm-1.15.7.tar.gz
Running cmd: /home/itv004068/aws/bin/pip install --no-cache-dir --no-index --find-links file:///home/itv004068/awscli-bundle/packages awscli-1.16.188.tar.gz
Symlink already exists: /home/itv004068/bin/aws
Removing symlink.
You can now run: /home/itv004068/bin/aws --version
[itv004068@g01 ~]$ aws=/home/itv004068/bin/aws
[itv004068@g01 ~]$ cd
[itv004068@g01 ~]$ aws configure
AWS Access Key ID [****************WL4U]: AKIAS3T7N3CTG7IBJCNI
AWS Secret Access Key [****************QTXF]: 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
Default region name [None]: ap-south-1
Default output format [None]: json
[itv004068@g01 ~]$ aws s3 ls
2023-01-01 05:46:09 aws-logs-196761999526-ap-south-1
2023-01-07 00:31:23 ganesh21232345
2023-01-06 21:34:19 saibuk
2022-12-26 10:59:46 srizeyo
2023-01-07 00:33:42 zeyo34
2022-12-15 22:47:23 zeyo35buck
2023-01-02 00:01:28 zeyoprac7
[itv004068@g01 ~]$ aws s3 ls s3://
2023-01-01 05:46:09 aws-logs-196761999526-ap-south-1
2023-01-07 00:42:23 saifavbuk
2022-12-26 10:59:46 srizeyo
2023-01-07 00:33:42 zeyo34
2023-01-07 00:35:41 zeyo34nv
2022-12-15 22:47:23 zeyo35buck
2023-01-02 00:01:28 zeyoprac7
[itv004068@g01 ~]$ aws s3 mb s3://sivazeyobuck
make_bucket failed: s3://sivazeyobuck An error occurred (TooManyBuckets) when calling the CreateBucket operation: You have attempted to create more buckets than allowed
[itv004068@g01 ~]$ aws s3 ls s3://
2023-01-07 00:53:13 aamolzeyobuck
2023-01-07 00:52:58 abdulzeyobuck
2023-01-07 00:52:11 ajayzeyobuck
2023-01-07 00:52:10 akashmbuck
2023-01-07 00:53:21 akashzeyobuck
2023-01-07 00:52:09 anishzeyobuck
2023-01-07 00:52:43 arun222zeyobuck
2023-01-07 00:52:02 arunzeyobuck
2023-01-07 00:53:07 arunzzeyobuck
2023-01-07 00:52:18 ashuzeyobuck
2023-01-01 05:46:09 aws-logs-196761999526-ap-south-1
2023-01-07 00:52:08 bvrzeyobuck
2023-01-07 00:52:43 chaitanyavzeyobuck
2023-01-07 00:53:08 daminizeyobuck
2023-01-07 00:52:26 daszeyobuck
2023-01-07 00:52:06 dejizeyobuck
2023-01-07 00:52:03 dhanazeyobuck
2023-01-07 00:52:12 ganizeyobuck
2023-01-07 00:53:11 gautamzeyobuck
2023-01-07 00:51:59 gopalzeyobuck
2023-01-07 00:52:26 gowthamizeyobuck
2023-01-07 00:52:39 itv004113bucket
2023-01-07 00:52:54 jeffreybuckzeyo
2023-01-07 00:52:33 karthikbuckzeyo
2023-01-07 00:52:40 kaviezhuzeyobuck
2023-01-07 00:52:05 kavisaizeyobuck
2023-01-07 00:52:17 khushboozeyobuck
2023-01-07 00:52:20 kikibuck34
2023-01-07 00:52:05 kiranmyezeyobuck
2023-01-07 00:51:45 kusumazeyobuck
2023-01-07 00:51:34 maruthizeyobuck
2023-01-07 00:52:58 mayankzeyobuck
2023-01-07 00:52:48 mohanbuck
2023-01-07 00:52:09 monamibuck
2023-01-07 00:52:48 monizeyobuck
2023-01-07 00:52:38 naveennewbuck
2023-01-07 00:51:42 nayeemzeyobuck
2023-01-07 00:52:26 nikhilzeyobuck
2023-01-07 00:52:27 nithishzeyobuck
2023-01-07 00:52:54 paalrajzeyobuck
2023-01-07 00:52:35 prabhatzeyobuck
2023-01-07 00:51:56 prachizeyobuck
2023-01-07 00:52:42 pranjalizeyobuck
2023-01-07 00:52:49 prashathzeyobuck
2023-01-07 00:53:14 pratikzeyobuck
2023-01-07 00:53:16 praveenzeyobuck
2023-01-07 00:51:36 rajebuck
2023-01-07 00:51:33 rajeshzeyobuck
2023-01-07 00:52:58 rakeshbuckzeyo
2023-01-07 00:53:15 ramazeyobuck
2023-01-07 00:52:54 ramzeyobuck
2023-01-07 00:51:59 ravalizeyobuck
2023-01-07 00:52:01 rohinizeyobuck
2023-01-07 00:53:04 rupazeyobuck
2023-01-07 00:53:15 ruthbuck
2023-01-07 00:48:49 saibuckzeyobgidata
2023-01-07 00:42:23 saifavbuk
2023-01-07 00:51:42 saishazeyo
2023-01-07 00:52:07 sambathzeyobuck
2023-01-07 00:52:47 sammyzeyobuck
2023-01-07 00:52:08 seshazeyobuck
2023-01-07 00:52:22 shailajazeyobuck
2023-01-07 00:53:10 shakeelazeyobuck
2023-01-07 00:52:32 shashizeyobuck
2023-01-07 00:52:58 sheenbuck
2023-01-07 00:53:19 shreebigdatazeyo
2023-01-07 00:52:19 shwathizeyobuck
2023-01-07 00:52:55 sravanthizeyobuck
2023-01-07 00:53:12 srik34zeyobuck
2022-12-26 10:59:46 srizeyo
2023-01-07 00:52:21 subashzeyobuck
2023-01-07 00:52:18 sudheerzeyobuck
2023-01-07 00:52:37 sunilzeyobuck
2023-01-07 00:52:29 surabhizeyobuck
2023-01-07 00:52:48 sushanthzeyobuck
2023-01-07 00:52:43 tarazeyobuck
2023-01-07 00:52:32 tarunbucketzeyo
2023-01-07 00:53:06 tejaswwinizeyobuck
2023-01-07 00:53:12 testbucketzeyo
2023-01-07 00:52:19 thangazeyobuck
2023-01-07 00:52:12 ullaszeyobuck
2023-01-07 00:52:49 venizeyobuck
2023-01-07 00:51:42 venkybuck
2023-01-07 00:52:27 vijayabuckzeyo
2023-01-07 00:53:14 vijitzeyobuck
2023-01-07 00:51:58 vinayzeyobuck
2023-01-07 00:52:08 vishnuzeyo
2023-01-07 00:51:27 vivekzeyobuck
2023-01-07 00:53:15 zebzeyobuck
2023-01-07 00:33:42 zeyo34
2023-01-07 00:35:41 zeyo34nv
2022-12-15 22:47:23 zeyo35buck
2023-01-02 00:01:28 zeyoprac7
2023-01-07 00:53:13 zeyyobuck
[itv004068@g01 ~]$ aws s3 rb s3://sivazeyobuck
remove_bucket: sivazeyobuck
[itv004068@g01 ~]$ aws s3 ls s3://
2023-01-07 00:53:51 anantzeyobuck
2023-01-07 00:53:38 anjayadhdbuck
2023-01-07 00:53:46 arpudha
2023-01-07 00:52:43 arun222zeyobuck
2023-01-07 00:53:55 arunzeyobuck
2023-01-07 00:53:46 asrzeyobuck
2023-01-07 00:53:50 atulzeyobuck
2023-01-01 05:46:09 aws-logs-196761999526-ap-south-1
2023-01-07 00:52:08 bvrzeyobuck
2023-01-07 00:54:01 chaganzeyobuck
2023-01-07 00:53:53 chandrazeyobuck
2023-01-07 00:54:25 chethanzeyobuck
2023-01-07 00:52:03 dhanazeyobuck
2023-01-07 00:53:49 fayazzeyobuck
2023-01-07 00:52:12 ganizeyobuck
2023-01-07 00:53:41 ggzeyobuck
2023-01-07 00:51:59 gopalzeyobuck
2023-01-07 00:52:26 gowthamizeyobuck
2023-01-07 00:53:23 gurubuckzeyo
2023-01-07 00:54:08 haranathzeyobuck
2023-01-07 00:53:36 himazeyobuck
2023-01-07 00:53:21 ishjbuc
2023-01-07 00:52:39 itv004113bucket
2023-01-07 00:52:54 jeffreybuckzeyo
2023-01-07 00:53:50 jjzeyobuck
2023-01-07 00:53:52 karthikazeyobuck
2023-01-07 00:52:20 kikibuck34
2023-01-07 00:52:58 mayankzeyobuck
2023-01-07 00:52:48 mohanbuck
2023-01-07 00:53:38 naga1zeyobuck
2023-01-07 00:54:19 nallathambizeyobuck
2023-01-07 00:54:16 nezeyobuck
2023-01-07 00:52:26 nikhilzeyobuck
2023-01-07 00:54:26 nivedhazeyobuck
2023-01-07 00:54:08 pareshzeyobuck
2023-01-07 00:51:36 rajebuck
2023-01-07 00:52:58 rakeshbuckzeyo
2023-01-07 00:53:15 ramazeyobuck
2023-01-07 00:52:01 rohinizeyobuck
2023-01-07 00:53:38 rohithzeyobuck
2023-01-07 00:53:15 ruthbuck
2023-01-07 00:53:30 sachizeyobuck
2023-01-07 00:48:49 saibuckzeyobgidata
2023-01-07 00:42:23 saifavbuk
2023-01-07 00:51:42 saishazeyo
2023-01-07 00:52:07 sambathzeyobuck
2023-01-07 00:54:19 sandeepkzeyobuck
2023-01-07 00:52:32 shashizeyobuck
2023-01-07 00:53:37 shivaleelazeyobuck
2023-01-07 00:53:19 shreebigdatazeyo
2023-01-07 00:53:52 soundharzeyobuck
2023-01-07 00:52:55 sravanthizeyobuck
2023-01-07 00:53:12 srik34zeyobuck
2023-01-07 00:54:04 srinivaszeyobuck
2022-12-26 10:59:46 srizeyo
2023-01-07 00:54:01 subrahmanyazeyobuck
2023-01-07 00:53:59 suganthizeyobuck
2023-01-07 00:52:37 sunilzeyobuck
2023-01-07 00:52:12 ullaszeyobuck
2023-01-07 00:54:22 umazyobuck
2023-01-07 00:53:55 uzzwalzeyobuck
2023-01-07 00:51:42 venkybuck
2023-01-07 00:53:14 vijitzeyobuck
2023-01-07 00:54:25 vishzeyobuck
2023-01-07 00:54:03 yaminizeyobuck
2023-01-07 00:33:42 zeyo34
2023-01-07 00:35:41 zeyo34nv
2022-12-15 22:47:23 zeyo35buck
2023-01-07 00:53:23 zeyobuck
2023-01-07 00:54:09 zeyopooja1
2023-01-02 00:01:28 zeyoprac7
[itv004068@g01 ~]$
==========================
[itv004068@g01 ~]$ aws s3 mb s3://sivazeyobuck
make_bucket: sivazeyobuck
[itv004068@g01 ~]$ aws s3 ls s3://
2023-01-01 05:46:09 aws-logs-196761999526-ap-south-1
2023-01-07 00:55:05 pallavi35zeyobuck
2023-01-07 00:52:58 rakeshbuckzeyo
2023-01-07 00:48:49 saibuckzeyobgidata
2023-01-07 02:46:11 sivazeyobuck
2022-12-26 10:59:46 srizeyo
2023-01-07 00:33:42 zeyo34
2023-01-07 00:35:41 zeyo34nv
2022-12-15 22:47:23 zeyo35buck
2023-01-02 00:01:28 zeyoprac7
[itv004068@g01 ~]$ aws s3 rb s3://sivazeyobuck
remove_bucket: sivazeyobuck
[itv004068@g01 ~]$ aws s3 ls s3://
2023-01-01 05:46:09 aws-logs-196761999526-ap-south-1
2023-01-07 00:55:05 pallavi35zeyobuck
2023-01-07 00:52:58 rakeshbuckzeyo
2023-01-07 00:48:49 saibuckzeyobgidata
2022-12-26 10:59:46 srizeyo
2023-01-07 00:33:42 zeyo34
2023-01-07 00:35:41 zeyo34nv
2022-12-15 22:47:23 zeyo35buck
2023-01-02 00:01:28 zeyoprac7
[itv004068@g01 ~]$ 
=======================================
==============================


===========================
Project Passage


This is __ .. My total years of exp and Relevant exp ..

I got chance to work on different Big Data Stack.Like(Hadoop,hdfs,hive,spark,sqoop,AWS). Recently i started migrating project AWS.

Before I used to in the data ingestion team where we used RDBMS as a source and we sqoop the data to HDFS and we processed it using hive and write to HDFS as avro. We use avro because of schema evolution. We have multiple RDBMS table in which we run multiple Sqoop Jobs and do the processing.

Later for example (1 year ago). I started working with Data application team where I have spark rigorously. In the data application. We have so many WEB apis coming with complex json json with different data models we almost run 7 spark jobs for different use cases  like  Customer data cleansing, Prediction Model spark jobs with currency conversions and few of the spark jobs do joins with AVRO data which generated during data ingestion  and we write data to different HDFS directories as per the requirement also with complex nested data generation.

My business uses impala to do analytics on processed data.


In the recent times we started migrating the jobs to AWS. with services s3 , EMR for spark jobs,Athena for Business analytics and ec2 for scheduling.

We have a done POC on EMR step executions run those spark jobs using EMR command Runner.
========================================================================================



===========================
08-01-2023
-----------
echo zeyobron> itv004068.txt
aws s3 cp itv004068.txt s3://saizeyobuck/
aws s3 ls s3://saizeyobuck/
aws s3 rm s3://saizeyobuck/itv004068.txt
aws s3 ls s3://saizeyobuck/

================================
[itv004068@g01 ~]$ echo zeyobron> itv004068.txt
[itv004068@g01 ~]$ aws s3 cp itv004068.txt s3://saizeyobuck/
upload: ./itv004068.txt to s3://saizeyobuck/itv004068.txt     
[itv004068@g01 ~]$ aws s3 ls s3://saizeyobuck/
2023-01-07 22:43:27          9 Abdul.txt
2023-01-07 22:40:23          9 Anant.txt
2023-01-07 22:43:13          9 Anithafile.txt
2023-01-07 22:43:28          9 Aswi.txt
2023-01-07 22:42:37         16 Bhavisya.txt
2023-01-07 22:42:13         12 GuruBigData.txt
2023-01-07 22:43:58          9 Jeffrey.txt
2023-01-07 22:44:15          9 Karthiktest.txt
2023-01-07 22:42:43          9 Mano.txt
2023-01-07 22:42:32          9 Mru.txt
2023-01-07 22:43:01          9 Mukundh.txt
2023-01-07 22:44:20          9 Revathy.txt
2023-01-07 22:42:14          9 Rohini.txt
2023-01-07 22:42:35          9 Saeed.txt
2023-01-07 22:42:44          9 Shwathi.txt
2023-01-07 22:40:14          0 abhishek.txt
2023-01-07 22:44:19          9 akashm.txt
2023-01-07 22:44:26         75 amar95159.txt
2023-01-07 22:43:47          9 anja.txt
2023-01-07 22:42:43          9 ankitdha.txt
2023-01-07 22:42:00          9 arp.txt
2023-01-07 22:41:48          9 ashutosh.txt
2023-01-07 22:44:17          9 asr.txt
2023-01-07 22:42:55          8 avanish.txt
2023-01-07 22:44:01          9 balaji.txt
2023-01-07 22:42:06          9 birundha.txt
2023-01-07 22:42:54          9 chakrapanireddy.txt
2023-01-07 22:42:41          9 dhanush.txt
2023-01-07 22:42:30          9 gowthamgowda.txt
2023-01-07 22:42:56          9 harika.txt
2023-01-07 22:42:22          9 himasekhar.txt
2023-01-07 22:44:07          9 itv004068.txt
2023-01-07 22:42:53          9 itv004171.txt
2023-01-07 22:42:25          9 kartheek.txt
2023-01-07 22:44:09          9 karthik.txt
2023-01-07 22:42:54          9 kaviezhu.txt
2023-01-07 22:42:03          9 kiki.txt
2023-01-07 22:43:45          9 kinghari.txt
2023-01-07 22:42:54          9 manojk.txt
2023-01-07 22:43:38          9 monira.txt
2023-01-07 22:42:42          9 nakulpatil.txt
2023-01-07 22:43:36          9 paalraj.txt
2023-01-07 22:44:19          9 prabhat4422.txt
2023-01-07 22:43:19          9 prasannakumar.txt
2023-01-07 22:42:06          9 prashanth_M
2023-01-07 22:40:52          9 praveen_t.txt
2023-01-07 22:43:20          9 pravinas.txt
2023-01-07 22:43:26          9 rajeshkumar.txt
2023-01-07 22:43:39          9 rakeshbijigi.txt
2023-01-07 22:42:12          9 ravikiranradhak.txt
2023-01-07 22:44:20          9 rupa.txt
2023-01-07 22:41:57          9 sanket.text
2023-01-07 22:43:41          9 satya.txt
2023-01-07 22:43:25         10 sheen.txt
2023-01-07 22:44:06          9 soundhar.txt
2023-01-07 22:43:27          9 sridevi.txt
2023-01-07 22:43:04          9 sunil.txt
2023-01-07 22:43:06          9 surabhi.txt
2023-01-07 22:43:30          9 suvarna.txt
2023-01-07 22:43:15          9 uma.txt
2023-01-07 22:43:46         13 umashankar10.txt
2023-01-07 22:42:37          9 venkat.txt
2023-01-07 22:42:55          9 venky.txt
2023-01-07 22:41:23          8 vineet.txt
2023-01-07 22:44:06         19 wasim.txt
[itv004068@g01 ~]$ aws s3 rm s3://saizeyobuck/itv004068.txt
delete: s3://saizeyobuck/itv004068.txt
[itv004068@g01 ~]$ aws s3 ls s3://saizeyobuck/
2023-01-07 22:44:47          9 Arunkumar.txt
2023-01-07 22:43:28          9 Aswi.txt
2023-01-07 22:42:37         16 Bhavisya.txt
2023-01-07 22:45:28          9 Gautam.txt
2023-01-07 22:42:13         12 GuruBigData.txt
2023-01-07 22:44:46          9 Pratik.txt
2023-01-07 22:42:35          9 Saeed.txt
2023-01-07 22:40:14          0 abhishek.txt
2023-01-07 22:44:32          9 akash.txt
2023-01-07 22:45:10          9 alnak.txt
2023-01-07 22:44:26         75 amar95159.txt
2023-01-07 22:43:47          9 anja.txt
2023-01-07 22:44:17          9 asr.txt
2023-01-07 22:44:33          8 chandra.txt
2023-01-07 22:45:14          9 chidvikas.txt
2023-01-07 22:44:28          9 durgafile.txt
2023-01-07 22:45:31          9 gautam.txt
2023-01-07 22:42:30          9 gowthamgowda.txt
2023-01-07 22:44:41          9 gowthami.text
2023-01-07 22:42:56          9 harika.txt
2023-01-07 22:42:53          9 itv004171.txt
2023-01-07 22:42:25          9 kartheek.txt
2023-01-07 22:43:45          9 kinghari.txt
2023-01-07 22:42:54          9 manojk.txt
2023-01-07 22:44:29          9 manojs9703
2023-01-07 22:44:52          1 mohd.txt
2023-01-07 22:42:42          9 nakulpatil.txt
2023-01-07 22:43:36          9 paalraj.txt
2023-01-07 22:44:19          9 prabhat4422.txt
2023-01-07 22:44:43          9 prachi.txt
2023-01-07 22:45:25          9 prasad_k.txt
2023-01-07 22:44:32          9 rajesh.txt
2023-01-07 22:43:26          9 rajeshkumar.txt
2023-01-07 22:42:12          9 ravikiranradhak.txt
2023-01-07 22:44:47          9 sachi.text
2023-01-07 22:44:32          9 saha.txt
2023-01-07 22:41:57          9 sanket.text
2023-01-07 22:43:41          9 satya.txt
2023-01-07 22:43:25         10 sheen.txt
2023-01-07 22:45:18          9 sibbalanagendra.txt
2023-01-07 22:44:27          9 sikandar.txt
2023-01-07 22:44:06          9 soundhar.txt
2023-01-07 22:43:06          9 surabhi.txt
2023-01-07 22:45:03          9 ullas.txt
2023-01-07 22:45:17          9 vishnuji.txt
2023-01-07 22:45:26         10 zaid.txt
[itv004068@g01 ~]$ 





===========================
2Handson
--------
mkdir zldir
touch zldir/file1
touch zldir/file2
touch zldir/file3

aws s3 sync zldir/ s3://saizeyobuck/itv004068/
rm -rf zldir/*
aws s3 sync s3://saizeyobuck/itv004068/ zldir/
aws s3 rm s3://saizeyobuck/itv004068 --recursive
----------------------------------------------
===============================
[itv004068@g01 ~]$ mkdir zldir
[itv004068@g01 ~]$ touch zldir/file1
[itv004068@g01 ~]$ touch zldir/file2
[itv004068@g01 ~]$ touch zldir/file3
[itv004068@g01 ~]$ aws s3 sync zldir/ s3://saizeyobuck/itv004068/
upload: zldir/file1 to s3://saizeyobuck/itv004068/file1
upload: zldir/file2 to s3://saizeyobuck/itv004068/file2
upload: zldir/file3 to s3://saizeyobuck/itv004068/file3
[itv004068@g01 ~]$ rm -rf zldir/*
[itv004068@g01 ~]$ aws s3 sync s3://saizeyobuck/itv004068/ zldir/
download: s3://saizeyobuck/itv004068/file1 to zldir/file1
download: s3://saizeyobuck/itv004068/file3 to zldir/file3
download: s3://saizeyobuck/itv004068/file2 to zldir/file2
[itv004068@g01 ~]$ aws s3 rm s3://saizeyobuck/itv004068 --recursive
delete: s3://saizeyobuck/itv004068/file3
delete: s3://saizeyobuck/itv004068/file1
delete: s3://saizeyobuck/itv004068/file2
[itv004068@g01 ~]$ 
===============================
=========================================================
EMR Step Execution Deployment

Once I develop the Code
We put it in eclipse
Commit the Code to GIT

Devops run Jenkins File which generates the Jar in the Production Bucket

Then we generate EMR Step Execution Script

Then we automate through Command Runner


That Command runner script we schedule in EC2 Crontab

=========================================================

Task 1 ---

cd
mkdir zdir
touch zdir/file1
touch zdir/file2
aws s3 sync zdir/ s3://saibucketzeyo/itv004068/
echo zeyobron>zdir/file1
aws s3 sync zdir/ s3://saibucketzeyo/itv004068/

-------------------
[itv004068@g01 ~]$ cd
[itv004068@g01 ~]$ mkdir zdir
[itv004068@g01 ~]$ touch zdir/file1
[itv004068@g01 ~]$ touch zdir/file2
[itv004068@g01 ~]$ aws s3 sync zdir/ s3://saibucketzeyo/itv004068/
upload: zdir/file2 to s3://saibucketzeyo/itv004068/file2
upload: zdir/file1 to s3://saibucketzeyo/itv004068/file1
[itv004068@g01 ~]$ echo zeyobron>zdir/file1
[itv004068@g01 ~]$ aws s3 sync zdir/ s3://saibucketzeyo/itv004068/
upload: zdir/file1 to s3://saibucketzeyo/itv004068/file1       
[itv004068@g01 ~]$ 
---------------------
===========
Task 2 --
-----------
rm -rf zdir/*
aws s3 sync s3://saibucketzeyo/itv004068/ zdir/

--------------
[itv004068@g01 ~]$ rm -rf zdir/*
[itv004068@g01 ~]$ aws s3 sync s3://saibucketzeyo/itv004068/ zdir/
download: s3://saibucketzeyo/itv004068/file2 to zdir/file2
download: s3://saibucketzeyo/itv004068/file1 to zdir/file1    
[itv004068@g01 ~]$
==========================================
=====================================================
14-01-2023:
-----------
First got Requirement
Create a development Cluster
Opened my spark shell
Developed my Code
Tested the results
Put that steps code to eclipse
Generated Jar and uploaded to s3
Tested the Jar using Step Execution manually
Then Got the Deployment Script

Step 1 --- Code
---------------
import scala.io._

val urldata = Source.fromURL("https://randomuser.me/api/0.8/?results=1000").mkString


val rdd = sc.parallelize(List(urldata))

val jsondf = spark.read.json(rdd)

jsondf.show()

val flatdf = jsondf.withColumn("results",explode(col("results"))).select("nationality","seed","version","results.user.username","results.user.cell","results.user.dob","results.user.email","results.user.gender","results.user.location.city","results.user.location.state","results.user.location.street","results.user.location.zip","results.user.md5","results.user.name.first","results.user.name.last","results.user.name.title","results.user.password","results.user.phone","results.user.picture.large","results.user.picture.medium","results.user.picture.thumbnail","results.user.registered","results.user.salt","results.user.sha1","results.user.sha256")

flatdf.write.format("parquet").mode("overwrite").save("s3://devzeyo/src/2023-01-14/apidata/")

Step 2 Code 
-----------

val flatdf = spark.read.parquet("s3://devzeyo/src/2023-01-14/apidata/")



flatdf.withColumn("current_date",current_date).write.format("jdbc").option("url","jdbc:mysql://zeyodb.cyewo8l7hsyn.ap-northeast-1.rds.amazonaws.com/zeyodb").option("driver","com.mysql.jdbc.Driver").option("user","root").option("password","Aditya908").option("dbtable","apidatatab").mode("append").save()

Step 3  Code


val data = spark.read.format("avro").load("s3://devzeyo/src/2023-01-14/extracted/")

val flatdf = spark.read.format("parquet").load("s3://devzeyo/src/2023-01-14/apidata/")

val rm=flatdf.withColumn("username",regexp_replace(col("username"),  "([0-9])", ""))

val joindf = data.join(broadcast(rm),Seq("username"),"left")

val dfnull = joindf.filter(col("nationality").isNull)

val dfnotnull=joindf.filter(col("nationality").isNotNull)

val replacenull= dfnull.na.fill("Not Available").na.fill(0)

val nonavailable=replacenull.withColumn("current_date",current_date)

val available=dfnotnull.withColumn("current_date",current_date)


available.write.format("parquet").mode("overwrite").save("s3://devzeyo/dest/2023-01-14/availablecustomer/")


nonavailable.write.format("parquet").mode("overwrite").save("s3://devzeyo/dest/2023-01-14/nonavailablecustomer/")





=====================================================

=====================================================
15-01-2023:
-----------
Task 1
aws emr create-cluster --applications Name=Hadoop Name=Spark --ec2-attributes '{"InstanceProfile":"EMR_EC2_DefaultRole","SubnetId":"subnet-045cf55130c1eab0a","EmrManagedSlaveSecurityGroup":"sg-059805369c527ce28","EmrManagedMasterSecurityGroup":"sg-0b514cbb46d39ee30"}' --release-label emr-5.36.0 --log-uri 's3n://aws-logs-196761999526-ap-south-1/elasticmapreduce/' --steps '[{"Args":["spark-submit","--deploy-mode","client","--master","yarn","--packages","org.apache.spark:spark-avro_2.11:2.4.0","--class","pack.custavail","s3://devzeyo/SparkDp-0.0.1-SNAPSHOT.jar","<URNAME>"],"Type":"CUSTOM_JAR","ActionOnFailure":"CONTINUE","Jar":"command-runner.jar","Properties":"","Name":"RunStep"}]' --instance-groups '[{"InstanceCount":1,"EbsConfiguration":{"EbsBlockDeviceConfigs":[{"VolumeSpecification":{"SizeInGB":32,"VolumeType":"gp2"},"VolumesPerInstance":2}]},"InstanceGroupType":"MASTER","InstanceType":"m5.xlarge","Name":"Master Instance Group"}]' --configurations '[{"Classification":"spark","Properties":{}}]' --auto-terminate --service-role EMR_DefaultRole  --name '<URNAME>Cluster' --scale-down-behavior TERMINATE_AT_TASK_COMPLETION --region ap-south-1



aws emr describe-cluster --cluster-id <CLUSTERID> | grep 'State'


aws s3 ls s3://devzeyo/dest/
===========================================================
Task:
-----
aws emr create-cluster --applications Name=Hadoop Name=Spark --ec2-attributes '{"InstanceProfile":"EMR_EC2_DefaultRole","SubnetId":"subnet-045cf55130c1eab0a","EmrManagedSlaveSecurityGroup":"sg-059805369c527ce28","EmrManagedMasterSecurityGroup":"sg-0b514cbb46d39ee30"}' --release-label emr-5.36.0 --log-uri 's3n://aws-logs-196761999526-ap-south-1/elasticmapreduce/' --steps '[{"Args":["spark-submit","--deploy-mode","client","--master","yarn","--packages","org.apache.spark:spark-avro_2.11:2.4.0","--class","pack.custavail","s3://devzeyo/SparkDp-0.0.1-SNAPSHOT.jar","Sivasankar"],"Type":"CUSTOM_JAR","ActionOnFailure":"CONTINUE","Jar":"command-runner.jar","Properties":"","Name":"RunStep"}]' --instance-groups '[{"InstanceCount":1,"EbsConfiguration":{"EbsBlockDeviceConfigs":[{"VolumeSpecification":{"SizeInGB":32,"VolumeType":"gp2"},"VolumesPerInstance":2}]},"InstanceGroupType":"MASTER","InstanceType":"m5.xlarge","Name":"Master Instance Group"}]' --configurations '[{"Classification":"spark","Properties":{}}]' --auto-terminate --service-role EMR_DefaultRole  --name 'SivasankarCluster' --scale-down-behavior TERMINATE_AT_TASK_COMPLETION --region ap-south-1



aws emr describe-cluster --cluster-id j-1OFQF5VRXXLQE | grep 'State'


aws s3 ls s3://devzeyo/dest/

Out put:
--------
[itv004068@g01 ~]$ aws emr create-cluster --applications Name=Hadoop Name=Spark --ec2-attributes '{"InstanceProfile":"EMR_EC2_DefaultRole","SubnetId":"subnet-045cf55130c1eab0a","EmrManagedSlaveSecurityGroup":"sg-059805369c527ce28","EmrManagedMasterSecurityGroup":"sg-0b514cbb46d39ee30"}' --release-label emr-5.36.0 --log-uri 's3n://aws-logs-196761999526-ap-south-1/elasticmapreduce/' --steps '[{"Args":["spark-submit","--deploy-mode","client","--master","yarn","--packages","org.apache.spark:spark-avro_2.11:2.4.0","--class","pack.custavail","s3://devzeyo/SparkDp-0.0.1-SNAPSHOT.jar","Sivasankar"],"Type":"CUSTOM_JAR","ActionOnFailure":"CONTINUE","Jar":"command-runner.jar","Properties":"","Name":"RunStep"}]' --instance-groups '[{"InstanceCount":1,"EbsConfiguration":{"EbsBlockDeviceConfigs":[{"VolumeSpecification":{"SizeInGB":32,"VolumeType":"gp2"},"VolumesPerInstance":2}]},"InstanceGroupType":"MASTER","InstanceType":"m5.xlarge","Name":"Master Instance Group"}]' --configurations '[{"Classification":"spark","Properties":{}}]' --auto-terminate --service-role EMR_DefaultRole  --name 'SivasankarCluster' --scale-down-behavior TERMINATE_AT_TASK_COMPLETION --region ap-south-1
{
    "ClusterId": "j-1OFQF5VRXXLQE"
}
[itv004068@g01 ~]$ aws emr describe-cluster --cluster-id j-1OFQF5VRXXLQE | grep 'State'
            "State": "STARTING", 
            "StateChangeReason": {
                    "State": "BOOTSTRAPPING", 
                    "StateChangeReason": {
[itv004068@g01 ~]$ aws s3 ls s3://devzeyo/dest/Sivasankar/
                           PRE availablecustomer/
                           PRE nonavailablecustomer/
2023-01-15 02:23:38          0 availablecustomer_$folder$
2023-01-15 02:23:43          0 nonavailablecustomer_$folder$
[itv004068@g01 ~]$ aws s3 ls s3://devzeyo/dest/
                           PRE 2023-01-14/
                           PRE <Dsnavee>/
                           PRE <Srishti>/
                           PRE <URNAME>/
                           PRE Ankit99/
                           PRE Arjun/
                           PRE Arunkumar123/
                           PRE Ashwin/
                           PRE BaluHitendra/
                           PRE Bilal/
                           PRE Chandana/
                           PRE Deji>/
                           PRE Dsna/
                           PRE Gajendra/
                           PRE Gurumoorthy/
                           PRE Gururaj/
                           PRE GururajSdir/
                           PRE Himasekhar/
                           PRE Irappa/
                           PRE KD85/
                           PRE Keerthi/
                           PRE Kiranmye/
                           PRE Kusuma/
                           PRE Latha/
                           PRE Mallidir/
                           PRE Mani/
                           PRE Nalla/
                           PRE Nallathambi/
                           PRE Namrata/
                           PRE Naveenkota/
                           PRE Neha/
                           PRE Prem/
                           PRE RAMATV/
                           PRE Ragapriya/
                           PRE Raja/
                           PRE Ramarajan/
                           PRE Rathan/
                           PRE Ravalika/
                           PRE SANDEEPK/
                           PRE Saeed/
                           PRE Sai/
                           PRE Saidir/
                           PRE Savin/
                           PRE ShaikMahammadAli/
                           PRE Shailendra/
                           PRE Shashi/
                           PRE Sivasankar/
                           PRE Sreeja/
                           PRE Subrahmanya/
                           PRE Subramani_batch34/
                           PRE Suchidir/
                           PRE Yash/
                           PRE aakashm/
                           PRE abdul234/
                           PRE ashish/
                           PRE bhaskarbabul/
                           PRE chandan/
                           PRE chandra/
                           PRE durga/
                           PRE gana/
                           PRE ganeshb/
                           PRE gani/
                           PRE gowthamikj/
                           PRE kavithasai/
                           PRE madhavan/
                           PRE mano/
                           PRE mrudhula/
                           PRE mubarak/
                           PRE nanda/
                           PRE pkvaustin/
                           PRE prakash/
                           PRE prasad_k_9/
                           PRE prasannakumar/
                           PRE praveent/
                           PRE rakesh/
                           PRE ram/
                           PRE ramakrishna/
                           PRE ravali/
                           PRE seshadris/
                           PRE sheela/
                           PRE shwathi/
                           PRE sikandar/
                           PRE siva/
                           PRE subash/
                           PRE sudhir/
                           PRE syed498/
                           PRE tarun/
                           PRE udaykiran/
                           PRE ullas/
                           PRE yuvi/
2023-01-07 21:41:38          0 
2023-01-15 00:54:09          0 2023-01-14_$folder$
2023-01-15 02:28:34          0 <Srishti>_$folder$
2023-01-15 01:21:06          0 <URNAME>_$folder$
2023-01-15 01:41:21          0 Bilal_$folder$
2023-01-15 01:58:01          0 Himasekhar_$folder$
2023-01-15 02:15:00          0 Sivasankar_$folder$
2023-01-15 01:58:55          0 aakashm_$folder$
2023-01-15 02:04:58          0 chandra_$folder$
2023-01-15 02:01:14          0 kavithasai_$folder$
[itv004068@g01 ~]$ 
=====================================================


====================================================
21-01-2023:
----------
from pyspark import SparkContext, SparkConf
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from collections import namedtuple
from pyspark.sql import *
from pyspark.sql.functions import *


conf = SparkConf().setAppName("first").setMaster("local[*]")
sc = SparkContext(conf=conf)

spark = SparkSession \
    .builder \
    .getOrCreate()



file1 = sc.textFile("file:///home/cloudera/revdata/file1.txt")

gymdata = file1.filter( lambda x : 'Gymnastics' in x )


schema=namedtuple("schema",["txnno","txndate","custno","amount","category","product","city","state","spendby"])

mapsplit = gymdata.map( lambda x : x.split(","))
schemardd = mapsplit.map( lambda x : schema(x[0],x[1],x[2],x[3],x[4],x[5],x[6],x[7],x[8]))

prodfilter  = schemardd.filter( lambda x : 'Gymnastics' in x.product )

df = prodfilter.toDF()
schemadf= df



file2=sc.textFile("file:///home/cloudera/revdata/file2.txt")

mapsplit = file2.map( lambda x : x.split(","))

rowrdd = mapsplit.map( lambda x : Row(x[0],x[1],x[2],x[3],x[4],x[5],x[6],x[7],x[8]) )




schema1 = StructType([ \
    StructField("txnno",StringType(),True), \
    StructField("txndate",StringType(),True), \
    StructField("custno",StringType(),True), \
    StructField("amount", StringType(), True), \
    StructField("category", StringType(), True), \
    StructField("product", StringType(), True), \
    StructField("city", StringType(), True), \
    StructField("state", StringType(), True), \
    StructField("spendby", StringType(), True) \
  ])


rowdf = spark.createDataFrame(rowrdd,schema1)


csvdf = spark.read.format("csv").option("header","true").load("file:///home/cloudera/revdata/file3.txt")
jsondf = spark.read.format("json").load("file:///home/cloudera/revdata/file4.json")
parquetdf = spark.read.format("parquet").load("file:///home/cloudera/revdata/file5.parquet")



columns = ['txnno','txndate','custno','amount','category','product','city','state','spendby']


schemadf= schemadf.select(*columns)
rowdf = rowdf.select(*columns)
csvdf= csvdf.select(*columns)
jsondf=jsondf.select(*columns)
parquetdf= parquetdf.select(*columns)


uniondf = schemadf.union(rowdf).union(csvdf).union(jsondf).union(parquetdf)




procdf = uniondf.withColumn("txndate",expr("split(txndate,'-')[2]")).withColumnRenamed("txndate","year").withColumn("status",expr("case when spendby='cash' then 1 else 0 end")).filter(col("txnno") > 50000)


aggdf = procdf.groupBy("category").agg(sum("amount").alias("total"))


aggdf.write.format("parquet").partitionBy("category").mode("overwrite").save("/user/cloudera/writedfDeploy")
================================================
Task1:
------
from pyspark import SparkContext, SparkConf
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from collections import namedtuple
from pyspark.sql import *
from pyspark.sql.functions import *


conf = SparkConf().setAppName("first").setMaster("local[*]")
sc = SparkContext(conf=conf)

spark = SparkSession \
    .builder \
    .getOrCreate()



file1 = sc.textFile("file:///home/itv004068/file1.txt")

gymdata = file1.filter( lambda x : 'Gymnastics' in x )


schema=namedtuple("schema",["txnno","txndate","custno","amount","category","product","city","state","spendby"])

mapsplit = gymdata.map( lambda x : x.split(","))
schemardd = mapsplit.map( lambda x : schema(x[0],x[1],x[2],x[3],x[4],x[5],x[6],x[7],x[8]))

prodfilter  = schemardd.filter( lambda x : 'Gymnastics' in x.product )

df = prodfilter.toDF()
schemadf = df



file2=sc.textFile("file:///home/itv004068/file2.txt")

mapsplit = file2.map( lambda x : x.split(","))

rowrdd = mapsplit.map( lambda x : Row(x[0],x[1],x[2],x[3],x[4],x[5],x[6],x[7],x[8]) )




schema1 = StructType([ \
    StructField("txnno",StringType(),True), \
    StructField("txndate",StringType(),True), \
    StructField("custno",StringType(),True), \
    StructField("amount", StringType(), True), \
    StructField("category", StringType(), True), \
    StructField("product", StringType(), True), \
    StructField("city", StringType(), True), \
    StructField("state", StringType(), True), \
    StructField("spendby", StringType(), True) \
  ])


rowdf = spark.createDataFrame(rowrdd,schema1)


csvdf = spark.read.format("csv").option("header","true").load("file:///home/itv004068/file3.txt")
jsondf = spark.read.format("json").load("file:///home/itv004068/file4.json")
parquetdf = spark.read.format("parquet").load("file:///home/itv004068/file5.parquet")



columns = ['txnno','txndate','custno','amount','category','product','city','state','spendby']


schemadf= schemadf.select(*columns)
rowdf = rowdf.select(*columns)
csvdf= csvdf.select(*columns)
jsondf=jsondf.select(*columns)
parquetdf= parquetdf.select(*columns)


uniondf = schemadf.union(rowdf).union(csvdf).union(jsondf).union(parquetdf)




procdf = uniondf.withColumn("txndate",expr("split(txndate,'-')[2]")).withColumnRenamed("txndate","year").withColumn("status",expr("case when spendby='cash' then 1 else 0 end")).filter(col("txnno") > 50000)


aggdf = procdf.groupBy("category").agg(sum("amount").alias("total"))


aggdf.write.format("parquet").partitionBy("category").mode("overwrite").save("/user/cloudera/writedfDeploy")

Out Put:
--------






==================================

==================================
22-01-2023:
-----------

aws emr create-cluster  --applications Name=Hadoop Name=Spark --ec2-attributes '{"KeyName":"34key","InstanceProfile":"EMR_EC2_DefaultRole","SubnetId":"subnet-00ffbb73f98c19122","EmrManagedSlaveSecurityGroup":"sg-059805369c527ce28","EmrManagedMasterSecurityGroup":"sg-0b514cbb46d39ee30"}' --release-label emr-5.36.0 --log-uri 's3n://aws-logs-196761999526-ap-south-1/elasticmapreduce/' --steps '[{"Args":["spark-submit","--deploy-mode","client","--master","yarn","--packages","org.apache.spark:spark-avro_2.11:2.4.0","s3://devzeyo/proj.py","s3://devzeyo/dep.conf","Sivasankardir"],"Type":"CUSTOM_JAR","ActionOnFailure":"CONTINUE","Jar":"command-runner.jar","Properties":"","Name":"Pyrun"}]' --instance-groups '[{"InstanceCount":1,"EbsConfiguration":{"EbsBlockDeviceConfigs":[{"VolumeSpecification":{"SizeInGB":32,"VolumeType":"gp2"},"VolumesPerInstance":2}]},"InstanceGroupType":"MASTER","InstanceType":"m5.xlarge","Name":"Master Instance Group"}]' --auto-terminate --ebs-root-volume-size 10 --service-role EMR_DefaultRole  --name 'Sivasankar' --scale-down-behavior TERMINATE_AT_TASK_COMPLETION --region ap-south-1





aws emr describe-cluster --cluster-id  j-3S58OBQ79ECK5| grep 'State'


aws s3 ls s3://devzeyo/dest/

=============================

Out put:
--------

[itv004068@g01 ~]$ aws emr create-cluster  --applications Name=Hadoop Name=Spark --ec2-attributes '{"KeyName":"34key","InstanceProfile":"EMR_EC2_DefaultRole","SubnetId":"subnet-00ffbb73f98c19122","EmrManagedSlaveSecurityGroup":"sg-059805369c527ce28","EmrManagedMasterSecurityGroup":"sg-0b514cbb46d39ee30"}' --release-label emr-5.36.0 --log-uri 's3n://aws-logs-196761999526-ap-south-1/elasticmapreduce/' --steps '[{"Args":["spark-submit","--deploy-mode","client","--master","yarn","--packages","org.apache.spark:spark-avro_2.11:2.4.0","s3://devzeyo/proj.py","s3://devzeyo/dep.conf","Sivasankardir"],"Type":"CUSTOM_JAR","ActionOnFailure":"CONTINUE","Jar":"command-runner.jar","Properties":"","Name":"Pyrun"}]' --instance-groups '[{"InstanceCount":1,"EbsConfiguration":{"EbsBlockDeviceConfigs":[{"VolumeSpecification":{"SizeInGB":32,"VolumeType":"gp2"},"VolumesPerInstance":2}]},"InstanceGroupType":"MASTER","InstanceType":"m5.xlarge","Name":"Master Instance Group"}]' --auto-terminate --ebs-root-volume-size 10 --service-role EMR_DefaultRole  --name 'Sivasankar' --scale-down-behavior TERMINATE_AT_TASK_COMPLETION --region ap-south-1
{
    "ClusterId": "j-3S58OBQ79ECK5"
}
[itv004068@g01 ~]$ aws emr describe-cluster --cluster-id  j-3S58OBQ79ECK5| grep 'State'
            "State": "STARTING", 
            "StateChangeReason": {}
                    "State": "PROVISIONING", 
                    "StateChangeReason": {
[itv004068@g01 ~]$ aws s3 ls s3://devzeyo/dest/
                           PRE 2023-01-14/
                           PRE <AAA>/
                           PRE <AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA>/
                           PRE <Dsnavee>/
                           PRE <Prathiba>/
                           PRE <Sanket Khandekar>/
                           PRE <Srishti>/
                           PRE <URNAME>/
                           PRE <prasuna22>/
                           PRE Abhishek/
                           PRE Ahmed/
                           PRE Akshata/
                           PRE Amaan/
                           PRE Amol/
                           PRE Ankit99/
                           PRE Anu/
                           PRE Anusha/
                           PRE Aparna/
                           PRE Arjun/
                           PRE Arunkumar123/
                           PRE Ashwin/
                           PRE Asr/
                           PRE Avanish/
                           PRE BaluHitendra/
                           PRE BhargavTeja/
                           PRE Bilal/
                           PRE ChaitanyaV/
                           PRE Chaitanyaa/
                           PRE Chandana/
                           PRE DSR/
                           PRE Deji>/
                           PRE Dsna/
                           PRE Gajendra/
                           PRE Gautam/
                           PRE GuruappaDir/
                           PRE Gurumoorthy/
                           PRE Gururaj/
                           PRE GururajSdir/
                           PRE Himasekhar/
                           PRE Irappa/
                           PRE Ishu_J/
                           PRE Jayanthi/
                           PRE KD85/
                           PRE Kallan/
                           PRE KarthikP/
                           PRE Keerthi/
                           PRE Khushboo/
                           PRE Kiranmye/
                           PRE KousicGR/
                           PRE Kusuma/
                           PRE Latha/
                           PRE Mallidir/
                           PRE Mani/
                           PRE ManojKumar/
                           PRE Manojk/
                           PRE Mayank/
                           PRE Mukundh/
                           PRE Mur/
                           PRE Nalla/
                           PRE Nallathambi/
                           PRE Namrata/
                           PRE Natasha/
                           PRE Navaneetha/
                           PRE Naveenkota/
                           PRE Neha/
                           PRE Paalraj/
                           PRE Pooja/
                           PRE Prathiba/
                           PRE Prem/
                           PRE RAMATV/
                           PRE RAVI/
                           PRE Ragapriya/
                           PRE Raja/
                           PRE Ramarajan/
                           PRE Rathan/
                           PRE Ravalika/
                           PRE SANDEEPK/
                           PRE SM97/
                           PRE Saeed/
                           PRE Safin/
                           PRE Sai/
                           PRE Saidir/
                           PRE Sandeep/
                           PRE Savin/
                           PRE ShaikMahammadAli/
                           PRE ShailajaK/
                           PRE Shailendra/
                           PRE Shashi/
                           PRE Shruti/
                           PRE Sivasankar/
                           PRE Sivdir/
                           PRE Sreeja/
                           PRE Srishti/
                           PRE Subrahmanya/
                           PRE Subramani_batch34/
                           PRE Suchidir/
                           PRE Suganthi/
                           PRE Surabhi/
                           PRE Sushanth/
                           PRE Tharun/
                           PRE Umashankardir/
                           PRE Vinodh/
                           PRE Yash/
                           PRE aakashm/
                           PRE abdul234/
                           PRE ajay/
                           PRE anitharaj/
                           PRE anja/
                           PRE arun/
                           PRE ashish/
                           PRE ashokavanjare/
                           PRE bhaskarbabul/
                           PRE boomab34/
                           PRE chandan/
                           PRE chandra/
                           PRE dhana/
                           PRE dhanush/
                           PRE durga/
                           PRE gana/
                           PRE ganeshb/
                           PRE gani/
                           PRE gowthamgowda/
                           PRE gowthamikj/
                           PRE harika/
                           PRE harry/
                           PRE itv004058/
                           PRE jay/
                           PRE karthi/
                           PRE kavithasai/
                           PRE kesavan/
                           PRE madhavan/
                           PRE manjus/
                           PRE mano/
                           PRE maruthi/
                           PRE mrudhula/
                           PRE mubarak/
                           PRE nanda/
                           PRE naveenkumar/
                           PRE pawan/
                           PRE pkvaustin/
                           PRE prabhat/
                           PRE prakash/
                           PRE prasad_k_9/
                           PRE prasannakumar/
                           PRE praveent/
                           PRE rajani/
                           PRE rakesh/
                           PRE ram/
                           PRE ramakrishna/
                           PRE ramimah/
                           PRE ravali/
                           PRE sakthivel2208/
                           PRE santhosh395/
                           PRE senthil/
                           PRE seshadris/
                           PRE sheela/
                           PRE shrikant/
                           PRE shwathi/
                           PRE sikandar/
                           PRE siva/
                           PRE sm/
                           PRE srikanth/
                           PRE subash/
                           PRE sudheer/
                           PRE sudhir/
                           PRE suriya/
                           PRE syed498/
                           PRE tarun/
                           PRE tejaswinic/
                           PRE udaykiran/
                           PRE ullas/
                           PRE usha/
                           PRE uzzwal/
                           PRE vamshikrish/
                           PRE venakt/
                           PRE venkata/
                           PRE venkatesh/
                           PRE venugopal7/
                           PRE vijayashanthi/
                           PRE vjaat/
                           PRE wasim/
                           PRE yuvi/
2023-01-15 00:54:09          0 2023-01-14_$folder$
2023-01-15 03:31:37          0 <Sanket Khandekar>_$folder$
2023-01-15 02:28:34          0 <Srishti>_$folder$
2023-01-15 01:21:06          0 <URNAME>_$folder$
2023-01-15 04:10:34          0 <prasuna22>_$folder$
2023-01-19 22:07:34          0 Anu_$folder$
2023-01-15 01:41:21          0 Bilal_$folder$
2023-01-15 12:02:21          0 Gautam_$folder$
2023-01-15 01:58:01          0 Himasekhar_$folder$
2023-01-19 09:51:40          0 Khushboo_$folder$
2023-01-15 11:44:58          0 Navaneetha_$folder$
2023-01-20 23:17:08          0 Pooja_$folder$
2023-01-15 12:16:22          0 RAMATV_$folder$
2023-01-15 08:23:47          0 Safin_$folder$
2023-01-15 19:37:51          0 Shruti_$folder$
2023-01-15 02:15:00          0 Sivasankar_$folder$
2023-01-15 01:58:55          0 aakashm_$folder$
2023-01-16 23:47:04          0 ajay_$folder$
2023-01-18 13:17:05          0 anja_$folder$
2023-01-15 02:04:58          0 chandra_$folder$
2023-01-15 02:01:14          0 kavithasai_$folder$
2023-01-15 07:31:12          0 manjus_$folder$
2023-01-15 12:45:59          0 rajani_$folder$
2023-01-20 21:38:27          0 santhosh395_$folder$
2023-01-15 17:00:01          0 uzzwal_$folder$
[itv004068@g01 ~]$ aws s3 ls s3://devzeyo/destpy/Sivasankardir/
                           PRE avail/
                           PRE noavail/
[itv004068@g01 ~]$ aws s3 ls s3://devzeyo/destpy/Sivasankardir/avail/
2023-01-22 02:06:38          0 _SUCCESS
2023-01-22 02:06:38    1654331 part-00000-4a86a700-0889-4ed9-bd93-4a44bfe3a58b-c000.snappy.parquet
[itv004068@g01 ~]$ aws s3 ls s3://devzeyo/destpy/Sivasankardir/noavail/
2023-01-22 02:06:36          0 _SUCCESS
2023-01-22 02:06:35     214728 part-00000-861806cc-cba2-4c83-a2ed-9fb59b8b893f-c000.snappy.parquet
[itv004068@g01 ~]$ 

===============================






==========
04-02-2023
==========
======================
Extract Zookeeper --- Done 
======================

======================
Extract Kafka --- Done
======================

======================
start zookeeper 
======================

Go inside zookeeper folder
Go inside bin folder
Open cmd in that folder
type .\zkserver

minimize the window

======================
start kafka
======================

Check whether you have tmp folder in the drive where you kept kafka delete kafka-logs folder if you have
Go inside kafka folder
Open cmd
shot below command 

.\bin\windows\kafka-server-start.bat .\config\server.properties

minimize the window

======================
Create a topic 34tk
======================

Go inside kafka folder
Go inside bin folder
Go inside windows folder
open cmd

kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic 34tk1


======================
Open producer black box (CLI)
======================

Go inside kafka folder
Go inside bin folder
Go inside windows folder
open cmd

kafka-console-producer.bat --broker-list localhost:9092 --topic 34tk1




======================
Open consumer black box (CLI)
======================


Go inside kafka folder
Go inside bin folder
Go inside windows folder
open cmd

kafka-console-consumer.bat --zookeeper localhost:2181  topic 34tk1

or (If not working)

kafka-console-consumer.bat --bootstrap-server localhost:9092  topic 34tk1



======================
Push the data from producer
======================

start pushing the data in producer console



======================
Check the data in consumer
======================

just click enter in consume console



=====================================
05-02-2023:
-----------
Remove tmp folder
start zookeeper
start kafka
create a topic tk34
Open producer console for tk34
Open Eclipse
Do basic configuration
Add kafka spark Jars

Put the below code and run the Code


======================
start zookeeper 
======================

Go inside zookeeper folder
Go inside bin folder
Open cmd in that folder
type .\zkserver

minimize the window

======================
start kafka
======================

Check whether you have tmp folder in the drive where you kept kafka delete kafka-logs folder if you have
Go inside kafka folder
Open cmd
shot below command 

.\bin\windows\kafka-server-start.bat .\config\server.properties

minimize the window

======================
Create a topic 34tk
======================

Go inside kafka folder
Go inside bin folder
Go inside windows folder
open cmd

kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic 34tk1


======================
Open producer black box (CLI)
======================

Go inside kafka folder
Go inside bin folder
Go inside windows folder
open cmd

kafka-console-producer.bat --broker-list localhost:9092 --topic 34tk1


==============
Handson Task1:
==============

=====> Code <=======

package pack

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.SparkSession
import org.apache.spark.streaming._
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.kafka010._
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe


object obj {


	def main(args:Array[String]):Unit={


			val conf = new SparkConf().setAppName("ES").setMaster("local[*]").set("spark.driver.allowMultipleContexts","true")

					val sc = new SparkContext(conf)
					sc.setLogLevel("Error")


					val spark = SparkSession
					.builder()
					.getOrCreate()
					import spark.implicits._ 



					val ssc = new StreamingContext(conf,Seconds(2))


					val tk = Array("tk34")


					val kp = 
					  
					Map[String, Object](
					    
							"bootstrap.servers" -> "localhost:9092"
							,"key.deserializer" -> classOf[StringDeserializer],
							"value.deserializer" -> classOf[StringDeserializer],
							"group.id" -> "example22",
							"auto.offset.reset" -> "latest"
					)


					val stream = KafkaUtils
					.createDirectStream[String, String](
					    ssc,PreferConsistent,Subscribe[String, String](
					        tk, kp))

					

         stream.map( x => x.value ).print()
         
         
         
         
         
         ssc.start()
         ssc.awaitTermination()
         
        
	}
}


--------
Out put:
--------




=====================================

Handson Task2:
--------------
package pack

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.SparkSession
import org.apache.spark.streaming._
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.kafka010._
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe


object obj {


	def main(args:Array[String]):Unit={


			val conf = new SparkConf().setAppName("ES").setMaster("local[*]").set("spark.driver.allowMultipleContexts","true")

					val sc = new SparkContext(conf)
					sc.setLogLevel("Error")


					val spark = SparkSession
					.builder()
					.getOrCreate()
					import spark.implicits._ 



					val ssc = new StreamingContext(conf,Seconds(2))


					val tk = Array("tk34")


					val kp = 

					Map[String, Object](

							"bootstrap.servers" -> "localhost:9092"
							,"key.deserializer" -> classOf[StringDeserializer],
							"value.deserializer" -> classOf[StringDeserializer],
							"group.id" -> "example22",
							"auto.offset.reset" -> "latest"
							)


					val stream = KafkaUtils
					.createDirectStream[String, String](
							ssc,PreferConsistent,Subscribe[String, String](
									tk, kp))



					val stream1 =  stream.map( x => x.value )

					stream1.print

					stream1.foreachRDD( x =>

					if(!x.isEmpty()){

					  
					       val data = x.map( x => x+",zeyobron") 
					       
					       val df = data.toDF("value")
					  
					       df.show()
					  

					}


				)






					ssc.start()
					ssc.awaitTermination()


	}
}


-------------
Out put:
--------


=============================
Task 1 ----


Process the data 
foreach --- Dataframe ---- show()




===========
Task 2 ----
===========


What is Kafka ISR?

Kafka In-Sync Replica (ISR):
___________________________

1.This is a new concept introduced in Kafka. This represents the number of replica in sync with each other in the cluster.
2.This includes both leader and follower replica.
3.The in-sync replica is always recommended to be always greater than 1.
4.The ideal state of the replication is  ISR == Replication Factor
5.The ISR can be controlled by min.insync.replicas property which can be set at the broker or topic level.



====================================
11-02-2023:
-----------
Itr 1

Start Zookeeper Server
Start Kafka Server
Create topic 34E
Open producer Console for topic 34E
Push A and B data
Consume using Spark with G1 - GroupID and Earliest
Run the Code and check whether A and B is getting consumed


Itr 2

Create topic 34L
Open producer Console for topic 34L
Push A and B data
Consume using Spark with G2 - GroupID and Latest
Run the Code and check whether A and B is not getting consumed

If you want to verify . Install Offset Explorer and Check


Iteration 1 Code

package pack

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.SparkSession
import org.apache.spark.streaming._
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.kafka010._
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe


object obj {


	def main(args:Array[String]):Unit={


			val conf = new SparkConf().setAppName("ES").setMaster("local[*]").set("spark.driver.allowMultipleContexts","true")

					val sc = new SparkContext(conf)
					sc.setLogLevel("Error")


					val spark = SparkSession
					.builder()
					.getOrCreate()
					import spark.implicits._ 



					val ssc = new StreamingContext(conf,Seconds(2))


					val tk = Array("34E")


					val kp = 

					Map[String, Object](

							"bootstrap.servers" -> "localhost:9092"
							,"key.deserializer" -> classOf[StringDeserializer],
							"value.deserializer" -> classOf[StringDeserializer],
							"group.id" -> "G1",
							"auto.offset.reset" -> "earliest"
							)


					val stream = KafkaUtils
					.createDirectStream[String, String](
							ssc,PreferConsistent,Subscribe[String, String](
									tk, kp))



					val stream1 =  stream.map( x => x.value )

					stream1.print
					
					
					
					
					

				/*	stream1.foreachRDD( x =>

					if(!x.isEmpty()){

					  
					       val data = x.map( x => x+",zeyobron") 
					       
					       val df = data.toDF("value")
					  
					       df.show()
					  

					}


				)

*/




					ssc.start()
					ssc.awaitTermination()


	}
}


Iteration 2 Code

package pack

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.SparkSession
import org.apache.spark.streaming._
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.kafka010._
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe


object obj {


	def main(args:Array[String]):Unit={


			val conf = new SparkConf().setAppName("ES").setMaster("local[*]").set("spark.driver.allowMultipleContexts","true")

					val sc = new SparkContext(conf)
					sc.setLogLevel("Error")


					val spark = SparkSession
					.builder()
					.getOrCreate()
					import spark.implicits._ 



					val ssc = new StreamingContext(conf,Seconds(2))


					val tk = Array("34L")


					val kp = 

					Map[String, Object](

							"bootstrap.servers" -> "localhost:9092"
							,"key.deserializer" -> classOf[StringDeserializer],
							"value.deserializer" -> classOf[StringDeserializer],
							"group.id" -> "G2",
							"auto.offset.reset" -> "latest"
							)


					val stream = KafkaUtils
					.createDirectStream[String, String](
							ssc,PreferConsistent,Subscribe[String, String](
									tk, kp))



					val stream1 =  stream.map( x => x.value )

					stream1.print
					
					
					
					
					

				/*	stream1.foreachRDD( x =>

					if(!x.isEmpty()){

					  
					       val data = x.map( x => x+",zeyobron") 
					       
					       val df = data.toDF("value")
					  
					       df.show()
					  

					}


				)

*/




					ssc.start()
					ssc.awaitTermination()


	}
}

=========================
AWS Kinesis Handson Task:
=========================

Step 1 ---- Configure aws

aws configure
access key --- AKIASVGFNNNDLCSTF3H3
Secret key --- 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
location --- ap-south-1
format --- json

Step 2 --- Create a unique stream for you

aws kinesis create-stream --stream-name <UNIQUE_STREAM_NAME> --shard-count 1

Step 3 ---- Add kinesis Jars to Eclipse Project


Step 4 ----- Put the Kinesis Spark Code with Kinesis MQ name changed



Step 5 ------ Start the Stream and Push the data after 2 Minute


aws kinesis put-record --stream-name <UNIQUE_STREAM_NAME> --partition-key 123 --data zeyobron

Step 1 ---- Configure aws

aws configure
access key --- AKIASVGFNNNDLCSTF3H3
Secret key --- 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
location --- ap-south-1
format --- json

Step 2 --- Create a unique stream for you

aws kinesis create-stream --stream-name <UNIQUE_STREAM_NAME> --shard-count 1

Step 3 ---- Add kinesis Jars to Eclipse Project


Step 4 ----- Put the Kinesis Spark Code with Kinesis MQ name changed



Step 5 ------ Start the Stream and Push the data after 2 Minute


aws kinesis put-record --stream-name <UNIQUE_STREAM_NAME> --partition-key 123 --data zeyobron

package pack

import org.apache.spark._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.kinesis.KinesisInputDStream
import org.apache.spark.streaming.{Seconds, StreamingContext}
import com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream
import org.apache.spark.streaming.Duration
import org.apache.spark._
import org.apache.spark.sql._
import com.amazonaws.protocol.StructuredPojo
import org.apache.spark.sql.functions._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming._
import com.amazonaws.services.kinesis.clientlibrary.lib.worker.KinesisClientLibConfiguration
import com.amazonaws.services.dynamodbv2.model.BillingMode
import com.amazonaws.services.cloudwatch.AmazonCloudWatch
import org.apache.hadoop.fs.s3a.S3AFileSystem
import com.fasterxml.jackson.dataformat.cbor.CBORFactory
import com.fasterxml.jackson.core.TSFBuilder
import org.apache.spark.streaming.kinesis.KinesisUtils

object obj {

  def b2s(a: Array[Byte]): String = new String(a)

	def main(args:Array[String]):Unit={

			val conf = new SparkConf().setAppName("first").setMaster("local[*]")
					.set("spark.driver.allowMultipleContexts","true")
					.set("AWS_ACCESS_KEY","AKIASVGFNNNDLCSTF3H3")
					.set("AWS_SECRET_KEY","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")
					.set("AWS_CBOR_DISABLE","true")
			
			val sc = new SparkContext(conf)
			sc.setLogLevel("ERROR")
			val spark = SparkSession
			.builder()
			.getOrCreate()
			import spark.implicits._
	  
	  
			
			
				val ssc = new StreamingContext(conf,Seconds(2))
			
				val stream= KinesisUtils.createStream(
								ssc, 
								"<UNIQUEGROUPNAME>",
								"<UNIQUESTREAMNAME>",   // YOUR_MQ_NAME
								"https://kinesis.ap-south-1.amazonaws.com",
								"ap-south-1",
								InitialPositionInStream.LATEST, 
								Seconds(2), 
								StorageLevel.MEMORY_AND_DISK_2)
			
			val finalstream=stream.map(x=>b2s(x))
			
			
			

			
			
			finalstream.print()
			
			ssc.start()
			ssc.awaitTermination()
			
			
			
			
			
			
			
		
	}
	
	
}



package pack

import org.apache.spark._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.kinesis.KinesisInputDStream
import org.apache.spark.streaming.{Seconds, StreamingContext}
import com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream
import org.apache.spark.streaming.Duration
import org.apache.spark._
import org.apache.spark.sql._
import com.amazonaws.protocol.StructuredPojo
import org.apache.spark.sql.functions._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming._
import com.amazonaws.services.kinesis.clientlibrary.lib.worker.KinesisClientLibConfiguration
import com.amazonaws.services.dynamodbv2.model.BillingMode
import com.amazonaws.services.cloudwatch.AmazonCloudWatch
import org.apache.hadoop.fs.s3a.S3AFileSystem
import com.fasterxml.jackson.dataformat.cbor.CBORFactory
import com.fasterxml.jackson.core.TSFBuilder
import org.apache.spark.streaming.kinesis.KinesisUtils

object obj {

  def b2s(a: Array[Byte]): String = new String(a)

	def main(args:Array[String]):Unit={

			val conf = new SparkConf().setAppName("first").setMaster("local[*]")
					.set("spark.driver.allowMultipleContexts","true")
					.set("AWS_ACCESS_KEY","AKIASVGFNNNDLCSTF3H3")
					.set("AWS_SECRET_KEY","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")
					.set("AWS_CBOR_DISABLE","true")
			
			val sc = new SparkContext(conf)
			sc.setLogLevel("ERROR")
			val spark = SparkSession
			.builder()
			.getOrCreate()
			import spark.implicits._
	  
	  
			
			
				val ssc = new StreamingContext(conf,Seconds(2))
			
				val stream= KinesisUtils.createStream(
								ssc, 
								"<UNIQUEGROUPNAME>",
								"<UNIQUESTREAMNAME>",   // YOUR_MQ_NAME
								"https://kinesis.ap-south-1.amazonaws.com",
								"ap-south-1",
								InitialPositionInStream.LATEST, 
								Seconds(2), 
								StorageLevel.MEMORY_AND_DISK_2)
			
			val finalstream=stream.map(x=>b2s(x))
			
			
			

			
			
			finalstream.print()
			
			ssc.start()
			ssc.awaitTermination()
			
			
			
			
			
			
			
		
	}
	
	
}



Login to the Cluster

browser -- e.cloudxlab.com

hive

create external table hivehtab_Sivasankar(key string,hid string,hname string) STORED BY "org.apache.hadoop.hive.hbase.HBaseStorageHandler" with serdeproperties ("hbase.columns.mapping" = ":key,tcf:pid,tcf:pname") tblproperties("hbase.table.name"="34tab");


create external table hivehtab_<UNIQUENAME>;



spark

spark-shell --conf "spark.driver.extraClassPath=/home/vasudhaclouds1683/hbasejars/*"


import org.apache.spark.sql.{SQLContext, _}
import org.apache.spark.sql.execution.datasources.hbase._
import org.apache.spark.{SparkConf, SparkContext}


def catalog = s"""{
    |"table":{"namespace":"default", "name":"34tab"},
    |"rowkey":"rowkey",
    |"columns":{
    |"skey":{"cf":"rowkey", "col":"rowkey", "type":"string"},
    |"sid":{"cf":"tcf", "col":"pid", "type":"string"},
    |"sname":{"cf":"tcf", "col":"pname", "type":"string"}
    |}
|}""".stripMargin


val df =   spark.read.options(Map(HBaseTableCatalog.tableCatalog->catalog )).format("org.apache.spark.sql.execution.datasources.hbase").load()

df.show()

username --- vasudhaclouds1683
password -- - 9ZWWGXIB


vasudhaclouds1683@e.cloudxlab.com's password:
    
                      MobaXterm Personal Edition v22.1                  
                   (SSH client, X server and network tools)               
                                                                          
      SSH session to vasudhaclouds1683@e.cloudxlab.com                   
        Direct SSH      :                                               
        SSH compression :                                               
        SSH-browser     :                                               
        X11-forwarding  :    (remote display is forwarded through SSH)  
                                                                          
      For more info, ctrl+click on help or visit our website.            
    

Last login: Sun Feb 12 06:42:27 2023
[vasudhaclouds1683@cxln4 ~]$ ls
datatxns.txt  hbasejars  mrWc-0.0.1-SNAPSHOT.jar  sparkd-0.0.1-SNAPSHOT.jar  SparkStreaming-0.0.1-SNAPSHOT.jar  srcfilew.txt
[vasudhaclouds1683@cxln4 ~]$ whoami
vasudhaclouds1683
[vasudhaclouds1683@cxln4 ~]$ hive
log4j:WARN No such property [maxFileSize] in org.apache.log4j.DailyRollingFileAppender.

Logging initialized using configuration in file:/etc/hive/2.6.2.0-205/0/hive-log4j.properties
hive> create external table hivehtab_Sivasankar(key string,hid string,hname string) STORED BY "org.apache.hadoop.hive.hbase.HBaseStorageHandler" with serdeproperties ("hbase.columns.mapping" = ":key,tcf:pid,tcf:pname") tblproperties("hbase.table.name"="34tab");
OK
Time taken: 4.071 seconds
hive> create external table hivehtab_Sivasankar;
FAILED: SemanticException [Error 10043]: Either list of columns or a custom serializer should be specified
hive> [vasudhaclouds1683@cxln4 ~]$ spark-shell --conf "spark.driver.extraClassPath=/home/vasudhaclouds1683/hbasejars/*"
SPARK_MAJOR_VERSION is set to 2, using Spark2
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
23/02/12 06:49:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/02/12 06:49:12 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/02/12 06:49:12 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/02/12 06:49:12 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/02/12 06:49:12 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
Spark context Web UI available at http://10.142.1.4:4045
Spark context available as 'sc' (master = local[*], app id = local-1676184553136).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.1.1.2.6.2.0-205
      /_/

Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_112)
Type in expressions to have them evaluated.
Type :help for more information.

scala> import org.apache.spark.sql.{SQLContext, _}
import org.apache.spark.sql.{SQLContext, _}

scala> import org.apache.spark.sql.execution.datasources.hbase._
import org.apache.spark.sql.execution.datasources.hbase._

scala> import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.{SparkConf, SparkContext}

scala>

scala>

scala> def catalog = s"""{
     |     |"table":{"namespace":"default", "name":"34tab"},
     |     |"rowkey":"rowkey",
     |     |"columns":{
     |     |"skey":{"cf":"rowkey", "col":"rowkey", "type":"string"},
     |     |"sid":{"cf":"tcf", "col":"pid", "type":"string"},
     |     |"sname":{"cf":"tcf", "col":"pname", "type":"string"}
     |     |}
     | |}""".stripMargin
catalog: String

scala>

scala>

scala> val df =   spark.read.options(Map(HBaseTableCatalog.tableCatalog->catalog )).format("org.apache.spark.sql.execution.datasources.hbase").load()
df: org.apache.spark.sql.DataFrame = [skey: string, sid: string ... 1 more field]

scala>

scala> df.show()
+----+----+------+
|skey| sid| sname|
+----+----+------+
| 001|zeyo|  null|
| 002| sai|  null|
| 003|null|vidyut|
| 004|null|haasya|
+----+----+------+


scala>
Remote side unexpectedly closed network connection



Session stopped
    - Press <return> to exit tab
    - Press R to restart session
    - Press S to save terminal output to file





===================================

18-02-2023:
-----------
1 - Windows  After installing datastax

Open cassandra cql shell directly

2 -- Extract cassandra - Go inside bin-- type cassandra and enter

	Again open cassandra CQL shell

3 -- Ubuntu/ mac --- Extract Cassandra Go inside bin -- type .\cassandra and enter

	Minimize it and open the cmd line for the same folder and type .\cqlsh




CREATE KEYSPACE zzdb WITH replication = {'class':'SimpleStrategy', 'replication_factor' : 1};

describe keyspaces;

use zzdb;

describe tables;

create table ztab(id int PRIMARY KEY,name text);

insert into ztab(id,name) values(1,'sai');
insert into ztab(id,name) values(2,'zeyo');

select * from ztab;
===================================
===================================
Spark Cassandra Integration

package pack

import org.apache.spark._
import org.apache.spark._
import org.apache.spark.sql._
import org.apache.spark.sql.functions._

object obj {


	def main(args:Array[String]):Unit={

			val conf = new SparkConf().setAppName("first").setMaster("local[*]")
				
			val sc = new SparkContext(conf)
			sc.setLogLevel("ERROR")
			val spark = SparkSession
			.builder()
			.getOrCreate()
			import spark.implicits._
	  
	  

			val df = spark.read.format("org.apache.spark.sql.cassandra")
			              .option("spark.cassandra.connection.host","localhost")
			              .option("spark.cassandra.connection.port","9042")
			              .option("keyspace","zzdb")
			              .option("table","ztab")
			              .load
			
			
			
			val df1 = df.withColumn("id",expr("case when id=1 then 2 else 0 end"))
			
			df1.show()
			
			df1.write.format("org.apache.spark.sql.cassandra")
			              .option("spark.cassandra.connection.host","localhost")
			              .option("spark.cassandra.connection.port","9042")
			              .option("keyspace","zzdb")
			              .option("table","ztab")
			              .mode("append")
			              .save
			
			
			
			
			
		
	}
	
	
}

===================================

====================================
19-02-2023:
----------
package pack

import org.apache.spark._
import org.apache.spark._
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._

object obj {

	def main(args:Array[String]):Unit={

			val conf = new SparkConf().setAppName("first").setMaster("local[*]")

					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")
					val spark = SparkSession
					.builder()
					.getOrCreate()
					import spark.implicits._


					val streamschema = StructType(Array(
							StructField("id", StringType, true),
							StructField("name", StringType, true)));  


			val df = spark
					.readStream
					.format("csv")
					.schema(streamschema)
					.load("file:///C:/data/datain/data")


			val finaldf = df.withColumn("status", lit("zeyo"))		
					
					
					finaldf.writeStream
					.format("console")
					.start()
					.awaitTermination()


	}
}

====================================
====================================
package pack

import org.apache.spark._
import org.apache.spark._
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._

object obj {

	def main(args:Array[String]):Unit={

			val conf = new SparkConf().setAppName("first").setMaster("local[*]")

					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")
					val spark = SparkSession
					.builder()
					.getOrCreate()
					import spark.implicits._



					val df = spark
					        .readStream
					        .format("kafka")
					        .option("kafka.bootstrap.servers","localhost:9092")
					        .option("subscribe","sstopic")
					        .load()
				
				val finaldf = df.withColumn("value",expr("cast(value as string)"))	       
				              .select("value")

					finaldf.writeStream 
					  .format("console")
					  .start()
					  .awaitTermination()




	}
}
====================================
====================================
package pack

import org.apache.spark._
import org.apache.spark._
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._

object obj {

	def main(args:Array[String]):Unit={

			val conf = new SparkConf().setAppName("first").setMaster("local[*]")

					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")
					val spark = SparkSession
					.builder()
					.getOrCreate()
					import spark.implicits._



					val df = spark
					        .readStream
					        .format("kafka")
					        .option("kafka.bootstrap.servers","localhost:9092")
					        .option("subscribe","sstopic")
					        .load()
				
				val finaldf = df.withColumn("value",expr("cast(value as string)"))	       
				              .select("value")

					finaldf.writeStream 
					  .format("console")
					  .start()
					  .awaitTermination()




	}
}
====================================
====================================
Kafka spark read/write

package pack

import org.apache.spark._
import org.apache.spark._
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._

object obj {

	def main(args:Array[String]):Unit={

			val conf = new SparkConf().setAppName("first").setMaster("local[*]")

					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")
					val spark = SparkSession
					.builder()
					.getOrCreate()
					import spark.implicits._



					val df = spark
					        .readStream
					        .format("kafka")
					        .option("kafka.bootstrap.servers","localhost:9092")
					        .option("subscribe","sstopic")
					        .load()
				
				val finaldf = df.withColumn("value",expr("cast(value as string)"))
				                .withColumn("value",expr("concat(value,',zeyo')"))
				              .select("value")

					finaldf.writeStream 
					  .format("kafka")
            .option("kafka.bootstrap.servers","localhost:9092")
            .option("checkpointLocation","file:///C:/data/kafkass")
            .option("topic","tttopic")
					  .start()
					  .awaitTermination()




	}
}
====================================

====================================
package pack

import org.apache.spark._
import org.apache.spark._
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._

object obj {

	def main(args:Array[String]):Unit={

			val conf = new SparkConf().setAppName("first").setMaster("local[*]")

					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")
					val spark = SparkSession
					.builder()
					.getOrCreate()
					import spark.implicits._



					val df = spark
					.readStream
					.format("kafka")
					.option("kafka.bootstrap.servers","localhost:9092")
					.option("subscribe","sstopic")
					.load()

					val finaldf = df.withColumn("value",expr("cast(value as string)"))
					.withColumn("value",expr("concat(value,',zeyo')"))
					.select("value")
					
					
					
					
					
					
					

					finaldf.writeStream 
					  .foreachBatch { (df: DataFrame, id: Long) =>

					          df.write.format("org.apache.spark.sql.cassandra")
					          .option("host","localhost")
					          .option("port","9042")
					          .option("host","zzdb")
					          .option("host","ztab")
					          .save()

			            }
			
			
			        .start()
			      .awaitTermination()




	}
}
====================================
====================================
package pack

import org.apache.spark._
import org.apache.spark._
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._

object obj {

	def main(args:Array[String]):Unit={

			val conf = new SparkConf().setAppName("first").setMaster("local[*]")

					val sc = new SparkContext(conf)
					sc.setLogLevel("ERROR")
					val spark = SparkSession
					.builder()
					.getOrCreate()
					import spark.implicits._



					val df = spark
					.readStream
					.format("kafka")
					.option("kafka.bootstrap.servers","localhost:9092")
					.option("subscribe","sstopic")
					.load()

					val finaldf = df.withColumn("value",expr("cast(value as string)"))
					.withColumn("value",expr("concat(value,',zeyo')"))
					.select("value")
					
					
					
					
					
					
					

					finaldf.writeStream 
					  .foreachBatch { (df: DataFrame, id: Long) =>

					          df.write.format("org.apache.spark.sql.cassandra")
					          .option("host","localhost")
					          .option("port","9042")
					          .option("keyspace","zzdb")
					          .option("table","ztab")
					          .save()

			            }
			
			
			        .start()
			      .awaitTermination()




	}
}
====================================